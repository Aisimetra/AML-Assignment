{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Artemisia_Sarteschi_829677.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FH5ja_uiJbr6"
      },
      "source": [
        "# Predicting Default Payments with Fully-Connected NNs\n",
        "\n",
        "The dataset contains information on default payments, demographic factors, credit data, history of payment, and bill statements of credit card clients in Taiwan from April 2005 to September 2005. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I4LTZI2luhcH",
        "outputId": "dc191e84-2e8d-4497-851e-4904b680dc70"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive',force_remount=True)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0-FgzT_cJbsH"
      },
      "source": [
        "##Inspecting the data\n",
        "\n",
        "any comment about data dimensionality/distribution goes here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tzg9dHfw_2gj"
      },
      "source": [
        "### Librerie necessarie\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1IWaSSOY3Uy8"
      },
      "source": [
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import preprocessing\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Dense, Activation\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7imbesOi_5-U"
      },
      "source": [
        "Importo i dataset precedentemente collegati tramite drive\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8vSiz47HXYYM"
      },
      "source": [
        "X_train = pd.read_csv('drive/MyDrive/Colab_Notebooks/X_train.csv')\n",
        "Y_train = pd.read_csv('drive/MyDrive/Colab_Notebooks/y_train.csv')\n",
        "Test = pd.read_csv('drive/MyDrive/Colab_Notebooks/X_test.csv')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "so1MmNQTAFRA"
      },
      "source": [
        "Visualizzo i dati e faccio alcune analisi esplorative sul target"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UAOlV6U4A_Ii",
        "outputId": "6d14d5ec-75a5-4340-d882-b644e37a1c51"
      },
      "source": [
        "print (X_train.shape, Y_train.shape)\n",
        "print (Test.shape)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(24000, 24) (24000, 2)\n",
            "(6000, 24)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 844
        },
        "id": "pA8e-TiJAOPh",
        "outputId": "a82f3a44-03e7-42f5-bdd3-98e924ace68a"
      },
      "source": [
        "X_train.info()\n",
        "X_train.describe()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 24000 entries, 0 to 23999\n",
            "Data columns (total 24 columns):\n",
            " #   Column     Non-Null Count  Dtype  \n",
            "---  ------     --------------  -----  \n",
            " 0   ID         24000 non-null  int64  \n",
            " 1   LIMIT_BAL  24000 non-null  float64\n",
            " 2   SEX        24000 non-null  int64  \n",
            " 3   EDUCATION  24000 non-null  int64  \n",
            " 4   MARRIAGE   24000 non-null  int64  \n",
            " 5   AGE        24000 non-null  int64  \n",
            " 6   PAY_0      24000 non-null  int64  \n",
            " 7   PAY_2      24000 non-null  int64  \n",
            " 8   PAY_3      24000 non-null  int64  \n",
            " 9   PAY_4      24000 non-null  int64  \n",
            " 10  PAY_5      24000 non-null  int64  \n",
            " 11  PAY_6      24000 non-null  int64  \n",
            " 12  BILL_AMT1  24000 non-null  float64\n",
            " 13  BILL_AMT2  24000 non-null  float64\n",
            " 14  BILL_AMT3  24000 non-null  float64\n",
            " 15  BILL_AMT4  24000 non-null  float64\n",
            " 16  BILL_AMT5  24000 non-null  float64\n",
            " 17  BILL_AMT6  24000 non-null  float64\n",
            " 18  PAY_AMT1   24000 non-null  float64\n",
            " 19  PAY_AMT2   24000 non-null  float64\n",
            " 20  PAY_AMT3   24000 non-null  float64\n",
            " 21  PAY_AMT4   24000 non-null  float64\n",
            " 22  PAY_AMT5   24000 non-null  float64\n",
            " 23  PAY_AMT6   24000 non-null  float64\n",
            "dtypes: float64(13), int64(11)\n",
            "memory usage: 4.4 MB\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>LIMIT_BAL</th>\n",
              "      <th>SEX</th>\n",
              "      <th>EDUCATION</th>\n",
              "      <th>MARRIAGE</th>\n",
              "      <th>AGE</th>\n",
              "      <th>PAY_0</th>\n",
              "      <th>PAY_2</th>\n",
              "      <th>PAY_3</th>\n",
              "      <th>PAY_4</th>\n",
              "      <th>PAY_5</th>\n",
              "      <th>PAY_6</th>\n",
              "      <th>BILL_AMT1</th>\n",
              "      <th>BILL_AMT2</th>\n",
              "      <th>BILL_AMT3</th>\n",
              "      <th>BILL_AMT4</th>\n",
              "      <th>BILL_AMT5</th>\n",
              "      <th>BILL_AMT6</th>\n",
              "      <th>PAY_AMT1</th>\n",
              "      <th>PAY_AMT2</th>\n",
              "      <th>PAY_AMT3</th>\n",
              "      <th>PAY_AMT4</th>\n",
              "      <th>PAY_AMT5</th>\n",
              "      <th>PAY_AMT6</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>24000.000000</td>\n",
              "      <td>24000.000000</td>\n",
              "      <td>24000.000000</td>\n",
              "      <td>24000.000000</td>\n",
              "      <td>24000.000000</td>\n",
              "      <td>24000.000000</td>\n",
              "      <td>24000.000000</td>\n",
              "      <td>24000.000000</td>\n",
              "      <td>24000.000000</td>\n",
              "      <td>24000.000000</td>\n",
              "      <td>24000.000000</td>\n",
              "      <td>24000.000000</td>\n",
              "      <td>24000.000000</td>\n",
              "      <td>24000.000000</td>\n",
              "      <td>2.400000e+04</td>\n",
              "      <td>24000.000000</td>\n",
              "      <td>24000.000000</td>\n",
              "      <td>24000.000000</td>\n",
              "      <td>24000.000000</td>\n",
              "      <td>2.400000e+04</td>\n",
              "      <td>24000.000000</td>\n",
              "      <td>24000.000000</td>\n",
              "      <td>24000.000000</td>\n",
              "      <td>24000.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>15010.821708</td>\n",
              "      <td>167226.653333</td>\n",
              "      <td>1.604917</td>\n",
              "      <td>1.854000</td>\n",
              "      <td>1.551417</td>\n",
              "      <td>35.494375</td>\n",
              "      <td>-0.016667</td>\n",
              "      <td>-0.131375</td>\n",
              "      <td>-0.168167</td>\n",
              "      <td>-0.220417</td>\n",
              "      <td>-0.265167</td>\n",
              "      <td>-0.288750</td>\n",
              "      <td>50927.468417</td>\n",
              "      <td>48914.770500</td>\n",
              "      <td>4.675708e+04</td>\n",
              "      <td>43013.532167</td>\n",
              "      <td>40150.333000</td>\n",
              "      <td>38763.540458</td>\n",
              "      <td>5670.826542</td>\n",
              "      <td>5.961101e+03</td>\n",
              "      <td>5258.246500</td>\n",
              "      <td>4880.847125</td>\n",
              "      <td>4818.849250</td>\n",
              "      <td>5159.462125</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>8680.406114</td>\n",
              "      <td>129734.959196</td>\n",
              "      <td>0.488879</td>\n",
              "      <td>0.792176</td>\n",
              "      <td>0.522766</td>\n",
              "      <td>9.235160</td>\n",
              "      <td>1.126473</td>\n",
              "      <td>1.197675</td>\n",
              "      <td>1.191685</td>\n",
              "      <td>1.168107</td>\n",
              "      <td>1.132949</td>\n",
              "      <td>1.152394</td>\n",
              "      <td>73400.840274</td>\n",
              "      <td>70923.493353</td>\n",
              "      <td>6.926506e+04</td>\n",
              "      <td>64069.494705</td>\n",
              "      <td>60635.882129</td>\n",
              "      <td>59281.986863</td>\n",
              "      <td>17084.401034</td>\n",
              "      <td>2.428412e+04</td>\n",
              "      <td>18242.618988</td>\n",
              "      <td>16304.718844</td>\n",
              "      <td>15619.425964</td>\n",
              "      <td>17458.604219</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>10000.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>21.000000</td>\n",
              "      <td>-2.000000</td>\n",
              "      <td>-2.000000</td>\n",
              "      <td>-2.000000</td>\n",
              "      <td>-2.000000</td>\n",
              "      <td>-2.000000</td>\n",
              "      <td>-2.000000</td>\n",
              "      <td>-165580.000000</td>\n",
              "      <td>-69777.000000</td>\n",
              "      <td>-1.572640e+05</td>\n",
              "      <td>-170000.000000</td>\n",
              "      <td>-81334.000000</td>\n",
              "      <td>-209051.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>7452.500000</td>\n",
              "      <td>50000.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>28.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>3537.000000</td>\n",
              "      <td>2989.750000</td>\n",
              "      <td>2.699500e+03</td>\n",
              "      <td>2329.000000</td>\n",
              "      <td>1763.000000</td>\n",
              "      <td>1271.750000</td>\n",
              "      <td>1000.000000</td>\n",
              "      <td>8.615000e+02</td>\n",
              "      <td>390.000000</td>\n",
              "      <td>285.750000</td>\n",
              "      <td>240.750000</td>\n",
              "      <td>112.750000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>15061.500000</td>\n",
              "      <td>140000.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>34.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>22321.500000</td>\n",
              "      <td>21140.500000</td>\n",
              "      <td>2.005000e+04</td>\n",
              "      <td>19010.000000</td>\n",
              "      <td>18085.000000</td>\n",
              "      <td>17108.500000</td>\n",
              "      <td>2100.000000</td>\n",
              "      <td>2.007000e+03</td>\n",
              "      <td>1800.000000</td>\n",
              "      <td>1500.000000</td>\n",
              "      <td>1500.000000</td>\n",
              "      <td>1500.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>22509.250000</td>\n",
              "      <td>240000.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>42.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>66377.000000</td>\n",
              "      <td>63035.250000</td>\n",
              "      <td>5.952925e+04</td>\n",
              "      <td>53927.750000</td>\n",
              "      <td>50007.500000</td>\n",
              "      <td>49101.750000</td>\n",
              "      <td>5005.000000</td>\n",
              "      <td>5.000000e+03</td>\n",
              "      <td>4500.000000</td>\n",
              "      <td>4000.000000</td>\n",
              "      <td>4021.000000</td>\n",
              "      <td>4000.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>29999.000000</td>\n",
              "      <td>1000000.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>79.000000</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>964511.000000</td>\n",
              "      <td>983931.000000</td>\n",
              "      <td>1.664089e+06</td>\n",
              "      <td>891586.000000</td>\n",
              "      <td>927171.000000</td>\n",
              "      <td>961664.000000</td>\n",
              "      <td>873552.000000</td>\n",
              "      <td>1.684259e+06</td>\n",
              "      <td>896040.000000</td>\n",
              "      <td>621000.000000</td>\n",
              "      <td>426529.000000</td>\n",
              "      <td>527143.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 ID       LIMIT_BAL  ...       PAY_AMT5       PAY_AMT6\n",
              "count  24000.000000    24000.000000  ...   24000.000000   24000.000000\n",
              "mean   15010.821708   167226.653333  ...    4818.849250    5159.462125\n",
              "std     8680.406114   129734.959196  ...   15619.425964   17458.604219\n",
              "min        1.000000    10000.000000  ...       0.000000       0.000000\n",
              "25%     7452.500000    50000.000000  ...     240.750000     112.750000\n",
              "50%    15061.500000   140000.000000  ...    1500.000000    1500.000000\n",
              "75%    22509.250000   240000.000000  ...    4021.000000    4000.000000\n",
              "max    29999.000000  1000000.000000  ...  426529.000000  527143.000000\n",
              "\n",
              "[8 rows x 24 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 450
        },
        "id": "yyThB2cgAU8m",
        "outputId": "aa23d86e-fbda-4449-fbf0-083c441c5085"
      },
      "source": [
        "Y_train.info()\n",
        "Y_train.describe()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 24000 entries, 0 to 23999\n",
            "Data columns (total 2 columns):\n",
            " #   Column                      Non-Null Count  Dtype\n",
            "---  ------                      --------------  -----\n",
            " 0   ID                          24000 non-null  int64\n",
            " 1   default.payment.next.month  24000 non-null  int64\n",
            "dtypes: int64(2)\n",
            "memory usage: 375.1 KB\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>default.payment.next.month</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>24000.000000</td>\n",
              "      <td>24000.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>15010.821708</td>\n",
              "      <td>0.221792</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>8680.406114</td>\n",
              "      <td>0.415460</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>7452.500000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>15061.500000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>22509.250000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>29999.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 ID  default.payment.next.month\n",
              "count  24000.000000                24000.000000\n",
              "mean   15010.821708                    0.221792\n",
              "std     8680.406114                    0.415460\n",
              "min        1.000000                    0.000000\n",
              "25%     7452.500000                    0.000000\n",
              "50%    15061.500000                    0.000000\n",
              "75%    22509.250000                    0.000000\n",
              "max    29999.000000                    1.000000"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 844
        },
        "id": "onqcLNfTAeWj",
        "outputId": "d0b6d88f-835e-4867-9fa1-86c52a44e958"
      },
      "source": [
        "Test.info()\n",
        "Test.describe()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 6000 entries, 0 to 5999\n",
            "Data columns (total 24 columns):\n",
            " #   Column     Non-Null Count  Dtype  \n",
            "---  ------     --------------  -----  \n",
            " 0   ID         6000 non-null   int64  \n",
            " 1   LIMIT_BAL  6000 non-null   float64\n",
            " 2   SEX        6000 non-null   int64  \n",
            " 3   EDUCATION  6000 non-null   int64  \n",
            " 4   MARRIAGE   6000 non-null   int64  \n",
            " 5   AGE        6000 non-null   int64  \n",
            " 6   PAY_0      6000 non-null   int64  \n",
            " 7   PAY_2      6000 non-null   int64  \n",
            " 8   PAY_3      6000 non-null   int64  \n",
            " 9   PAY_4      6000 non-null   int64  \n",
            " 10  PAY_5      6000 non-null   int64  \n",
            " 11  PAY_6      6000 non-null   int64  \n",
            " 12  BILL_AMT1  6000 non-null   float64\n",
            " 13  BILL_AMT2  6000 non-null   float64\n",
            " 14  BILL_AMT3  6000 non-null   float64\n",
            " 15  BILL_AMT4  6000 non-null   float64\n",
            " 16  BILL_AMT5  6000 non-null   float64\n",
            " 17  BILL_AMT6  6000 non-null   float64\n",
            " 18  PAY_AMT1   6000 non-null   float64\n",
            " 19  PAY_AMT2   6000 non-null   float64\n",
            " 20  PAY_AMT3   6000 non-null   float64\n",
            " 21  PAY_AMT4   6000 non-null   float64\n",
            " 22  PAY_AMT5   6000 non-null   float64\n",
            " 23  PAY_AMT6   6000 non-null   float64\n",
            "dtypes: float64(13), int64(11)\n",
            "memory usage: 1.1 MB\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>LIMIT_BAL</th>\n",
              "      <th>SEX</th>\n",
              "      <th>EDUCATION</th>\n",
              "      <th>MARRIAGE</th>\n",
              "      <th>AGE</th>\n",
              "      <th>PAY_0</th>\n",
              "      <th>PAY_2</th>\n",
              "      <th>PAY_3</th>\n",
              "      <th>PAY_4</th>\n",
              "      <th>PAY_5</th>\n",
              "      <th>PAY_6</th>\n",
              "      <th>BILL_AMT1</th>\n",
              "      <th>BILL_AMT2</th>\n",
              "      <th>BILL_AMT3</th>\n",
              "      <th>BILL_AMT4</th>\n",
              "      <th>BILL_AMT5</th>\n",
              "      <th>BILL_AMT6</th>\n",
              "      <th>PAY_AMT1</th>\n",
              "      <th>PAY_AMT2</th>\n",
              "      <th>PAY_AMT3</th>\n",
              "      <th>PAY_AMT4</th>\n",
              "      <th>PAY_AMT5</th>\n",
              "      <th>PAY_AMT6</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>6000.000000</td>\n",
              "      <td>6000.000000</td>\n",
              "      <td>6000.000000</td>\n",
              "      <td>6000.000000</td>\n",
              "      <td>6000.000000</td>\n",
              "      <td>6000.000000</td>\n",
              "      <td>6000.000000</td>\n",
              "      <td>6000.000000</td>\n",
              "      <td>6000.000000</td>\n",
              "      <td>6000.000000</td>\n",
              "      <td>6000.000000</td>\n",
              "      <td>6000.000000</td>\n",
              "      <td>6000.000000</td>\n",
              "      <td>6000.000000</td>\n",
              "      <td>6000.000000</td>\n",
              "      <td>6000.000000</td>\n",
              "      <td>6000.000000</td>\n",
              "      <td>6000.000000</td>\n",
              "      <td>6000.000000</td>\n",
              "      <td>6000.000000</td>\n",
              "      <td>6000.000000</td>\n",
              "      <td>6000.000000</td>\n",
              "      <td>6000.000000</td>\n",
              "      <td>6000.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>14959.213167</td>\n",
              "      <td>168515.000000</td>\n",
              "      <td>1.599000</td>\n",
              "      <td>1.849667</td>\n",
              "      <td>1.553667</td>\n",
              "      <td>35.450000</td>\n",
              "      <td>-0.016833</td>\n",
              "      <td>-0.143333</td>\n",
              "      <td>-0.158333</td>\n",
              "      <td>-0.221667</td>\n",
              "      <td>-0.270333</td>\n",
              "      <td>-0.300500</td>\n",
              "      <td>52406.780833</td>\n",
              "      <td>50236.293833</td>\n",
              "      <td>48037.445500</td>\n",
              "      <td>44260.616167</td>\n",
              "      <td>40955.672833</td>\n",
              "      <td>39304.640167</td>\n",
              "      <td>5634.596333</td>\n",
              "      <td>5761.414167</td>\n",
              "      <td>5095.421500</td>\n",
              "      <td>4606.995833</td>\n",
              "      <td>4721.541167</td>\n",
              "      <td>5439.664333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>8580.495129</td>\n",
              "      <td>129804.158748</td>\n",
              "      <td>0.490142</td>\n",
              "      <td>0.783051</td>\n",
              "      <td>0.518811</td>\n",
              "      <td>9.149232</td>\n",
              "      <td>1.113144</td>\n",
              "      <td>1.195280</td>\n",
              "      <td>1.217447</td>\n",
              "      <td>1.173354</td>\n",
              "      <td>1.134226</td>\n",
              "      <td>1.140358</td>\n",
              "      <td>74562.970408</td>\n",
              "      <td>72162.483421</td>\n",
              "      <td>69682.063894</td>\n",
              "      <td>65371.581858</td>\n",
              "      <td>61438.887878</td>\n",
              "      <td>60633.455058</td>\n",
              "      <td>14291.261596</td>\n",
              "      <td>17191.910749</td>\n",
              "      <td>14794.321136</td>\n",
              "      <td>12796.017970</td>\n",
              "      <td>13830.882694</td>\n",
              "      <td>18999.354760</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>7.000000</td>\n",
              "      <td>10000.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>21.000000</td>\n",
              "      <td>-2.000000</td>\n",
              "      <td>-2.000000</td>\n",
              "      <td>-2.000000</td>\n",
              "      <td>-2.000000</td>\n",
              "      <td>-2.000000</td>\n",
              "      <td>-2.000000</td>\n",
              "      <td>-14386.000000</td>\n",
              "      <td>-13543.000000</td>\n",
              "      <td>-11925.000000</td>\n",
              "      <td>-9157.000000</td>\n",
              "      <td>-61372.000000</td>\n",
              "      <td>-339603.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>7643.250000</td>\n",
              "      <td>50000.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>28.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>3713.500000</td>\n",
              "      <td>2932.750000</td>\n",
              "      <td>2598.750000</td>\n",
              "      <td>2314.250000</td>\n",
              "      <td>1764.250000</td>\n",
              "      <td>1159.750000</td>\n",
              "      <td>1000.000000</td>\n",
              "      <td>750.000000</td>\n",
              "      <td>399.250000</td>\n",
              "      <td>316.000000</td>\n",
              "      <td>279.000000</td>\n",
              "      <td>132.750000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>14786.500000</td>\n",
              "      <td>140000.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>34.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>22719.500000</td>\n",
              "      <td>21352.000000</td>\n",
              "      <td>20293.500000</td>\n",
              "      <td>19185.000000</td>\n",
              "      <td>18163.500000</td>\n",
              "      <td>16823.000000</td>\n",
              "      <td>2200.000000</td>\n",
              "      <td>2021.500000</td>\n",
              "      <td>1893.500000</td>\n",
              "      <td>1567.000000</td>\n",
              "      <td>1504.000000</td>\n",
              "      <td>1502.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>22437.750000</td>\n",
              "      <td>240000.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>41.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>69965.250000</td>\n",
              "      <td>67077.750000</td>\n",
              "      <td>63667.500000</td>\n",
              "      <td>56358.250000</td>\n",
              "      <td>50754.000000</td>\n",
              "      <td>49584.750000</td>\n",
              "      <td>5017.250000</td>\n",
              "      <td>5000.000000</td>\n",
              "      <td>4575.750000</td>\n",
              "      <td>4200.000000</td>\n",
              "      <td>4100.000000</td>\n",
              "      <td>4200.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>30000.000000</td>\n",
              "      <td>800000.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>72.000000</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>7.000000</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>7.000000</td>\n",
              "      <td>7.000000</td>\n",
              "      <td>7.000000</td>\n",
              "      <td>653062.000000</td>\n",
              "      <td>671563.000000</td>\n",
              "      <td>693131.000000</td>\n",
              "      <td>706864.000000</td>\n",
              "      <td>489200.000000</td>\n",
              "      <td>527566.000000</td>\n",
              "      <td>302000.000000</td>\n",
              "      <td>384986.000000</td>\n",
              "      <td>344261.000000</td>\n",
              "      <td>330982.000000</td>\n",
              "      <td>303512.000000</td>\n",
              "      <td>528666.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 ID      LIMIT_BAL  ...       PAY_AMT5       PAY_AMT6\n",
              "count   6000.000000    6000.000000  ...    6000.000000    6000.000000\n",
              "mean   14959.213167  168515.000000  ...    4721.541167    5439.664333\n",
              "std     8580.495129  129804.158748  ...   13830.882694   18999.354760\n",
              "min        7.000000   10000.000000  ...       0.000000       0.000000\n",
              "25%     7643.250000   50000.000000  ...     279.000000     132.750000\n",
              "50%    14786.500000  140000.000000  ...    1504.000000    1502.000000\n",
              "75%    22437.750000  240000.000000  ...    4100.000000    4200.000000\n",
              "max    30000.000000  800000.000000  ...  303512.000000  528666.000000\n",
              "\n",
              "[8 rows x 24 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "czDUPIcdApsg"
      },
      "source": [
        "Noto che i valori non sono tutti dello stesso tipo, sarà quindi necessario portarli tutti in float64 per utilizzarli al meglio. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O8XDFP1tFwKT"
      },
      "source": [
        "Inoltre, in accordo con il Data Dictionary fornito si può notare che nelle colonne **EDUCATION** e **MARRIAGE** il valore minimo presente è zero anche se il valore minimo che tale colonna può assumere è uno: per questo motivo, nella fase di preparazione dei dati, porteremo tutti gli 0 per **EDUCATION** a 6 (unknown) e per **MARRIAGE** a 3 (other). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0jxzg1uVF_ZU",
        "outputId": "c9375136-a2a6-4d30-f6f5-37ddeb052d1f"
      },
      "source": [
        "print(X_train['EDUCATION'].value_counts())\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2    11186\n",
            "1     8481\n",
            "3     3959\n",
            "5      224\n",
            "4       97\n",
            "6       43\n",
            "0       10\n",
            "Name: EDUCATION, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j-ywdZ3LGyWl",
        "outputId": "d79a5259-3be4-4baa-e245-d77e6f203914"
      },
      "source": [
        "print(X_train['MARRIAGE'].value_counts())"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2    12747\n",
            "1    10942\n",
            "3      266\n",
            "0       45\n",
            "Name: MARRIAGE, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34FZwQCLFykf"
      },
      "source": [
        "Controllo quindi se il target sia sbilanciato."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UXDdH6K0Cprt",
        "outputId": "3a6e66ec-d6fe-451b-cac2-2b010e01f507"
      },
      "source": [
        "Y_train['default.payment.next.month'].value_counts()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    18677\n",
              "1     5323\n",
              "Name: default.payment.next.month, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NVpy2Xl7Cx5u"
      },
      "source": [
        "Noto che il dataset è profondamente sbilanciata, troviamo che la classe di maggioranza è **default.payment.next.month = 0** sarà quindi necessario bilanciarla."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PjWrQr5vWTTG"
      },
      "source": [
        "## Preparing the data\n",
        "\n",
        "describe the choice made during the preprocessing operations, also taking into account the previous considerations during the data inspection."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yj5NRMqgm-DC"
      },
      "source": [
        "Per prepare in modo più efficiente i dati riunisco **X_train** e **Y_train** in \n",
        "un unico dataset **df** e su esso eseguo il bilanciamento del target."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A1HvWP7InC3u"
      },
      "source": [
        "df = pd.merge(X_train, Y_train, on='ID')"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        },
        "id": "jP8g_mvjmv3F",
        "outputId": "1540c2b7-805a-405e-f841-5c2ad2c31927"
      },
      "source": [
        "df.describe()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>LIMIT_BAL</th>\n",
              "      <th>SEX</th>\n",
              "      <th>EDUCATION</th>\n",
              "      <th>MARRIAGE</th>\n",
              "      <th>AGE</th>\n",
              "      <th>PAY_0</th>\n",
              "      <th>PAY_2</th>\n",
              "      <th>PAY_3</th>\n",
              "      <th>PAY_4</th>\n",
              "      <th>PAY_5</th>\n",
              "      <th>PAY_6</th>\n",
              "      <th>BILL_AMT1</th>\n",
              "      <th>BILL_AMT2</th>\n",
              "      <th>BILL_AMT3</th>\n",
              "      <th>BILL_AMT4</th>\n",
              "      <th>BILL_AMT5</th>\n",
              "      <th>BILL_AMT6</th>\n",
              "      <th>PAY_AMT1</th>\n",
              "      <th>PAY_AMT2</th>\n",
              "      <th>PAY_AMT3</th>\n",
              "      <th>PAY_AMT4</th>\n",
              "      <th>PAY_AMT5</th>\n",
              "      <th>PAY_AMT6</th>\n",
              "      <th>default.payment.next.month</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>24000.000000</td>\n",
              "      <td>24000.000000</td>\n",
              "      <td>24000.000000</td>\n",
              "      <td>24000.000000</td>\n",
              "      <td>24000.000000</td>\n",
              "      <td>24000.000000</td>\n",
              "      <td>24000.000000</td>\n",
              "      <td>24000.000000</td>\n",
              "      <td>24000.000000</td>\n",
              "      <td>24000.000000</td>\n",
              "      <td>24000.000000</td>\n",
              "      <td>24000.000000</td>\n",
              "      <td>24000.000000</td>\n",
              "      <td>24000.000000</td>\n",
              "      <td>2.400000e+04</td>\n",
              "      <td>24000.000000</td>\n",
              "      <td>24000.000000</td>\n",
              "      <td>24000.000000</td>\n",
              "      <td>24000.000000</td>\n",
              "      <td>2.400000e+04</td>\n",
              "      <td>24000.000000</td>\n",
              "      <td>24000.000000</td>\n",
              "      <td>24000.000000</td>\n",
              "      <td>24000.000000</td>\n",
              "      <td>24000.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>15010.821708</td>\n",
              "      <td>167226.653333</td>\n",
              "      <td>1.604917</td>\n",
              "      <td>1.854000</td>\n",
              "      <td>1.551417</td>\n",
              "      <td>35.494375</td>\n",
              "      <td>-0.016667</td>\n",
              "      <td>-0.131375</td>\n",
              "      <td>-0.168167</td>\n",
              "      <td>-0.220417</td>\n",
              "      <td>-0.265167</td>\n",
              "      <td>-0.288750</td>\n",
              "      <td>50927.468417</td>\n",
              "      <td>48914.770500</td>\n",
              "      <td>4.675708e+04</td>\n",
              "      <td>43013.532167</td>\n",
              "      <td>40150.333000</td>\n",
              "      <td>38763.540458</td>\n",
              "      <td>5670.826542</td>\n",
              "      <td>5.961101e+03</td>\n",
              "      <td>5258.246500</td>\n",
              "      <td>4880.847125</td>\n",
              "      <td>4818.849250</td>\n",
              "      <td>5159.462125</td>\n",
              "      <td>0.221792</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>8680.406114</td>\n",
              "      <td>129734.959196</td>\n",
              "      <td>0.488879</td>\n",
              "      <td>0.792176</td>\n",
              "      <td>0.522766</td>\n",
              "      <td>9.235160</td>\n",
              "      <td>1.126473</td>\n",
              "      <td>1.197675</td>\n",
              "      <td>1.191685</td>\n",
              "      <td>1.168107</td>\n",
              "      <td>1.132949</td>\n",
              "      <td>1.152394</td>\n",
              "      <td>73400.840274</td>\n",
              "      <td>70923.493353</td>\n",
              "      <td>6.926506e+04</td>\n",
              "      <td>64069.494705</td>\n",
              "      <td>60635.882129</td>\n",
              "      <td>59281.986863</td>\n",
              "      <td>17084.401034</td>\n",
              "      <td>2.428412e+04</td>\n",
              "      <td>18242.618988</td>\n",
              "      <td>16304.718844</td>\n",
              "      <td>15619.425964</td>\n",
              "      <td>17458.604219</td>\n",
              "      <td>0.415460</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>10000.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>21.000000</td>\n",
              "      <td>-2.000000</td>\n",
              "      <td>-2.000000</td>\n",
              "      <td>-2.000000</td>\n",
              "      <td>-2.000000</td>\n",
              "      <td>-2.000000</td>\n",
              "      <td>-2.000000</td>\n",
              "      <td>-165580.000000</td>\n",
              "      <td>-69777.000000</td>\n",
              "      <td>-1.572640e+05</td>\n",
              "      <td>-170000.000000</td>\n",
              "      <td>-81334.000000</td>\n",
              "      <td>-209051.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>7452.500000</td>\n",
              "      <td>50000.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>28.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>3537.000000</td>\n",
              "      <td>2989.750000</td>\n",
              "      <td>2.699500e+03</td>\n",
              "      <td>2329.000000</td>\n",
              "      <td>1763.000000</td>\n",
              "      <td>1271.750000</td>\n",
              "      <td>1000.000000</td>\n",
              "      <td>8.615000e+02</td>\n",
              "      <td>390.000000</td>\n",
              "      <td>285.750000</td>\n",
              "      <td>240.750000</td>\n",
              "      <td>112.750000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>15061.500000</td>\n",
              "      <td>140000.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>34.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>22321.500000</td>\n",
              "      <td>21140.500000</td>\n",
              "      <td>2.005000e+04</td>\n",
              "      <td>19010.000000</td>\n",
              "      <td>18085.000000</td>\n",
              "      <td>17108.500000</td>\n",
              "      <td>2100.000000</td>\n",
              "      <td>2.007000e+03</td>\n",
              "      <td>1800.000000</td>\n",
              "      <td>1500.000000</td>\n",
              "      <td>1500.000000</td>\n",
              "      <td>1500.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>22509.250000</td>\n",
              "      <td>240000.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>42.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>66377.000000</td>\n",
              "      <td>63035.250000</td>\n",
              "      <td>5.952925e+04</td>\n",
              "      <td>53927.750000</td>\n",
              "      <td>50007.500000</td>\n",
              "      <td>49101.750000</td>\n",
              "      <td>5005.000000</td>\n",
              "      <td>5.000000e+03</td>\n",
              "      <td>4500.000000</td>\n",
              "      <td>4000.000000</td>\n",
              "      <td>4021.000000</td>\n",
              "      <td>4000.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>29999.000000</td>\n",
              "      <td>1000000.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>79.000000</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>964511.000000</td>\n",
              "      <td>983931.000000</td>\n",
              "      <td>1.664089e+06</td>\n",
              "      <td>891586.000000</td>\n",
              "      <td>927171.000000</td>\n",
              "      <td>961664.000000</td>\n",
              "      <td>873552.000000</td>\n",
              "      <td>1.684259e+06</td>\n",
              "      <td>896040.000000</td>\n",
              "      <td>621000.000000</td>\n",
              "      <td>426529.000000</td>\n",
              "      <td>527143.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 ID       LIMIT_BAL  ...       PAY_AMT6  default.payment.next.month\n",
              "count  24000.000000    24000.000000  ...   24000.000000                24000.000000\n",
              "mean   15010.821708   167226.653333  ...    5159.462125                    0.221792\n",
              "std     8680.406114   129734.959196  ...   17458.604219                    0.415460\n",
              "min        1.000000    10000.000000  ...       0.000000                    0.000000\n",
              "25%     7452.500000    50000.000000  ...     112.750000                    0.000000\n",
              "50%    15061.500000   140000.000000  ...    1500.000000                    0.000000\n",
              "75%    22509.250000   240000.000000  ...    4000.000000                    0.000000\n",
              "max    29999.000000  1000000.000000  ...  527143.000000                    1.000000\n",
              "\n",
              "[8 rows x 25 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1TOLPTm4n4pd"
      },
      "source": [
        "Come precedentemente citato correggo i valori di **EDUCATION** e **MARRIAGE**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gDD1MRxao0qD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "618b3421-4fda-49a9-f4cd-edaf82a766e9"
      },
      "source": [
        "fil = (df.EDUCATION == 0)\n",
        "df.loc[fil, 'EDUCATION'] = 6\n",
        "df.EDUCATION.value_counts()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2    11186\n",
              "1     8481\n",
              "3     3959\n",
              "5      224\n",
              "4       97\n",
              "6       53\n",
              "Name: EDUCATION, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VKBTxJYYpFBA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5dab379d-ce3f-4723-ea5f-30b3f5d643a7"
      },
      "source": [
        "df.loc[df.MARRIAGE == 0, 'MARRIAGE'] = 3\n",
        "df.MARRIAGE.value_counts()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2    12747\n",
              "1    10942\n",
              "3      311\n",
              "Name: MARRIAGE, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ydVI1QaptGi"
      },
      "source": [
        "Correggo quindi il bilanciamento del target effettuando un downsampling della classe maggioritaria "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DoLAXuI-n5p6"
      },
      "source": [
        "df = df.sort_values(by=['default.payment.next.month'])"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SuLnqtxsoDp_"
      },
      "source": [
        "df = df.iloc[10000:]"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JxjMDZhmqOH9"
      },
      "source": [
        "#Downsampling\n",
        "df = df.sample(frac=1)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F2aeZB6IHf1P",
        "outputId": "ce0ef835-bee2-42d4-b7ea-0ae073d0fab0"
      },
      "source": [
        "df['default.payment.next.month'].value_counts()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    8677\n",
              "1    5323\n",
              "Name: default.payment.next.month, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M95Q5AVuHsVb"
      },
      "source": [
        "Ricostruisco quindi la divisione tra il target e le altre variabili. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "KQQMPAf_oO9z",
        "outputId": "d0cc380c-d324-4341-fd22-cb73190587ba"
      },
      "source": [
        "X_train_temp = df\n",
        "Y_train_temp = df[['ID','default.payment.next.month']]\n",
        "Y_train_temp['default.payment.next.month'].value_counts()\n",
        "Y_train_temp"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>default.payment.next.month</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1795</th>\n",
              "      <td>9323</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>636</th>\n",
              "      <td>13264</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>695</th>\n",
              "      <td>29915</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7121</th>\n",
              "      <td>21290</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11844</th>\n",
              "      <td>19252</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2401</th>\n",
              "      <td>11306</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>225</th>\n",
              "      <td>23993</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13354</th>\n",
              "      <td>4839</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22988</th>\n",
              "      <td>10358</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23870</th>\n",
              "      <td>29825</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>14000 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          ID  default.payment.next.month\n",
              "1795    9323                           0\n",
              "636    13264                           0\n",
              "695    29915                           0\n",
              "7121   21290                           0\n",
              "11844  19252                           1\n",
              "...      ...                         ...\n",
              "2401   11306                           1\n",
              "225    23993                           0\n",
              "13354   4839                           1\n",
              "22988  10358                           0\n",
              "23870  29825                           1\n",
              "\n",
              "[14000 rows x 2 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 504
        },
        "id": "YdnOTXqLn4-F",
        "outputId": "bdb7c346-ce55-4a00-c9ac-7808ea795d94"
      },
      "source": [
        "X_train_temp.drop(['ID','SEX','default.payment.next.month'],axis=1,inplace=True)\n",
        "Y_train_temp.drop(['ID'],axis=1,inplace=True)\n",
        "Y_train_temp"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py:4174: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  errors=errors,\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>default.payment.next.month</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1795</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>636</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>695</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7121</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11844</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2401</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>225</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13354</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22988</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23870</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>14000 rows × 1 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       default.payment.next.month\n",
              "1795                            0\n",
              "636                             0\n",
              "695                             0\n",
              "7121                            0\n",
              "11844                           1\n",
              "...                           ...\n",
              "2401                            1\n",
              "225                             0\n",
              "13354                           1\n",
              "22988                           0\n",
              "23870                           1\n",
              "\n",
              "[14000 rows x 1 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WrqFike6JEfo"
      },
      "source": [
        "Divido i dati per l'addestramento e la validazione."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zl7CynwyxOvI"
      },
      "source": [
        "X_train, X_val, Y_train, Y_val = train_test_split(X_train_temp, Y_train_temp, test_size=0.2, random_state=1)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P8CmcP5BshZv",
        "outputId": "dc68f512-8abc-4a2a-9a3b-cdeb67e6e405"
      },
      "source": [
        "print(X_train.shape, Y_train.shape, X_val.shape,Y_val.shape, X_train_temp.shape)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(11200, 22) (11200, 1) (2800, 22) (2800, 1) (14000, 22)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3VXxG2eJivP"
      },
      "source": [
        "Per completare la preparazione dei dati li converto interamente in **float32** ed effettuo la normalizzazione. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n2EfQG0heW1M"
      },
      "source": [
        "X_train = X_train.astype('float32')\n",
        "X_val = X_val.astype('float32')"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Keu9IpZnJuGk"
      },
      "source": [
        "scaler = preprocessing.MinMaxScaler((0,1))\n",
        "scaler.fit(X_train_temp)\n",
        "\n",
        "XX_train = scaler.transform(X_train.values)\n",
        "XX_val  = scaler.transform(X_val.values) \n",
        "\n",
        "YY_train = Y_train.values \n",
        "YY_val  = Y_val.values "
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NfR6yLSs-RiH",
        "outputId": "5a20f3a7-e436-476d-bc3e-a0026aea74c6"
      },
      "source": [
        "print (XX_train.shape, YY_train.shape, XX_val.shape, YY_val.shape)\n"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(11200, 22) (11200, 1) (2800, 22) (2800, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eb9aljYxJbsK"
      },
      "source": [
        "## Building the network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CqWtUiPc8VLM"
      },
      "source": [
        "Definisco le metriche che mi serviranno per valutare il modello."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZMeWa6ok8TTn"
      },
      "source": [
        "from keras import backend as K\n",
        "\n",
        "def recall_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    return recall\n",
        "\n",
        "def precision_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    return precision\n",
        "\n",
        "def f1_m(y_true, y_pred):\n",
        "    precision = precision_m(y_true, y_pred)\n",
        "    recall = recall_m(y_true, y_pred)\n",
        "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v2ks_GP7AYB5"
      },
      "source": [
        "Per la costruzione del modello ho deciso di creare la mia rete neurale con tre Layer nascosti rispettivamente da 256, 128, 64 che dopo numerose prove è risulato essere il miglior numero di nodi e layer da impiegare."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQlqc_EJQAv8"
      },
      "source": [
        "L'ottimizzatore scelto è SGD con un learning rate di *0.005*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uObXI-S2P_3Q"
      },
      "source": [
        "La funzione di attivazione scelta è una di quelle standard ovvero **relu** e come funzione di attivazione del layer di output **sigmoid** perchè è più efficente dal punto di vista computazionale rispetto a **softmax**. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U4G1LYQ47Ao5"
      },
      "source": [
        "nb_classes = 1 \n",
        "\n",
        "initializer = keras.initializers.GlorotUniform(seed=1234) \n",
        "\n",
        "# Modello\n",
        "model = Sequential()\n",
        "model.add(Dense(256, input_shape=(X_train.shape[1],), activation = \"relu\", kernel_initializer=initializer))\n",
        "model.add(Dense(128, activation = \"relu\", kernel_initializer=initializer))\n",
        "model.add(Dense(64, activation = \"relu\", kernel_initializer=initializer))\n",
        "model.add(Dense(nb_classes, activation='sigmoid',kernel_initializer=initializer))\n",
        "\n",
        "from tensorflow.keras.optimizers import SGD \n",
        "model.compile(optimizer=SGD(learning_rate=0.005), loss='binary_crossentropy',metrics=['accuracy',f1_m,precision_m, recall_m],)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aObic0JKQSeN"
      },
      "source": [
        "Dopo una serie di prove ho trovato che la grandezza migliore per il **batch_size** è 128."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uR3TIBU-DNOq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00176659-61f4-4080-a6f7-f932b27254f5"
      },
      "source": [
        "n_epochs = 300\n",
        "batch_size = 128 \n",
        "\n",
        "history = model.fit(XX_train, YY_train, epochs=n_epochs, batch_size=batch_size, validation_data=(XX_val, YY_val))  "
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/300\n",
            "88/88 [==============================] - 2s 9ms/step - loss: 0.6929 - accuracy: 0.5148 - f1_m: 0.5238 - precision_m: 0.4725 - recall_m: 0.7338 - val_loss: 0.6842 - val_accuracy: 0.6721 - val_f1_m: 0.3698 - val_precision_m: 0.6382 - val_recall_m: 0.2627\n",
            "Epoch 2/300\n",
            "88/88 [==============================] - 0s 5ms/step - loss: 0.6793 - accuracy: 0.6342 - f1_m: 0.1197 - precision_m: 0.5428 - recall_m: 0.0732 - val_loss: 0.6736 - val_accuracy: 0.6304 - val_f1_m: 0.0195 - val_precision_m: 0.3636 - val_recall_m: 0.0100\n",
            "Epoch 3/300\n",
            "88/88 [==============================] - 0s 5ms/step - loss: 0.6718 - accuracy: 0.6187 - f1_m: 0.0065 - precision_m: 0.1477 - recall_m: 0.0033 - val_loss: 0.6672 - val_accuracy: 0.6282 - val_f1_m: 0.0042 - val_precision_m: 0.0909 - val_recall_m: 0.0022\n",
            "Epoch 4/300\n",
            "88/88 [==============================] - 0s 5ms/step - loss: 0.6670 - accuracy: 0.6179 - f1_m: 0.0020 - precision_m: 0.0455 - recall_m: 0.0010 - val_loss: 0.6627 - val_accuracy: 0.6275 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
            "Epoch 5/300\n",
            "88/88 [==============================] - 0s 5ms/step - loss: 0.6635 - accuracy: 0.6179 - f1_m: 4.2882e-04 - precision_m: 0.0114 - recall_m: 2.1853e-04 - val_loss: 0.6593 - val_accuracy: 0.6275 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
            "Epoch 6/300\n",
            "88/88 [==============================] - 0s 5ms/step - loss: 0.6607 - accuracy: 0.6179 - f1_m: 4.1322e-04 - precision_m: 0.0114 - recall_m: 2.1044e-04 - val_loss: 0.6565 - val_accuracy: 0.6275 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
            "Epoch 7/300\n",
            "88/88 [==============================] - 0s 5ms/step - loss: 0.6584 - accuracy: 0.6178 - f1_m: 4.4563e-04 - precision_m: 0.0114 - recall_m: 2.2727e-04 - val_loss: 0.6542 - val_accuracy: 0.6275 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
            "Epoch 8/300\n",
            "88/88 [==============================] - 0s 5ms/step - loss: 0.6564 - accuracy: 0.6178 - f1_m: 3.5511e-04 - precision_m: 0.0114 - recall_m: 1.8038e-04 - val_loss: 0.6522 - val_accuracy: 0.6275 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
            "Epoch 9/300\n",
            "88/88 [==============================] - 0s 5ms/step - loss: 0.6547 - accuracy: 0.6180 - f1_m: 0.0018 - precision_m: 0.0455 - recall_m: 9.3507e-04 - val_loss: 0.6504 - val_accuracy: 0.6271 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
            "Epoch 10/300\n",
            "88/88 [==============================] - 0s 5ms/step - loss: 0.6530 - accuracy: 0.6184 - f1_m: 0.0039 - precision_m: 0.0795 - recall_m: 0.0020 - val_loss: 0.6488 - val_accuracy: 0.6271 - val_f1_m: 0.0023 - val_precision_m: 0.0455 - val_recall_m: 0.0012\n",
            "Epoch 11/300\n",
            "88/88 [==============================] - 0s 5ms/step - loss: 0.6514 - accuracy: 0.6185 - f1_m: 0.0041 - precision_m: 0.1023 - recall_m: 0.0021 - val_loss: 0.6472 - val_accuracy: 0.6275 - val_f1_m: 0.0042 - val_precision_m: 0.0909 - val_recall_m: 0.0022\n",
            "Epoch 12/300\n",
            "88/88 [==============================] - 0s 5ms/step - loss: 0.6499 - accuracy: 0.6195 - f1_m: 0.0090 - precision_m: 0.2273 - recall_m: 0.0046 - val_loss: 0.6455 - val_accuracy: 0.6282 - val_f1_m: 0.0101 - val_precision_m: 0.2273 - val_recall_m: 0.0052\n",
            "Epoch 13/300\n",
            "88/88 [==============================] - 0s 5ms/step - loss: 0.6483 - accuracy: 0.6200 - f1_m: 0.0123 - precision_m: 0.2614 - recall_m: 0.0063 - val_loss: 0.6440 - val_accuracy: 0.6289 - val_f1_m: 0.0137 - val_precision_m: 0.3182 - val_recall_m: 0.0070\n",
            "Epoch 14/300\n",
            "88/88 [==============================] - 0s 5ms/step - loss: 0.6468 - accuracy: 0.6209 - f1_m: 0.0173 - precision_m: 0.3011 - recall_m: 0.0090 - val_loss: 0.6424 - val_accuracy: 0.6289 - val_f1_m: 0.0156 - val_precision_m: 0.3636 - val_recall_m: 0.0080\n",
            "Epoch 15/300\n",
            "88/88 [==============================] - 0s 5ms/step - loss: 0.6451 - accuracy: 0.6220 - f1_m: 0.0248 - precision_m: 0.4792 - recall_m: 0.0128 - val_loss: 0.6407 - val_accuracy: 0.6304 - val_f1_m: 0.0234 - val_precision_m: 0.4773 - val_recall_m: 0.0121\n",
            "Epoch 16/300\n",
            "88/88 [==============================] - 0s 5ms/step - loss: 0.6433 - accuracy: 0.6229 - f1_m: 0.0325 - precision_m: 0.4981 - recall_m: 0.0169 - val_loss: 0.6389 - val_accuracy: 0.6311 - val_f1_m: 0.0289 - val_precision_m: 0.5303 - val_recall_m: 0.0149\n",
            "Epoch 17/300\n",
            "88/88 [==============================] - 0s 5ms/step - loss: 0.6415 - accuracy: 0.6237 - f1_m: 0.0367 - precision_m: 0.5451 - recall_m: 0.0193 - val_loss: 0.6373 - val_accuracy: 0.6311 - val_f1_m: 0.0350 - val_precision_m: 0.5545 - val_recall_m: 0.0183\n",
            "Epoch 18/300\n",
            "88/88 [==============================] - 0s 5ms/step - loss: 0.6398 - accuracy: 0.6271 - f1_m: 0.0573 - precision_m: 0.6703 - recall_m: 0.0304 - val_loss: 0.6355 - val_accuracy: 0.6339 - val_f1_m: 0.0504 - val_precision_m: 0.6833 - val_recall_m: 0.0265\n",
            "Epoch 19/300\n",
            "88/88 [==============================] - 0s 5ms/step - loss: 0.6380 - accuracy: 0.6292 - f1_m: 0.0695 - precision_m: 0.7551 - recall_m: 0.0371 - val_loss: 0.6338 - val_accuracy: 0.6393 - val_f1_m: 0.0810 - val_precision_m: 0.7553 - val_recall_m: 0.0437\n",
            "Epoch 20/300\n",
            "88/88 [==============================] - 0s 5ms/step - loss: 0.6362 - accuracy: 0.6347 - f1_m: 0.1010 - precision_m: 0.7928 - recall_m: 0.0547 - val_loss: 0.6321 - val_accuracy: 0.6450 - val_f1_m: 0.1136 - val_precision_m: 0.8185 - val_recall_m: 0.0623\n",
            "Epoch 21/300\n",
            "88/88 [==============================] - 0s 5ms/step - loss: 0.6344 - accuracy: 0.6412 - f1_m: 0.1341 - precision_m: 0.8506 - recall_m: 0.0741 - val_loss: 0.6304 - val_accuracy: 0.6486 - val_f1_m: 0.1453 - val_precision_m: 0.7790 - val_recall_m: 0.0819\n",
            "Epoch 22/300\n",
            "88/88 [==============================] - 0s 5ms/step - loss: 0.6326 - accuracy: 0.6497 - f1_m: 0.1745 - precision_m: 0.8645 - recall_m: 0.0990 - val_loss: 0.6286 - val_accuracy: 0.6557 - val_f1_m: 0.1835 - val_precision_m: 0.7862 - val_recall_m: 0.1052\n",
            "Epoch 23/300\n",
            "88/88 [==============================] - 0s 5ms/step - loss: 0.6307 - accuracy: 0.6596 - f1_m: 0.2232 - precision_m: 0.8627 - recall_m: 0.1301 - val_loss: 0.6268 - val_accuracy: 0.6611 - val_f1_m: 0.2146 - val_precision_m: 0.7704 - val_recall_m: 0.1264\n",
            "Epoch 24/300\n",
            "88/88 [==============================] - 0s 5ms/step - loss: 0.6289 - accuracy: 0.6657 - f1_m: 0.2542 - precision_m: 0.8539 - recall_m: 0.1516 - val_loss: 0.6251 - val_accuracy: 0.6643 - val_f1_m: 0.2390 - val_precision_m: 0.7573 - val_recall_m: 0.1442\n",
            "Epoch 25/300\n",
            "88/88 [==============================] - 0s 5ms/step - loss: 0.6270 - accuracy: 0.6703 - f1_m: 0.2786 - precision_m: 0.8411 - recall_m: 0.1701 - val_loss: 0.6233 - val_accuracy: 0.6718 - val_f1_m: 0.2731 - val_precision_m: 0.7713 - val_recall_m: 0.1684\n",
            "Epoch 26/300\n",
            "88/88 [==============================] - 0s 5ms/step - loss: 0.6251 - accuracy: 0.6748 - f1_m: 0.3002 - precision_m: 0.8424 - recall_m: 0.1846 - val_loss: 0.6216 - val_accuracy: 0.6746 - val_f1_m: 0.2927 - val_precision_m: 0.7636 - val_recall_m: 0.1845\n",
            "Epoch 27/300\n",
            "88/88 [==============================] - 0s 5ms/step - loss: 0.6233 - accuracy: 0.6775 - f1_m: 0.3154 - precision_m: 0.8279 - recall_m: 0.1981 - val_loss: 0.6198 - val_accuracy: 0.6811 - val_f1_m: 0.3212 - val_precision_m: 0.7626 - val_recall_m: 0.2067\n",
            "Epoch 28/300\n",
            "88/88 [==============================] - 0s 5ms/step - loss: 0.6214 - accuracy: 0.6826 - f1_m: 0.3387 - precision_m: 0.8297 - recall_m: 0.2158 - val_loss: 0.6181 - val_accuracy: 0.6839 - val_f1_m: 0.3392 - val_precision_m: 0.7564 - val_recall_m: 0.2215\n",
            "Epoch 29/300\n",
            "88/88 [==============================] - 0s 5ms/step - loss: 0.6195 - accuracy: 0.6860 - f1_m: 0.3543 - precision_m: 0.8205 - recall_m: 0.2289 - val_loss: 0.6163 - val_accuracy: 0.6875 - val_f1_m: 0.3548 - val_precision_m: 0.7627 - val_recall_m: 0.2343\n",
            "Epoch 30/300\n",
            "88/88 [==============================] - 0s 5ms/step - loss: 0.6177 - accuracy: 0.6879 - f1_m: 0.3618 - precision_m: 0.8140 - recall_m: 0.2356 - val_loss: 0.6146 - val_accuracy: 0.6907 - val_f1_m: 0.3697 - val_precision_m: 0.7598 - val_recall_m: 0.2476\n",
            "Epoch 31/300\n",
            "88/88 [==============================] - 0s 5ms/step - loss: 0.6158 - accuracy: 0.6913 - f1_m: 0.3819 - precision_m: 0.8093 - recall_m: 0.2532 - val_loss: 0.6128 - val_accuracy: 0.6929 - val_f1_m: 0.3777 - val_precision_m: 0.7621 - val_recall_m: 0.2543\n",
            "Epoch 32/300\n",
            "88/88 [==============================] - 0s 5ms/step - loss: 0.6140 - accuracy: 0.6952 - f1_m: 0.3971 - precision_m: 0.8044 - recall_m: 0.2674 - val_loss: 0.6111 - val_accuracy: 0.6950 - val_f1_m: 0.3879 - val_precision_m: 0.7622 - val_recall_m: 0.2635\n",
            "Epoch 33/300\n",
            "88/88 [==============================] - 0s 5ms/step - loss: 0.6122 - accuracy: 0.6963 - f1_m: 0.4023 - precision_m: 0.8054 - recall_m: 0.2709 - val_loss: 0.6095 - val_accuracy: 0.6986 - val_f1_m: 0.4052 - val_precision_m: 0.7617 - val_recall_m: 0.2800\n",
            "Epoch 34/300\n",
            "88/88 [==============================] - 0s 5ms/step - loss: 0.6104 - accuracy: 0.7013 - f1_m: 0.4241 - precision_m: 0.8006 - recall_m: 0.2922 - val_loss: 0.6079 - val_accuracy: 0.7011 - val_f1_m: 0.4201 - val_precision_m: 0.7567 - val_recall_m: 0.2951\n",
            "Epoch 35/300\n",
            "88/88 [==============================] - 0s 5ms/step - loss: 0.6086 - accuracy: 0.7013 - f1_m: 0.4250 - precision_m: 0.7972 - recall_m: 0.2932 - val_loss: 0.6064 - val_accuracy: 0.7025 - val_f1_m: 0.4279 - val_precision_m: 0.7542 - val_recall_m: 0.3030\n",
            "Epoch 36/300\n",
            "88/88 [==============================] - 0s 5ms/step - loss: 0.6070 - accuracy: 0.7051 - f1_m: 0.4397 - precision_m: 0.7951 - recall_m: 0.3074 - val_loss: 0.6049 - val_accuracy: 0.7036 - val_f1_m: 0.4338 - val_precision_m: 0.7523 - val_recall_m: 0.3086\n",
            "Epoch 37/300\n",
            "88/88 [==============================] - 0s 5ms/step - loss: 0.6053 - accuracy: 0.7072 - f1_m: 0.4509 - precision_m: 0.7913 - recall_m: 0.3191 - val_loss: 0.6034 - val_accuracy: 0.7043 - val_f1_m: 0.4390 - val_precision_m: 0.7496 - val_recall_m: 0.3145\n",
            "Epoch 38/300\n",
            "88/88 [==============================] - 0s 5ms/step - loss: 0.6038 - accuracy: 0.7081 - f1_m: 0.4570 - precision_m: 0.7869 - recall_m: 0.3252 - val_loss: 0.6019 - val_accuracy: 0.7043 - val_f1_m: 0.4419 - val_precision_m: 0.7449 - val_recall_m: 0.3183\n",
            "Epoch 39/300\n",
            "88/88 [==============================] - 0s 5ms/step - loss: 0.6023 - accuracy: 0.7094 - f1_m: 0.4609 - precision_m: 0.7865 - recall_m: 0.3292 - val_loss: 0.6007 - val_accuracy: 0.7061 - val_f1_m: 0.4504 - val_precision_m: 0.7420 - val_recall_m: 0.3275\n",
            "Epoch 40/300\n",
            "88/88 [==============================] - 0s 5ms/step - loss: 0.6008 - accuracy: 0.7113 - f1_m: 0.4675 - precision_m: 0.7857 - recall_m: 0.3361 - val_loss: 0.5995 - val_accuracy: 0.7089 - val_f1_m: 0.4602 - val_precision_m: 0.7414 - val_recall_m: 0.3378\n",
            "Epoch 41/300\n",
            "88/88 [==============================] - 0s 5ms/step - loss: 0.5994 - accuracy: 0.7140 - f1_m: 0.4774 - precision_m: 0.7848 - recall_m: 0.3468 - val_loss: 0.5982 - val_accuracy: 0.7093 - val_f1_m: 0.4632 - val_precision_m: 0.7385 - val_recall_m: 0.3415\n",
            "Epoch 42/300\n",
            "88/88 [==============================] - 0s 5ms/step - loss: 0.5981 - accuracy: 0.7146 - f1_m: 0.4820 - precision_m: 0.7798 - recall_m: 0.3530 - val_loss: 0.5969 - val_accuracy: 0.7093 - val_f1_m: 0.4616 - val_precision_m: 0.7438 - val_recall_m: 0.3391\n",
            "Epoch 43/300\n",
            "88/88 [==============================] - 0s 5ms/step - loss: 0.5968 - accuracy: 0.7150 - f1_m: 0.4826 - precision_m: 0.7821 - recall_m: 0.3528 - val_loss: 0.5958 - val_accuracy: 0.7100 - val_f1_m: 0.4666 - val_precision_m: 0.7378 - val_recall_m: 0.3453\n",
            "Epoch 44/300\n",
            "88/88 [==============================] - 0s 5ms/step - loss: 0.5955 - accuracy: 0.7163 - f1_m: 0.4874 - precision_m: 0.7821 - recall_m: 0.3577 - val_loss: 0.5948 - val_accuracy: 0.7118 - val_f1_m: 0.4725 - val_precision_m: 0.7398 - val_recall_m: 0.3509\n",
            "Epoch 45/300\n",
            "88/88 [==============================] - 0s 5ms/step - loss: 0.5943 - accuracy: 0.7163 - f1_m: 0.4870 - precision_m: 0.7762 - recall_m: 0.3587 - val_loss: 0.5938 - val_accuracy: 0.7125 - val_f1_m: 0.4771 - val_precision_m: 0.7377 - val_recall_m: 0.3566\n",
            "Epoch 46/300\n",
            "88/88 [==============================] - 0s 5ms/step - loss: 0.5932 - accuracy: 0.7174 - f1_m: 0.4928 - precision_m: 0.7797 - recall_m: 0.3639 - val_loss: 0.5929 - val_accuracy: 0.7150 - val_f1_m: 0.4872 - val_precision_m: 0.7373 - val_recall_m: 0.3677\n",
            "Epoch 47/300\n",
            "88/88 [==============================] - 0s 5ms/step - loss: 0.5921 - accuracy: 0.7192 - f1_m: 0.4995 - precision_m: 0.7824 - recall_m: 0.3712 - val_loss: 0.5919 - val_accuracy: 0.7154 - val_f1_m: 0.4867 - val_precision_m: 0.7395 - val_recall_m: 0.3668\n",
            "Epoch 48/300\n",
            "88/88 [==============================] - 0s 5ms/step - loss: 0.5910 - accuracy: 0.7197 - f1_m: 0.5006 - precision_m: 0.7781 - recall_m: 0.3730 - val_loss: 0.5910 - val_accuracy: 0.7154 - val_f1_m: 0.4867 - val_precision_m: 0.7395 - val_recall_m: 0.3668\n",
            "Epoch 49/300\n",
            "88/88 [==============================] - 0s 5ms/step - loss: 0.5900 - accuracy: 0.7203 - f1_m: 0.5021 - precision_m: 0.7798 - recall_m: 0.3733 - val_loss: 0.5902 - val_accuracy: 0.7150 - val_f1_m: 0.4871 - val_precision_m: 0.7365 - val_recall_m: 0.3678\n",
            "Epoch 50/300\n",
            "88/88 [==============================] - 0s 5ms/step - loss: 0.5890 - accuracy: 0.7214 - f1_m: 0.5030 - precision_m: 0.7785 - recall_m: 0.3755 - val_loss: 0.5895 - val_accuracy: 0.7168 - val_f1_m: 0.4924 - val_precision_m: 0.7385 - val_recall_m: 0.3733\n",
            "Epoch 51/300\n",
            "88/88 [==============================] - 0s 5ms/step - loss: 0.5880 - accuracy: 0.7216 - f1_m: 0.5051 - precision_m: 0.7803 - recall_m: 0.3779 - val_loss: 0.5888 - val_accuracy: 0.7171 - val_f1_m: 0.4955 - val_precision_m: 0.7367 - val_recall_m: 0.3772\n",
            "Epoch 52/300\n",
            "88/88 [==============================] - 0s 5ms/step - loss: 0.5871 - accuracy: 0.7222 - f1_m: 0.5097 - precision_m: 0.7757 - recall_m: 0.3834 - val_loss: 0.5881 - val_accuracy: 0.7171 - val_f1_m: 0.4947 - val_precision_m: 0.7373 - val_recall_m: 0.3763\n",
            "Epoch 53/300\n",
            "88/88 [==============================] - 0s 5ms/step - loss: 0.5862 - accuracy: 0.7225 - f1_m: 0.5099 - precision_m: 0.7799 - recall_m: 0.3827 - val_loss: 0.5876 - val_accuracy: 0.7175 - val_f1_m: 0.5005 - val_precision_m: 0.7317 - val_recall_m: 0.3852\n",
            "Epoch 54/300\n",
            "88/88 [==============================] - 0s 5ms/step - loss: 0.5854 - accuracy: 0.7224 - f1_m: 0.5119 - precision_m: 0.7753 - recall_m: 0.3857 - val_loss: 0.5869 - val_accuracy: 0.7182 - val_f1_m: 0.5030 - val_precision_m: 0.7320 - val_recall_m: 0.3882\n",
            "Epoch 55/300\n",
            "88/88 [==============================] - 0s 5ms/step - loss: 0.5846 - accuracy: 0.7227 - f1_m: 0.5133 - precision_m: 0.7712 - recall_m: 0.3887 - val_loss: 0.5863 - val_accuracy: 0.7182 - val_f1_m: 0.5023 - val_precision_m: 0.7325 - val_recall_m: 0.3871\n",
            "Epoch 56/300\n",
            "88/88 [==============================] - 0s 5ms/step - loss: 0.5838 - accuracy: 0.7228 - f1_m: 0.5153 - precision_m: 0.7729 - recall_m: 0.3896 - val_loss: 0.5858 - val_accuracy: 0.7189 - val_f1_m: 0.5053 - val_precision_m: 0.7321 - val_recall_m: 0.3913\n",
            "Epoch 57/300\n",
            "88/88 [==============================] - 0s 5ms/step - loss: 0.5831 - accuracy: 0.7243 - f1_m: 0.5209 - precision_m: 0.7741 - recall_m: 0.3974 - val_loss: 0.5851 - val_accuracy: 0.7186 - val_f1_m: 0.5025 - val_precision_m: 0.7334 - val_recall_m: 0.3873\n",
            "Epoch 58/300\n",
            "88/88 [==============================] - 0s 5ms/step - loss: 0.5824 - accuracy: 0.7245 - f1_m: 0.5201 - precision_m: 0.7720 - recall_m: 0.3968 - val_loss: 0.5846 - val_accuracy: 0.7186 - val_f1_m: 0.5011 - val_precision_m: 0.7343 - val_recall_m: 0.3850\n",
            "Epoch 59/300\n",
            "88/88 [==============================] - 0s 5ms/step - loss: 0.5817 - accuracy: 0.7241 - f1_m: 0.5172 - precision_m: 0.7707 - recall_m: 0.3922 - val_loss: 0.5842 - val_accuracy: 0.7218 - val_f1_m: 0.5133 - val_precision_m: 0.7357 - val_recall_m: 0.4002\n",
            "Epoch 60/300\n",
            "88/88 [==============================] - 0s 5ms/step - loss: 0.5810 - accuracy: 0.7254 - f1_m: 0.5228 - precision_m: 0.7725 - recall_m: 0.4001 - val_loss: 0.5837 - val_accuracy: 0.7211 - val_f1_m: 0.5113 - val_precision_m: 0.7345 - val_recall_m: 0.3983\n",
            "Epoch 61/300\n",
            "88/88 [==============================] - 0s 5ms/step - loss: 0.5804 - accuracy: 0.7252 - f1_m: 0.5220 - precision_m: 0.7757 - recall_m: 0.3978 - val_loss: 0.5837 - val_accuracy: 0.7250 - val_f1_m: 0.5245 - val_precision_m: 0.7358 - val_recall_m: 0.4142\n",
            "Epoch 62/300\n",
            "88/88 [==============================] - 0s 5ms/step - loss: 0.5798 - accuracy: 0.7269 - f1_m: 0.5265 - precision_m: 0.7684 - recall_m: 0.4042 - val_loss: 0.5831 - val_accuracy: 0.7246 - val_f1_m: 0.5222 - val_precision_m: 0.7362 - val_recall_m: 0.4111\n",
            "Epoch 63/300\n",
            "88/88 [==============================] - 0s 5ms/step - loss: 0.5792 - accuracy: 0.7272 - f1_m: 0.5315 - precision_m: 0.7699 - recall_m: 0.4093 - val_loss: 0.5827 - val_accuracy: 0.7250 - val_f1_m: 0.5232 - val_precision_m: 0.7368 - val_recall_m: 0.4120\n",
            "Epoch 64/300\n",
            "88/88 [==============================] - 0s 5ms/step - loss: 0.5786 - accuracy: 0.7275 - f1_m: 0.5348 - precision_m: 0.7679 - recall_m: 0.4139 - val_loss: 0.5822 - val_accuracy: 0.7229 - val_f1_m: 0.5140 - val_precision_m: 0.7386 - val_recall_m: 0.4003\n",
            "Epoch 65/300\n",
            "88/88 [==============================] - 0s 5ms/step - loss: 0.5782 - accuracy: 0.7268 - f1_m: 0.5264 - precision_m: 0.7724 - recall_m: 0.4040 - val_loss: 0.5820 - val_accuracy: 0.7250 - val_f1_m: 0.5225 - val_precision_m: 0.7370 - val_recall_m: 0.4111\n",
            "Epoch 66/300\n",
            "88/88 [==============================] - 0s 5ms/step - loss: 0.5776 - accuracy: 0.7277 - f1_m: 0.5303 - precision_m: 0.7664 - recall_m: 0.4082 - val_loss: 0.5816 - val_accuracy: 0.7250 - val_f1_m: 0.5225 - val_precision_m: 0.7370 - val_recall_m: 0.4111\n",
            "Epoch 67/300\n",
            "88/88 [==============================] - 0s 5ms/step - loss: 0.5771 - accuracy: 0.7275 - f1_m: 0.5344 - precision_m: 0.7714 - recall_m: 0.4125 - val_loss: 0.5814 - val_accuracy: 0.7261 - val_f1_m: 0.5261 - val_precision_m: 0.7367 - val_recall_m: 0.4157\n",
            "Epoch 68/300\n",
            "88/88 [==============================] - 0s 5ms/step - loss: 0.5767 - accuracy: 0.7285 - f1_m: 0.5327 - precision_m: 0.7695 - recall_m: 0.4123 - val_loss: 0.5812 - val_accuracy: 0.7271 - val_f1_m: 0.5294 - val_precision_m: 0.7369 - val_recall_m: 0.4196\n",
            "Epoch 69/300\n",
            "88/88 [==============================] - 0s 5ms/step - loss: 0.5762 - accuracy: 0.7297 - f1_m: 0.5409 - precision_m: 0.7690 - recall_m: 0.4222 - val_loss: 0.5808 - val_accuracy: 0.7250 - val_f1_m: 0.5189 - val_precision_m: 0.7426 - val_recall_m: 0.4052\n",
            "Epoch 70/300\n",
            "88/88 [==============================] - 0s 5ms/step - loss: 0.5758 - accuracy: 0.7283 - f1_m: 0.5338 - precision_m: 0.7683 - recall_m: 0.4126 - val_loss: 0.5805 - val_accuracy: 0.7268 - val_f1_m: 0.5274 - val_precision_m: 0.7384 - val_recall_m: 0.4167\n",
            "Epoch 71/300\n",
            "88/88 [==============================] - 0s 5ms/step - loss: 0.5754 - accuracy: 0.7295 - f1_m: 0.5388 - precision_m: 0.7703 - recall_m: 0.4192 - val_loss: 0.5803 - val_accuracy: 0.7268 - val_f1_m: 0.5267 - val_precision_m: 0.7390 - val_recall_m: 0.4157\n",
            "Epoch 72/300\n",
            "88/88 [==============================] - 0s 5ms/step - loss: 0.5750 - accuracy: 0.7297 - f1_m: 0.5350 - precision_m: 0.7696 - recall_m: 0.4141 - val_loss: 0.5801 - val_accuracy: 0.7279 - val_f1_m: 0.5313 - val_precision_m: 0.7383 - val_recall_m: 0.4216\n",
            "Epoch 73/300\n",
            "88/88 [==============================] - 0s 5ms/step - loss: 0.5746 - accuracy: 0.7304 - f1_m: 0.5420 - precision_m: 0.7710 - recall_m: 0.4209 - val_loss: 0.5799 - val_accuracy: 0.7293 - val_f1_m: 0.5348 - val_precision_m: 0.7399 - val_recall_m: 0.4252\n",
            "Epoch 74/300\n",
            "88/88 [==============================] - 0s 5ms/step - loss: 0.5742 - accuracy: 0.7313 - f1_m: 0.5435 - precision_m: 0.7701 - recall_m: 0.4240 - val_loss: 0.5796 - val_accuracy: 0.7282 - val_f1_m: 0.5323 - val_precision_m: 0.7388 - val_recall_m: 0.4226\n",
            "Epoch 75/300\n",
            "88/88 [==============================] - 0s 5ms/step - loss: 0.5738 - accuracy: 0.7315 - f1_m: 0.5435 - precision_m: 0.7698 - recall_m: 0.4246 - val_loss: 0.5795 - val_accuracy: 0.7304 - val_f1_m: 0.5378 - val_precision_m: 0.7402 - val_recall_m: 0.4288\n",
            "Epoch 76/300\n",
            "88/88 [==============================] - 0s 5ms/step - loss: 0.5735 - accuracy: 0.7320 - f1_m: 0.5449 - precision_m: 0.7711 - recall_m: 0.4243 - val_loss: 0.5794 - val_accuracy: 0.7300 - val_f1_m: 0.5379 - val_precision_m: 0.7386 - val_recall_m: 0.4296\n",
            "Epoch 77/300\n",
            "88/88 [==============================] - 0s 5ms/step - loss: 0.5732 - accuracy: 0.7317 - f1_m: 0.5438 - precision_m: 0.7672 - recall_m: 0.4248 - val_loss: 0.5791 - val_accuracy: 0.7304 - val_f1_m: 0.5382 - val_precision_m: 0.7395 - val_recall_m: 0.4296\n",
            "Epoch 78/300\n",
            "88/88 [==============================] - 0s 5ms/step - loss: 0.5728 - accuracy: 0.7327 - f1_m: 0.5468 - precision_m: 0.7719 - recall_m: 0.4276 - val_loss: 0.5789 - val_accuracy: 0.7304 - val_f1_m: 0.5392 - val_precision_m: 0.7380 - val_recall_m: 0.4315\n",
            "Epoch 79/300\n",
            "88/88 [==============================] - 0s 5ms/step - loss: 0.5725 - accuracy: 0.7325 - f1_m: 0.5470 - precision_m: 0.7666 - recall_m: 0.4291 - val_loss: 0.5786 - val_accuracy: 0.7307 - val_f1_m: 0.5377 - val_precision_m: 0.7419 - val_recall_m: 0.4280\n",
            "Epoch 80/300\n",
            "88/88 [==============================] - 0s 5ms/step - loss: 0.5722 - accuracy: 0.7329 - f1_m: 0.5482 - precision_m: 0.7721 - recall_m: 0.4290 - val_loss: 0.5785 - val_accuracy: 0.7307 - val_f1_m: 0.5395 - val_precision_m: 0.7389 - val_recall_m: 0.4315\n",
            "Epoch 81/300\n",
            "88/88 [==============================] - 0s 5ms/step - loss: 0.5719 - accuracy: 0.7331 - f1_m: 0.5471 - precision_m: 0.7716 - recall_m: 0.4283 - val_loss: 0.5785 - val_accuracy: 0.7307 - val_f1_m: 0.5421 - val_precision_m: 0.7345 - val_recall_m: 0.4363\n",
            "Epoch 82/300\n",
            "88/88 [==============================] - 0s 5ms/step - loss: 0.5716 - accuracy: 0.7327 - f1_m: 0.5459 - precision_m: 0.7649 - recall_m: 0.4281 - val_loss: 0.5782 - val_accuracy: 0.7307 - val_f1_m: 0.5421 - val_precision_m: 0.7345 - val_recall_m: 0.4363\n",
            "Epoch 83/300\n",
            "88/88 [==============================] - 0s 5ms/step - loss: 0.5713 - accuracy: 0.7331 - f1_m: 0.5503 - precision_m: 0.7708 - recall_m: 0.4326 - val_loss: 0.5780 - val_accuracy: 0.7318 - val_f1_m: 0.5427 - val_precision_m: 0.7386 - val_recall_m: 0.4355\n",
            "Epoch 84/300\n",
            "88/88 [==============================] - 0s 5ms/step - loss: 0.5711 - accuracy: 0.7330 - f1_m: 0.5492 - precision_m: 0.7706 - recall_m: 0.4306 - val_loss: 0.5778 - val_accuracy: 0.7307 - val_f1_m: 0.5421 - val_precision_m: 0.7345 - val_recall_m: 0.4363\n",
            "Epoch 85/300\n",
            "88/88 [==============================] - 0s 5ms/step - loss: 0.5708 - accuracy: 0.7331 - f1_m: 0.5490 - precision_m: 0.7689 - recall_m: 0.4300 - val_loss: 0.5779 - val_accuracy: 0.7321 - val_f1_m: 0.5454 - val_precision_m: 0.7362 - val_recall_m: 0.4404\n",
            "Epoch 86/300\n",
            "88/88 [==============================] - 0s 5ms/step - loss: 0.5705 - accuracy: 0.7344 - f1_m: 0.5543 - precision_m: 0.7709 - recall_m: 0.4362 - val_loss: 0.5775 - val_accuracy: 0.7314 - val_f1_m: 0.5427 - val_precision_m: 0.7362 - val_recall_m: 0.4363\n",
            "Epoch 87/300\n",
            "88/88 [==============================] - 0s 5ms/step - loss: 0.5703 - accuracy: 0.7330 - f1_m: 0.5498 - precision_m: 0.7667 - recall_m: 0.4324 - val_loss: 0.5774 - val_accuracy: 0.7321 - val_f1_m: 0.5454 - val_precision_m: 0.7362 - val_recall_m: 0.4404\n",
            "Epoch 88/300\n",
            "88/88 [==============================] - 0s 5ms/step - loss: 0.5700 - accuracy: 0.7342 - f1_m: 0.5531 - precision_m: 0.7707 - recall_m: 0.4351 - val_loss: 0.5775 - val_accuracy: 0.7336 - val_f1_m: 0.5495 - val_precision_m: 0.7366 - val_recall_m: 0.4451\n",
            "Epoch 89/300\n",
            "88/88 [==============================] - 0s 5ms/step - loss: 0.5698 - accuracy: 0.7346 - f1_m: 0.5536 - precision_m: 0.7652 - recall_m: 0.4374 - val_loss: 0.5770 - val_accuracy: 0.7329 - val_f1_m: 0.5438 - val_precision_m: 0.7405 - val_recall_m: 0.4362\n",
            "Epoch 90/300\n",
            "88/88 [==============================] - 0s 5ms/step - loss: 0.5695 - accuracy: 0.7346 - f1_m: 0.5512 - precision_m: 0.7676 - recall_m: 0.4339 - val_loss: 0.5770 - val_accuracy: 0.7336 - val_f1_m: 0.5490 - val_precision_m: 0.7371 - val_recall_m: 0.4442\n",
            "Epoch 91/300\n",
            "88/88 [==============================] - 0s 5ms/step - loss: 0.5692 - accuracy: 0.7348 - f1_m: 0.5548 - precision_m: 0.7680 - recall_m: 0.4380 - val_loss: 0.5766 - val_accuracy: 0.7339 - val_f1_m: 0.5459 - val_precision_m: 0.7426 - val_recall_m: 0.4380\n",
            "Epoch 92/300\n",
            "88/88 [==============================] - 0s 5ms/step - loss: 0.5690 - accuracy: 0.7354 - f1_m: 0.5563 - precision_m: 0.7728 - recall_m: 0.4397 - val_loss: 0.5766 - val_accuracy: 0.7350 - val_f1_m: 0.5512 - val_precision_m: 0.7396 - val_recall_m: 0.4460\n",
            "Epoch 93/300\n",
            "88/88 [==============================] - 0s 5ms/step - loss: 0.5687 - accuracy: 0.7354 - f1_m: 0.5566 - precision_m: 0.7690 - recall_m: 0.4399 - val_loss: 0.5767 - val_accuracy: 0.7350 - val_f1_m: 0.5537 - val_precision_m: 0.7361 - val_recall_m: 0.4505\n",
            "Epoch 94/300\n",
            "88/88 [==============================] - 0s 5ms/step - loss: 0.5685 - accuracy: 0.7351 - f1_m: 0.5567 - precision_m: 0.7678 - recall_m: 0.4414 - val_loss: 0.5762 - val_accuracy: 0.7339 - val_f1_m: 0.5454 - val_precision_m: 0.7435 - val_recall_m: 0.4372\n",
            "Epoch 95/300\n",
            "88/88 [==============================] - 0s 5ms/step - loss: 0.5683 - accuracy: 0.7353 - f1_m: 0.5551 - precision_m: 0.7739 - recall_m: 0.4367 - val_loss: 0.5762 - val_accuracy: 0.7361 - val_f1_m: 0.5553 - val_precision_m: 0.7384 - val_recall_m: 0.4515\n",
            "Epoch 96/300\n",
            "88/88 [==============================] - 0s 5ms/step - loss: 0.5681 - accuracy: 0.7352 - f1_m: 0.5551 - precision_m: 0.7662 - recall_m: 0.4392 - val_loss: 0.5759 - val_accuracy: 0.7368 - val_f1_m: 0.5529 - val_precision_m: 0.7443 - val_recall_m: 0.4466\n",
            "Epoch 97/300\n",
            "88/88 [==============================] - 0s 5ms/step - loss: 0.5679 - accuracy: 0.7352 - f1_m: 0.5569 - precision_m: 0.7695 - recall_m: 0.4399 - val_loss: 0.5759 - val_accuracy: 0.7364 - val_f1_m: 0.5548 - val_precision_m: 0.7396 - val_recall_m: 0.4505\n",
            "Epoch 98/300\n",
            "88/88 [==============================] - 0s 5ms/step - loss: 0.5676 - accuracy: 0.7358 - f1_m: 0.5573 - precision_m: 0.7656 - recall_m: 0.4413 - val_loss: 0.5756 - val_accuracy: 0.7336 - val_f1_m: 0.5455 - val_precision_m: 0.7414 - val_recall_m: 0.4380\n",
            "Epoch 99/300\n",
            "88/88 [==============================] - 0s 5ms/step - loss: 0.5675 - accuracy: 0.7359 - f1_m: 0.5569 - precision_m: 0.7699 - recall_m: 0.4402 - val_loss: 0.5755 - val_accuracy: 0.7368 - val_f1_m: 0.5537 - val_precision_m: 0.7435 - val_recall_m: 0.4477\n",
            "Epoch 100/300\n",
            "88/88 [==============================] - 0s 5ms/step - loss: 0.5673 - accuracy: 0.7359 - f1_m: 0.5585 - precision_m: 0.7703 - recall_m: 0.4428 - val_loss: 0.5754 - val_accuracy: 0.7375 - val_f1_m: 0.5553 - val_precision_m: 0.7442 - val_recall_m: 0.4495\n",
            "Epoch 101/300\n",
            "88/88 [==============================] - 0s 6ms/step - loss: 0.5671 - accuracy: 0.7362 - f1_m: 0.5600 - precision_m: 0.7707 - recall_m: 0.4431 - val_loss: 0.5754 - val_accuracy: 0.7375 - val_f1_m: 0.5565 - val_precision_m: 0.7428 - val_recall_m: 0.4514\n",
            "Epoch 102/300\n",
            "88/88 [==============================] - 0s 5ms/step - loss: 0.5669 - accuracy: 0.7354 - f1_m: 0.5572 - precision_m: 0.7644 - recall_m: 0.4430 - val_loss: 0.5751 - val_accuracy: 0.7368 - val_f1_m: 0.5531 - val_precision_m: 0.7445 - val_recall_m: 0.4466\n",
            "Epoch 103/300\n",
            "88/88 [==============================] - 0s 6ms/step - loss: 0.5667 - accuracy: 0.7362 - f1_m: 0.5580 - precision_m: 0.7688 - recall_m: 0.4413 - val_loss: 0.5750 - val_accuracy: 0.7371 - val_f1_m: 0.5546 - val_precision_m: 0.7444 - val_recall_m: 0.4487\n",
            "Epoch 104/300\n",
            "88/88 [==============================] - 0s 5ms/step - loss: 0.5665 - accuracy: 0.7359 - f1_m: 0.5593 - precision_m: 0.7708 - recall_m: 0.4435 - val_loss: 0.5749 - val_accuracy: 0.7375 - val_f1_m: 0.5549 - val_precision_m: 0.7452 - val_recall_m: 0.4487\n",
            "Epoch 105/300\n",
            "88/88 [==============================] - 0s 5ms/step - loss: 0.5663 - accuracy: 0.7358 - f1_m: 0.5586 - precision_m: 0.7659 - recall_m: 0.4426 - val_loss: 0.5747 - val_accuracy: 0.7368 - val_f1_m: 0.5531 - val_precision_m: 0.7445 - val_recall_m: 0.4466\n",
            "Epoch 106/300\n",
            "88/88 [==============================] - 0s 5ms/step - loss: 0.5661 - accuracy: 0.7367 - f1_m: 0.5599 - precision_m: 0.7672 - recall_m: 0.4443 - val_loss: 0.5746 - val_accuracy: 0.7364 - val_f1_m: 0.5524 - val_precision_m: 0.7443 - val_recall_m: 0.4458\n",
            "Epoch 107/300\n",
            "88/88 [==============================] - 0s 5ms/step - loss: 0.5659 - accuracy: 0.7366 - f1_m: 0.5603 - precision_m: 0.7703 - recall_m: 0.4444 - val_loss: 0.5747 - val_accuracy: 0.7368 - val_f1_m: 0.5580 - val_precision_m: 0.7387 - val_recall_m: 0.4543\n",
            "Epoch 108/300\n",
            "88/88 [==============================] - 0s 5ms/step - loss: 0.5657 - accuracy: 0.7363 - f1_m: 0.5608 - precision_m: 0.7684 - recall_m: 0.4460 - val_loss: 0.5744 - val_accuracy: 0.7361 - val_f1_m: 0.5529 - val_precision_m: 0.7426 - val_recall_m: 0.4469\n",
            "Epoch 109/300\n",
            "88/88 [==============================] - 0s 6ms/step - loss: 0.5656 - accuracy: 0.7364 - f1_m: 0.5603 - precision_m: 0.7661 - recall_m: 0.4449 - val_loss: 0.5742 - val_accuracy: 0.7361 - val_f1_m: 0.5505 - val_precision_m: 0.7455 - val_recall_m: 0.4430\n",
            "Epoch 110/300\n",
            "88/88 [==============================] - 0s 5ms/step - loss: 0.5654 - accuracy: 0.7364 - f1_m: 0.5611 - precision_m: 0.7664 - recall_m: 0.4466 - val_loss: 0.5740 - val_accuracy: 0.7354 - val_f1_m: 0.5485 - val_precision_m: 0.7443 - val_recall_m: 0.4409\n",
            "Epoch 111/300\n",
            "88/88 [==============================] - 0s 5ms/step - loss: 0.5652 - accuracy: 0.7365 - f1_m: 0.5616 - precision_m: 0.7682 - recall_m: 0.4460 - val_loss: 0.5739 - val_accuracy: 0.7350 - val_f1_m: 0.5494 - val_precision_m: 0.7419 - val_recall_m: 0.4430\n",
            "Epoch 112/300\n",
            "88/88 [==============================] - 0s 5ms/step - loss: 0.5651 - accuracy: 0.7362 - f1_m: 0.5601 - precision_m: 0.7661 - recall_m: 0.4462 - val_loss: 0.5738 - val_accuracy: 0.7343 - val_f1_m: 0.5465 - val_precision_m: 0.7411 - val_recall_m: 0.4398\n",
            "Epoch 113/300\n",
            "88/88 [==============================] - 0s 5ms/step - loss: 0.5648 - accuracy: 0.7364 - f1_m: 0.5606 - precision_m: 0.7655 - recall_m: 0.4464 - val_loss: 0.5737 - val_accuracy: 0.7332 - val_f1_m: 0.5424 - val_precision_m: 0.7423 - val_recall_m: 0.4335\n",
            "Epoch 114/300\n",
            "88/88 [==============================] - 0s 5ms/step - loss: 0.5648 - accuracy: 0.7362 - f1_m: 0.5608 - precision_m: 0.7670 - recall_m: 0.4461 - val_loss: 0.5735 - val_accuracy: 0.7354 - val_f1_m: 0.5501 - val_precision_m: 0.7424 - val_recall_m: 0.4438\n",
            "Epoch 115/300\n",
            "88/88 [==============================] - 0s 5ms/step - loss: 0.5646 - accuracy: 0.7367 - f1_m: 0.5616 - precision_m: 0.7707 - recall_m: 0.4453 - val_loss: 0.5736 - val_accuracy: 0.7389 - val_f1_m: 0.5612 - val_precision_m: 0.7449 - val_recall_m: 0.4569\n",
            "Epoch 116/300\n",
            "88/88 [==============================] - 0s 6ms/step - loss: 0.5643 - accuracy: 0.7371 - f1_m: 0.5610 - precision_m: 0.7656 - recall_m: 0.4474 - val_loss: 0.5733 - val_accuracy: 0.7368 - val_f1_m: 0.5533 - val_precision_m: 0.7441 - val_recall_m: 0.4476\n",
            "Epoch 117/300\n",
            "88/88 [==============================] - 0s 5ms/step - loss: 0.5642 - accuracy: 0.7368 - f1_m: 0.5619 - precision_m: 0.7685 - recall_m: 0.4471 - val_loss: 0.5732 - val_accuracy: 0.7386 - val_f1_m: 0.5581 - val_precision_m: 0.7466 - val_recall_m: 0.4527\n",
            "Epoch 118/300\n",
            "88/88 [==============================] - 0s 5ms/step - loss: 0.5640 - accuracy: 0.7372 - f1_m: 0.5623 - precision_m: 0.7671 - recall_m: 0.4467 - val_loss: 0.5732 - val_accuracy: 0.7389 - val_f1_m: 0.5592 - val_precision_m: 0.7472 - val_recall_m: 0.4537\n",
            "Epoch 119/300\n",
            "88/88 [==============================] - 0s 5ms/step - loss: 0.5639 - accuracy: 0.7371 - f1_m: 0.5622 - precision_m: 0.7675 - recall_m: 0.4471 - val_loss: 0.5731 - val_accuracy: 0.7379 - val_f1_m: 0.5575 - val_precision_m: 0.7435 - val_recall_m: 0.4527\n",
            "Epoch 120/300\n",
            "88/88 [==============================] - 0s 5ms/step - loss: 0.5637 - accuracy: 0.7369 - f1_m: 0.5624 - precision_m: 0.7681 - recall_m: 0.4485 - val_loss: 0.5730 - val_accuracy: 0.7386 - val_f1_m: 0.5593 - val_precision_m: 0.7442 - val_recall_m: 0.4547\n",
            "Epoch 121/300\n",
            "88/88 [==============================] - 0s 5ms/step - loss: 0.5636 - accuracy: 0.7371 - f1_m: 0.5623 - precision_m: 0.7650 - recall_m: 0.4482 - val_loss: 0.5727 - val_accuracy: 0.7371 - val_f1_m: 0.5536 - val_precision_m: 0.7451 - val_recall_m: 0.4473\n",
            "Epoch 122/300\n",
            "88/88 [==============================] - 0s 5ms/step - loss: 0.5634 - accuracy: 0.7370 - f1_m: 0.5623 - precision_m: 0.7662 - recall_m: 0.4480 - val_loss: 0.5728 - val_accuracy: 0.7382 - val_f1_m: 0.5604 - val_precision_m: 0.7418 - val_recall_m: 0.4568\n",
            "Epoch 123/300\n",
            "88/88 [==============================] - 0s 5ms/step - loss: 0.5632 - accuracy: 0.7373 - f1_m: 0.5649 - precision_m: 0.7668 - recall_m: 0.4507 - val_loss: 0.5727 - val_accuracy: 0.7382 - val_f1_m: 0.5602 - val_precision_m: 0.7416 - val_recall_m: 0.4567\n",
            "Epoch 124/300\n",
            "88/88 [==============================] - 0s 5ms/step - loss: 0.5631 - accuracy: 0.7365 - f1_m: 0.5627 - precision_m: 0.7675 - recall_m: 0.4485 - val_loss: 0.5729 - val_accuracy: 0.7382 - val_f1_m: 0.5637 - val_precision_m: 0.7375 - val_recall_m: 0.4626\n",
            "Epoch 125/300\n",
            "88/88 [==============================] - 0s 5ms/step - loss: 0.5628 - accuracy: 0.7379 - f1_m: 0.5671 - precision_m: 0.7676 - recall_m: 0.4537 - val_loss: 0.5723 - val_accuracy: 0.7371 - val_f1_m: 0.5542 - val_precision_m: 0.7440 - val_recall_m: 0.4484\n",
            "Epoch 126/300\n",
            "88/88 [==============================] - 0s 5ms/step - loss: 0.5628 - accuracy: 0.7366 - f1_m: 0.5642 - precision_m: 0.7700 - recall_m: 0.4504 - val_loss: 0.5723 - val_accuracy: 0.7379 - val_f1_m: 0.5586 - val_precision_m: 0.7416 - val_recall_m: 0.4546\n",
            "Epoch 127/300\n",
            "88/88 [==============================] - 0s 6ms/step - loss: 0.5626 - accuracy: 0.7370 - f1_m: 0.5611 - precision_m: 0.7644 - recall_m: 0.4470 - val_loss: 0.5722 - val_accuracy: 0.7379 - val_f1_m: 0.5582 - val_precision_m: 0.7425 - val_recall_m: 0.4534\n",
            "Epoch 128/300\n",
            "88/88 [==============================] - 0s 5ms/step - loss: 0.5624 - accuracy: 0.7372 - f1_m: 0.5618 - precision_m: 0.7646 - recall_m: 0.4474 - val_loss: 0.5720 - val_accuracy: 0.7382 - val_f1_m: 0.5562 - val_precision_m: 0.7454 - val_recall_m: 0.4504\n",
            "Epoch 129/300\n",
            "88/88 [==============================] - 0s 5ms/step - loss: 0.5623 - accuracy: 0.7369 - f1_m: 0.5613 - precision_m: 0.7655 - recall_m: 0.4466 - val_loss: 0.5722 - val_accuracy: 0.7371 - val_f1_m: 0.5604 - val_precision_m: 0.7359 - val_recall_m: 0.4587\n",
            "Epoch 130/300\n",
            "88/88 [==============================] - 0s 5ms/step - loss: 0.5621 - accuracy: 0.7379 - f1_m: 0.5652 - precision_m: 0.7659 - recall_m: 0.4525 - val_loss: 0.5718 - val_accuracy: 0.7375 - val_f1_m: 0.5565 - val_precision_m: 0.7425 - val_recall_m: 0.4514\n",
            "Epoch 131/300\n",
            "88/88 [==============================] - 0s 6ms/step - loss: 0.5619 - accuracy: 0.7379 - f1_m: 0.5635 - precision_m: 0.7666 - recall_m: 0.4495 - val_loss: 0.5717 - val_accuracy: 0.7379 - val_f1_m: 0.5560 - val_precision_m: 0.7441 - val_recall_m: 0.4504\n",
            "Epoch 132/300\n",
            "88/88 [==============================] - 0s 5ms/step - loss: 0.5618 - accuracy: 0.7373 - f1_m: 0.5653 - precision_m: 0.7651 - recall_m: 0.4526 - val_loss: 0.5716 - val_accuracy: 0.7368 - val_f1_m: 0.5529 - val_precision_m: 0.7443 - val_recall_m: 0.4463\n",
            "Epoch 133/300\n",
            "88/88 [==============================] - 0s 5ms/step - loss: 0.5617 - accuracy: 0.7371 - f1_m: 0.5626 - precision_m: 0.7663 - recall_m: 0.4478 - val_loss: 0.5716 - val_accuracy: 0.7368 - val_f1_m: 0.5573 - val_precision_m: 0.7388 - val_recall_m: 0.4534\n",
            "Epoch 134/300\n",
            "88/88 [==============================] - 0s 5ms/step - loss: 0.5615 - accuracy: 0.7376 - f1_m: 0.5635 - precision_m: 0.7644 - recall_m: 0.4499 - val_loss: 0.5716 - val_accuracy: 0.7371 - val_f1_m: 0.5589 - val_precision_m: 0.7375 - val_recall_m: 0.4564\n",
            "Epoch 135/300\n",
            "88/88 [==============================] - 1s 6ms/step - loss: 0.5614 - accuracy: 0.7375 - f1_m: 0.5644 - precision_m: 0.7659 - recall_m: 0.4507 - val_loss: 0.5713 - val_accuracy: 0.7371 - val_f1_m: 0.5571 - val_precision_m: 0.7401 - val_recall_m: 0.4525\n",
            "Epoch 136/300\n",
            "88/88 [==============================] - 0s 5ms/step - loss: 0.5613 - accuracy: 0.7377 - f1_m: 0.5634 - precision_m: 0.7668 - recall_m: 0.4488 - val_loss: 0.5715 - val_accuracy: 0.7371 - val_f1_m: 0.5589 - val_precision_m: 0.7373 - val_recall_m: 0.4563\n",
            "Epoch 137/300\n",
            "88/88 [==============================] - 0s 6ms/step - loss: 0.5611 - accuracy: 0.7384 - f1_m: 0.5666 - precision_m: 0.7678 - recall_m: 0.4528 - val_loss: 0.5712 - val_accuracy: 0.7368 - val_f1_m: 0.5570 - val_precision_m: 0.7385 - val_recall_m: 0.4533\n",
            "Epoch 138/300\n",
            "88/88 [==============================] - 0s 5ms/step - loss: 0.5610 - accuracy: 0.7386 - f1_m: 0.5668 - precision_m: 0.7679 - recall_m: 0.4533 - val_loss: 0.5711 - val_accuracy: 0.7364 - val_f1_m: 0.5567 - val_precision_m: 0.7372 - val_recall_m: 0.4533\n",
            "Epoch 139/300\n",
            "88/88 [==============================] - 0s 5ms/step - loss: 0.5607 - accuracy: 0.7382 - f1_m: 0.5657 - precision_m: 0.7667 - recall_m: 0.4520 - val_loss: 0.5708 - val_accuracy: 0.7364 - val_f1_m: 0.5537 - val_precision_m: 0.7408 - val_recall_m: 0.4484\n",
            "Epoch 140/300\n",
            "88/88 [==============================] - 0s 5ms/step - loss: 0.5606 - accuracy: 0.7374 - f1_m: 0.5636 - precision_m: 0.7657 - recall_m: 0.4493 - val_loss: 0.5713 - val_accuracy: 0.7382 - val_f1_m: 0.5626 - val_precision_m: 0.7384 - val_recall_m: 0.4604\n",
            "Epoch 141/300\n",
            "88/88 [==============================] - 0s 6ms/step - loss: 0.5605 - accuracy: 0.7391 - f1_m: 0.5673 - precision_m: 0.7673 - recall_m: 0.4548 - val_loss: 0.5710 - val_accuracy: 0.7371 - val_f1_m: 0.5590 - val_precision_m: 0.7371 - val_recall_m: 0.4562\n",
            "Epoch 142/300\n",
            "88/88 [==============================] - 0s 5ms/step - loss: 0.5604 - accuracy: 0.7389 - f1_m: 0.5679 - precision_m: 0.7662 - recall_m: 0.4546 - val_loss: 0.5707 - val_accuracy: 0.7371 - val_f1_m: 0.5575 - val_precision_m: 0.7390 - val_recall_m: 0.4533\n",
            "Epoch 143/300\n",
            "88/88 [==============================] - 0s 5ms/step - loss: 0.5602 - accuracy: 0.7382 - f1_m: 0.5641 - precision_m: 0.7663 - recall_m: 0.4507 - val_loss: 0.5706 - val_accuracy: 0.7379 - val_f1_m: 0.5582 - val_precision_m: 0.7416 - val_recall_m: 0.4533\n",
            "Epoch 144/300\n",
            "88/88 [==============================] - 0s 5ms/step - loss: 0.5600 - accuracy: 0.7389 - f1_m: 0.5678 - precision_m: 0.7670 - recall_m: 0.4540 - val_loss: 0.5704 - val_accuracy: 0.7361 - val_f1_m: 0.5528 - val_precision_m: 0.7405 - val_recall_m: 0.4471\n",
            "Epoch 145/300\n",
            "88/88 [==============================] - 0s 5ms/step - loss: 0.5599 - accuracy: 0.7381 - f1_m: 0.5643 - precision_m: 0.7672 - recall_m: 0.4509 - val_loss: 0.5707 - val_accuracy: 0.7361 - val_f1_m: 0.5568 - val_precision_m: 0.7347 - val_recall_m: 0.4543\n",
            "Epoch 146/300\n",
            "88/88 [==============================] - 0s 5ms/step - loss: 0.5598 - accuracy: 0.7387 - f1_m: 0.5665 - precision_m: 0.7691 - recall_m: 0.4537 - val_loss: 0.5709 - val_accuracy: 0.7382 - val_f1_m: 0.5633 - val_precision_m: 0.7378 - val_recall_m: 0.4613\n",
            "Epoch 147/300\n",
            "88/88 [==============================] - 0s 5ms/step - loss: 0.5596 - accuracy: 0.7395 - f1_m: 0.5689 - precision_m: 0.7646 - recall_m: 0.4568 - val_loss: 0.5702 - val_accuracy: 0.7368 - val_f1_m: 0.5550 - val_precision_m: 0.7400 - val_recall_m: 0.4502\n",
            "Epoch 148/300\n",
            "88/88 [==============================] - 0s 6ms/step - loss: 0.5595 - accuracy: 0.7386 - f1_m: 0.5666 - precision_m: 0.7667 - recall_m: 0.4535 - val_loss: 0.5702 - val_accuracy: 0.7371 - val_f1_m: 0.5575 - val_precision_m: 0.7390 - val_recall_m: 0.4533\n",
            "Epoch 149/300\n",
            "88/88 [==============================] - 0s 5ms/step - loss: 0.5594 - accuracy: 0.7387 - f1_m: 0.5677 - precision_m: 0.7681 - recall_m: 0.4548 - val_loss: 0.5701 - val_accuracy: 0.7368 - val_f1_m: 0.5571 - val_precision_m: 0.7376 - val_recall_m: 0.4533\n",
            "Epoch 150/300\n",
            "88/88 [==============================] - 0s 5ms/step - loss: 0.5592 - accuracy: 0.7395 - f1_m: 0.5669 - precision_m: 0.7702 - recall_m: 0.4538 - val_loss: 0.5703 - val_accuracy: 0.7361 - val_f1_m: 0.5571 - val_precision_m: 0.7354 - val_recall_m: 0.4543\n",
            "Epoch 151/300\n",
            "88/88 [==============================] - 0s 5ms/step - loss: 0.5591 - accuracy: 0.7401 - f1_m: 0.5702 - precision_m: 0.7688 - recall_m: 0.4568 - val_loss: 0.5699 - val_accuracy: 0.7371 - val_f1_m: 0.5574 - val_precision_m: 0.7392 - val_recall_m: 0.4533\n",
            "Epoch 152/300\n",
            "88/88 [==============================] - 0s 5ms/step - loss: 0.5590 - accuracy: 0.7398 - f1_m: 0.5693 - precision_m: 0.7681 - recall_m: 0.4563 - val_loss: 0.5698 - val_accuracy: 0.7364 - val_f1_m: 0.5557 - val_precision_m: 0.7386 - val_recall_m: 0.4513\n",
            "Epoch 153/300\n",
            "88/88 [==============================] - 0s 5ms/step - loss: 0.5589 - accuracy: 0.7395 - f1_m: 0.5666 - precision_m: 0.7651 - recall_m: 0.4530 - val_loss: 0.5698 - val_accuracy: 0.7364 - val_f1_m: 0.5557 - val_precision_m: 0.7386 - val_recall_m: 0.4513\n",
            "Epoch 154/300\n",
            "88/88 [==============================] - 0s 5ms/step - loss: 0.5588 - accuracy: 0.7393 - f1_m: 0.5702 - precision_m: 0.7706 - recall_m: 0.4568 - val_loss: 0.5701 - val_accuracy: 0.7364 - val_f1_m: 0.5580 - val_precision_m: 0.7359 - val_recall_m: 0.4551\n",
            "Epoch 155/300\n",
            "88/88 [==============================] - 0s 6ms/step - loss: 0.5586 - accuracy: 0.7394 - f1_m: 0.5702 - precision_m: 0.7668 - recall_m: 0.4572 - val_loss: 0.5697 - val_accuracy: 0.7368 - val_f1_m: 0.5566 - val_precision_m: 0.7389 - val_recall_m: 0.4523\n",
            "Epoch 156/300\n",
            "88/88 [==============================] - 0s 6ms/step - loss: 0.5584 - accuracy: 0.7390 - f1_m: 0.5671 - precision_m: 0.7691 - recall_m: 0.4538 - val_loss: 0.5701 - val_accuracy: 0.7375 - val_f1_m: 0.5612 - val_precision_m: 0.7373 - val_recall_m: 0.4589\n",
            "Epoch 157/300\n",
            "88/88 [==============================] - 0s 5ms/step - loss: 0.5584 - accuracy: 0.7397 - f1_m: 0.5699 - precision_m: 0.7676 - recall_m: 0.4571 - val_loss: 0.5696 - val_accuracy: 0.7364 - val_f1_m: 0.5562 - val_precision_m: 0.7376 - val_recall_m: 0.4523\n",
            "Epoch 158/300\n",
            "88/88 [==============================] - 0s 6ms/step - loss: 0.5583 - accuracy: 0.7387 - f1_m: 0.5646 - precision_m: 0.7659 - recall_m: 0.4518 - val_loss: 0.5698 - val_accuracy: 0.7368 - val_f1_m: 0.5589 - val_precision_m: 0.7366 - val_recall_m: 0.4559\n",
            "Epoch 159/300\n",
            "88/88 [==============================] - 0s 5ms/step - loss: 0.5582 - accuracy: 0.7395 - f1_m: 0.5694 - precision_m: 0.7673 - recall_m: 0.4571 - val_loss: 0.5694 - val_accuracy: 0.7364 - val_f1_m: 0.5557 - val_precision_m: 0.7386 - val_recall_m: 0.4513\n",
            "Epoch 160/300\n",
            "88/88 [==============================] - 0s 5ms/step - loss: 0.5580 - accuracy: 0.7394 - f1_m: 0.5671 - precision_m: 0.7662 - recall_m: 0.4548 - val_loss: 0.5694 - val_accuracy: 0.7371 - val_f1_m: 0.5577 - val_precision_m: 0.7401 - val_recall_m: 0.4532\n",
            "Epoch 161/300\n",
            "88/88 [==============================] - 0s 5ms/step - loss: 0.5578 - accuracy: 0.7396 - f1_m: 0.5703 - precision_m: 0.7714 - recall_m: 0.4576 - val_loss: 0.5694 - val_accuracy: 0.7375 - val_f1_m: 0.5589 - val_precision_m: 0.7393 - val_recall_m: 0.4550\n",
            "Epoch 162/300\n",
            "88/88 [==============================] - 0s 6ms/step - loss: 0.5578 - accuracy: 0.7395 - f1_m: 0.5679 - precision_m: 0.7687 - recall_m: 0.4543 - val_loss: 0.5695 - val_accuracy: 0.7371 - val_f1_m: 0.5603 - val_precision_m: 0.7367 - val_recall_m: 0.4576\n",
            "Epoch 163/300\n",
            "88/88 [==============================] - 0s 5ms/step - loss: 0.5577 - accuracy: 0.7401 - f1_m: 0.5692 - precision_m: 0.7665 - recall_m: 0.4558 - val_loss: 0.5693 - val_accuracy: 0.7379 - val_f1_m: 0.5593 - val_precision_m: 0.7406 - val_recall_m: 0.4550\n",
            "Epoch 164/300\n",
            "88/88 [==============================] - 0s 5ms/step - loss: 0.5576 - accuracy: 0.7398 - f1_m: 0.5692 - precision_m: 0.7672 - recall_m: 0.4564 - val_loss: 0.5691 - val_accuracy: 0.7364 - val_f1_m: 0.5547 - val_precision_m: 0.7411 - val_recall_m: 0.4490\n",
            "Epoch 165/300\n",
            "88/88 [==============================] - 0s 5ms/step - loss: 0.5575 - accuracy: 0.7401 - f1_m: 0.5714 - precision_m: 0.7712 - recall_m: 0.4586 - val_loss: 0.5691 - val_accuracy: 0.7379 - val_f1_m: 0.5588 - val_precision_m: 0.7422 - val_recall_m: 0.4540\n",
            "Epoch 166/300\n",
            "88/88 [==============================] - 0s 5ms/step - loss: 0.5573 - accuracy: 0.7395 - f1_m: 0.5704 - precision_m: 0.7689 - recall_m: 0.4571 - val_loss: 0.5691 - val_accuracy: 0.7379 - val_f1_m: 0.5592 - val_precision_m: 0.7406 - val_recall_m: 0.4548\n",
            "Epoch 167/300\n",
            "88/88 [==============================] - 0s 5ms/step - loss: 0.5571 - accuracy: 0.7398 - f1_m: 0.5705 - precision_m: 0.7689 - recall_m: 0.4575 - val_loss: 0.5689 - val_accuracy: 0.7350 - val_f1_m: 0.5510 - val_precision_m: 0.7394 - val_recall_m: 0.4450\n",
            "Epoch 168/300\n",
            "88/88 [==============================] - 0s 5ms/step - loss: 0.5572 - accuracy: 0.7395 - f1_m: 0.5677 - precision_m: 0.7683 - recall_m: 0.4545 - val_loss: 0.5692 - val_accuracy: 0.7379 - val_f1_m: 0.5610 - val_precision_m: 0.7392 - val_recall_m: 0.4576\n",
            "Epoch 169/300\n",
            "88/88 [==============================] - 0s 5ms/step - loss: 0.5570 - accuracy: 0.7396 - f1_m: 0.5701 - precision_m: 0.7655 - recall_m: 0.4573 - val_loss: 0.5688 - val_accuracy: 0.7379 - val_f1_m: 0.5580 - val_precision_m: 0.7426 - val_recall_m: 0.4528\n",
            "Epoch 170/300\n",
            "88/88 [==============================] - 0s 5ms/step - loss: 0.5569 - accuracy: 0.7398 - f1_m: 0.5694 - precision_m: 0.7693 - recall_m: 0.4563 - val_loss: 0.5690 - val_accuracy: 0.7379 - val_f1_m: 0.5603 - val_precision_m: 0.7394 - val_recall_m: 0.4567\n",
            "Epoch 171/300\n",
            "88/88 [==============================] - 0s 5ms/step - loss: 0.5568 - accuracy: 0.7399 - f1_m: 0.5684 - precision_m: 0.7673 - recall_m: 0.4551 - val_loss: 0.5689 - val_accuracy: 0.7379 - val_f1_m: 0.5591 - val_precision_m: 0.7414 - val_recall_m: 0.4548\n",
            "Epoch 172/300\n",
            "88/88 [==============================] - 0s 5ms/step - loss: 0.5566 - accuracy: 0.7395 - f1_m: 0.5672 - precision_m: 0.7656 - recall_m: 0.4545 - val_loss: 0.5696 - val_accuracy: 0.7382 - val_f1_m: 0.5636 - val_precision_m: 0.7362 - val_recall_m: 0.4624\n",
            "Epoch 173/300\n",
            "88/88 [==============================] - 0s 5ms/step - loss: 0.5566 - accuracy: 0.7399 - f1_m: 0.5721 - precision_m: 0.7701 - recall_m: 0.4587 - val_loss: 0.5692 - val_accuracy: 0.7379 - val_f1_m: 0.5628 - val_precision_m: 0.7359 - val_recall_m: 0.4614\n",
            "Epoch 174/300\n",
            "88/88 [==============================] - 0s 5ms/step - loss: 0.5565 - accuracy: 0.7401 - f1_m: 0.5704 - precision_m: 0.7658 - recall_m: 0.4582 - val_loss: 0.5687 - val_accuracy: 0.7375 - val_f1_m: 0.5592 - val_precision_m: 0.7392 - val_recall_m: 0.4557\n",
            "Epoch 175/300\n",
            "88/88 [==============================] - 0s 5ms/step - loss: 0.5564 - accuracy: 0.7396 - f1_m: 0.5719 - precision_m: 0.7708 - recall_m: 0.4587 - val_loss: 0.5687 - val_accuracy: 0.7386 - val_f1_m: 0.5618 - val_precision_m: 0.7409 - val_recall_m: 0.4585\n",
            "Epoch 176/300\n",
            "88/88 [==============================] - 0s 5ms/step - loss: 0.5563 - accuracy: 0.7391 - f1_m: 0.5681 - precision_m: 0.7664 - recall_m: 0.4547 - val_loss: 0.5689 - val_accuracy: 0.7386 - val_f1_m: 0.5630 - val_precision_m: 0.7393 - val_recall_m: 0.4604\n",
            "Epoch 177/300\n",
            "88/88 [==============================] - 1s 6ms/step - loss: 0.5562 - accuracy: 0.7395 - f1_m: 0.5693 - precision_m: 0.7645 - recall_m: 0.4578 - val_loss: 0.5685 - val_accuracy: 0.7371 - val_f1_m: 0.5563 - val_precision_m: 0.7419 - val_recall_m: 0.4506\n",
            "Epoch 178/300\n",
            "88/88 [==============================] - 0s 5ms/step - loss: 0.5561 - accuracy: 0.7396 - f1_m: 0.5710 - precision_m: 0.7697 - recall_m: 0.4571 - val_loss: 0.5685 - val_accuracy: 0.7375 - val_f1_m: 0.5576 - val_precision_m: 0.7418 - val_recall_m: 0.4527\n",
            "Epoch 179/300\n",
            "88/88 [==============================] - 0s 5ms/step - loss: 0.5561 - accuracy: 0.7391 - f1_m: 0.5692 - precision_m: 0.7678 - recall_m: 0.4565 - val_loss: 0.5687 - val_accuracy: 0.7389 - val_f1_m: 0.5634 - val_precision_m: 0.7402 - val_recall_m: 0.4604\n",
            "Epoch 180/300\n",
            "88/88 [==============================] - 0s 5ms/step - loss: 0.5559 - accuracy: 0.7392 - f1_m: 0.5680 - precision_m: 0.7631 - recall_m: 0.4555 - val_loss: 0.5684 - val_accuracy: 0.7379 - val_f1_m: 0.5587 - val_precision_m: 0.7412 - val_recall_m: 0.4546\n",
            "Epoch 181/300\n",
            "88/88 [==============================] - 0s 6ms/step - loss: 0.5558 - accuracy: 0.7399 - f1_m: 0.5706 - precision_m: 0.7687 - recall_m: 0.4577 - val_loss: 0.5687 - val_accuracy: 0.7393 - val_f1_m: 0.5644 - val_precision_m: 0.7410 - val_recall_m: 0.4613\n",
            "Epoch 182/300\n",
            "88/88 [==============================] - 0s 5ms/step - loss: 0.5558 - accuracy: 0.7398 - f1_m: 0.5718 - precision_m: 0.7672 - recall_m: 0.4593 - val_loss: 0.5683 - val_accuracy: 0.7375 - val_f1_m: 0.5580 - val_precision_m: 0.7410 - val_recall_m: 0.4536\n",
            "Epoch 183/300\n",
            "88/88 [==============================] - 0s 5ms/step - loss: 0.5557 - accuracy: 0.7397 - f1_m: 0.5702 - precision_m: 0.7666 - recall_m: 0.4578 - val_loss: 0.5683 - val_accuracy: 0.7379 - val_f1_m: 0.5591 - val_precision_m: 0.7418 - val_recall_m: 0.4546\n",
            "Epoch 184/300\n",
            "88/88 [==============================] - 0s 5ms/step - loss: 0.5556 - accuracy: 0.7394 - f1_m: 0.5703 - precision_m: 0.7687 - recall_m: 0.4573 - val_loss: 0.5684 - val_accuracy: 0.7396 - val_f1_m: 0.5639 - val_precision_m: 0.7431 - val_recall_m: 0.4603\n",
            "Epoch 185/300\n",
            "88/88 [==============================] - 1s 6ms/step - loss: 0.5554 - accuracy: 0.7399 - f1_m: 0.5706 - precision_m: 0.7702 - recall_m: 0.4571 - val_loss: 0.5688 - val_accuracy: 0.7386 - val_f1_m: 0.5646 - val_precision_m: 0.7374 - val_recall_m: 0.4633\n",
            "Epoch 186/300\n",
            "88/88 [==============================] - 0s 6ms/step - loss: 0.5554 - accuracy: 0.7400 - f1_m: 0.5716 - precision_m: 0.7685 - recall_m: 0.4601 - val_loss: 0.5683 - val_accuracy: 0.7393 - val_f1_m: 0.5626 - val_precision_m: 0.7435 - val_recall_m: 0.4584\n",
            "Epoch 187/300\n",
            "88/88 [==============================] - 1s 6ms/step - loss: 0.5553 - accuracy: 0.7398 - f1_m: 0.5704 - precision_m: 0.7662 - recall_m: 0.4577 - val_loss: 0.5681 - val_accuracy: 0.7382 - val_f1_m: 0.5600 - val_precision_m: 0.7423 - val_recall_m: 0.4555\n",
            "Epoch 188/300\n",
            "88/88 [==============================] - 0s 6ms/step - loss: 0.5553 - accuracy: 0.7391 - f1_m: 0.5697 - precision_m: 0.7690 - recall_m: 0.4574 - val_loss: 0.5682 - val_accuracy: 0.7393 - val_f1_m: 0.5626 - val_precision_m: 0.7435 - val_recall_m: 0.4584\n",
            "Epoch 189/300\n",
            "88/88 [==============================] - 0s 5ms/step - loss: 0.5552 - accuracy: 0.7397 - f1_m: 0.5707 - precision_m: 0.7680 - recall_m: 0.4581 - val_loss: 0.5684 - val_accuracy: 0.7389 - val_f1_m: 0.5649 - val_precision_m: 0.7383 - val_recall_m: 0.4632\n",
            "Epoch 190/300\n",
            "88/88 [==============================] - 0s 6ms/step - loss: 0.5551 - accuracy: 0.7396 - f1_m: 0.5710 - precision_m: 0.7648 - recall_m: 0.4595 - val_loss: 0.5681 - val_accuracy: 0.7382 - val_f1_m: 0.5600 - val_precision_m: 0.7423 - val_recall_m: 0.4555\n",
            "Epoch 191/300\n",
            "88/88 [==============================] - 0s 6ms/step - loss: 0.5550 - accuracy: 0.7402 - f1_m: 0.5702 - precision_m: 0.7656 - recall_m: 0.4581 - val_loss: 0.5682 - val_accuracy: 0.7393 - val_f1_m: 0.5632 - val_precision_m: 0.7430 - val_recall_m: 0.4593\n",
            "Epoch 192/300\n",
            "88/88 [==============================] - 0s 5ms/step - loss: 0.5550 - accuracy: 0.7399 - f1_m: 0.5731 - precision_m: 0.7684 - recall_m: 0.4611 - val_loss: 0.5682 - val_accuracy: 0.7396 - val_f1_m: 0.5642 - val_precision_m: 0.7437 - val_recall_m: 0.4603\n",
            "Epoch 193/300\n",
            "88/88 [==============================] - 0s 6ms/step - loss: 0.5549 - accuracy: 0.7400 - f1_m: 0.5722 - precision_m: 0.7696 - recall_m: 0.4586 - val_loss: 0.5684 - val_accuracy: 0.7386 - val_f1_m: 0.5646 - val_precision_m: 0.7371 - val_recall_m: 0.4632\n",
            "Epoch 194/300\n",
            "88/88 [==============================] - 0s 5ms/step - loss: 0.5548 - accuracy: 0.7400 - f1_m: 0.5710 - precision_m: 0.7669 - recall_m: 0.4583 - val_loss: 0.5681 - val_accuracy: 0.7400 - val_f1_m: 0.5650 - val_precision_m: 0.7441 - val_recall_m: 0.4613\n",
            "Epoch 195/300\n",
            "88/88 [==============================] - 0s 6ms/step - loss: 0.5548 - accuracy: 0.7403 - f1_m: 0.5732 - precision_m: 0.7678 - recall_m: 0.4615 - val_loss: 0.5680 - val_accuracy: 0.7382 - val_f1_m: 0.5606 - val_precision_m: 0.7417 - val_recall_m: 0.4565\n",
            "Epoch 196/300\n",
            "88/88 [==============================] - 0s 6ms/step - loss: 0.5546 - accuracy: 0.7396 - f1_m: 0.5715 - precision_m: 0.7674 - recall_m: 0.4588 - val_loss: 0.5681 - val_accuracy: 0.7393 - val_f1_m: 0.5640 - val_precision_m: 0.7426 - val_recall_m: 0.4604\n",
            "Epoch 197/300\n",
            "88/88 [==============================] - 1s 6ms/step - loss: 0.5545 - accuracy: 0.7397 - f1_m: 0.5724 - precision_m: 0.7660 - recall_m: 0.4613 - val_loss: 0.5678 - val_accuracy: 0.7375 - val_f1_m: 0.5569 - val_precision_m: 0.7432 - val_recall_m: 0.4514\n",
            "Epoch 198/300\n",
            "88/88 [==============================] - 1s 6ms/step - loss: 0.5545 - accuracy: 0.7394 - f1_m: 0.5688 - precision_m: 0.7642 - recall_m: 0.4562 - val_loss: 0.5678 - val_accuracy: 0.7379 - val_f1_m: 0.5583 - val_precision_m: 0.7426 - val_recall_m: 0.4533\n",
            "Epoch 199/300\n",
            "88/88 [==============================] - 0s 6ms/step - loss: 0.5544 - accuracy: 0.7396 - f1_m: 0.5689 - precision_m: 0.7696 - recall_m: 0.4562 - val_loss: 0.5683 - val_accuracy: 0.7393 - val_f1_m: 0.5664 - val_precision_m: 0.7383 - val_recall_m: 0.4650\n",
            "Epoch 200/300\n",
            "88/88 [==============================] - 0s 6ms/step - loss: 0.5543 - accuracy: 0.7399 - f1_m: 0.5727 - precision_m: 0.7658 - recall_m: 0.4614 - val_loss: 0.5678 - val_accuracy: 0.7375 - val_f1_m: 0.5575 - val_precision_m: 0.7423 - val_recall_m: 0.4523\n",
            "Epoch 201/300\n",
            "88/88 [==============================] - 0s 6ms/step - loss: 0.5543 - accuracy: 0.7397 - f1_m: 0.5698 - precision_m: 0.7679 - recall_m: 0.4578 - val_loss: 0.5679 - val_accuracy: 0.7396 - val_f1_m: 0.5654 - val_precision_m: 0.7426 - val_recall_m: 0.4623\n",
            "Epoch 202/300\n",
            "88/88 [==============================] - 0s 5ms/step - loss: 0.5542 - accuracy: 0.7396 - f1_m: 0.5704 - precision_m: 0.7683 - recall_m: 0.4576 - val_loss: 0.5679 - val_accuracy: 0.7396 - val_f1_m: 0.5649 - val_precision_m: 0.7435 - val_recall_m: 0.4613\n",
            "Epoch 203/300\n",
            "88/88 [==============================] - 1s 6ms/step - loss: 0.5541 - accuracy: 0.7396 - f1_m: 0.5694 - precision_m: 0.7660 - recall_m: 0.4569 - val_loss: 0.5678 - val_accuracy: 0.7379 - val_f1_m: 0.5603 - val_precision_m: 0.7408 - val_recall_m: 0.4563\n",
            "Epoch 204/300\n",
            "88/88 [==============================] - 0s 6ms/step - loss: 0.5541 - accuracy: 0.7397 - f1_m: 0.5705 - precision_m: 0.7674 - recall_m: 0.4574 - val_loss: 0.5683 - val_accuracy: 0.7396 - val_f1_m: 0.5673 - val_precision_m: 0.7388 - val_recall_m: 0.4662\n",
            "Epoch 205/300\n",
            "88/88 [==============================] - 0s 6ms/step - loss: 0.5540 - accuracy: 0.7396 - f1_m: 0.5701 - precision_m: 0.7611 - recall_m: 0.4596 - val_loss: 0.5678 - val_accuracy: 0.7400 - val_f1_m: 0.5655 - val_precision_m: 0.7436 - val_recall_m: 0.4622\n",
            "Epoch 206/300\n",
            "88/88 [==============================] - 0s 6ms/step - loss: 0.5539 - accuracy: 0.7395 - f1_m: 0.5711 - precision_m: 0.7665 - recall_m: 0.4581 - val_loss: 0.5678 - val_accuracy: 0.7407 - val_f1_m: 0.5674 - val_precision_m: 0.7447 - val_recall_m: 0.4641\n",
            "Epoch 207/300\n",
            "88/88 [==============================] - 0s 6ms/step - loss: 0.5539 - accuracy: 0.7398 - f1_m: 0.5727 - precision_m: 0.7657 - recall_m: 0.4618 - val_loss: 0.5678 - val_accuracy: 0.7407 - val_f1_m: 0.5674 - val_precision_m: 0.7447 - val_recall_m: 0.4641\n",
            "Epoch 208/300\n",
            "88/88 [==============================] - 0s 5ms/step - loss: 0.5538 - accuracy: 0.7397 - f1_m: 0.5709 - precision_m: 0.7631 - recall_m: 0.4604 - val_loss: 0.5676 - val_accuracy: 0.7375 - val_f1_m: 0.5594 - val_precision_m: 0.7406 - val_recall_m: 0.4553\n",
            "Epoch 209/300\n",
            "88/88 [==============================] - 0s 5ms/step - loss: 0.5537 - accuracy: 0.7401 - f1_m: 0.5699 - precision_m: 0.7664 - recall_m: 0.4579 - val_loss: 0.5677 - val_accuracy: 0.7407 - val_f1_m: 0.5674 - val_precision_m: 0.7447 - val_recall_m: 0.4641\n",
            "Epoch 210/300\n",
            "88/88 [==============================] - 0s 5ms/step - loss: 0.5536 - accuracy: 0.7396 - f1_m: 0.5713 - precision_m: 0.7658 - recall_m: 0.4598 - val_loss: 0.5680 - val_accuracy: 0.7396 - val_f1_m: 0.5677 - val_precision_m: 0.7379 - val_recall_m: 0.4669\n",
            "Epoch 211/300\n",
            "88/88 [==============================] - 0s 6ms/step - loss: 0.5535 - accuracy: 0.7406 - f1_m: 0.5725 - precision_m: 0.7653 - recall_m: 0.4620 - val_loss: 0.5675 - val_accuracy: 0.7371 - val_f1_m: 0.5571 - val_precision_m: 0.7415 - val_recall_m: 0.4520\n",
            "Epoch 212/300\n",
            "88/88 [==============================] - 0s 5ms/step - loss: 0.5536 - accuracy: 0.7397 - f1_m: 0.5715 - precision_m: 0.7649 - recall_m: 0.4602 - val_loss: 0.5676 - val_accuracy: 0.7386 - val_f1_m: 0.5625 - val_precision_m: 0.7408 - val_recall_m: 0.4591\n",
            "Epoch 213/300\n",
            "88/88 [==============================] - 1s 6ms/step - loss: 0.5535 - accuracy: 0.7403 - f1_m: 0.5719 - precision_m: 0.7652 - recall_m: 0.4594 - val_loss: 0.5675 - val_accuracy: 0.7386 - val_f1_m: 0.5614 - val_precision_m: 0.7428 - val_recall_m: 0.4572\n",
            "Epoch 214/300\n",
            "88/88 [==============================] - 0s 6ms/step - loss: 0.5534 - accuracy: 0.7390 - f1_m: 0.5697 - precision_m: 0.7622 - recall_m: 0.4594 - val_loss: 0.5675 - val_accuracy: 0.7382 - val_f1_m: 0.5617 - val_precision_m: 0.7405 - val_recall_m: 0.4581\n",
            "Epoch 215/300\n",
            "88/88 [==============================] - 1s 6ms/step - loss: 0.5533 - accuracy: 0.7400 - f1_m: 0.5731 - precision_m: 0.7656 - recall_m: 0.4619 - val_loss: 0.5678 - val_accuracy: 0.7393 - val_f1_m: 0.5661 - val_precision_m: 0.7395 - val_recall_m: 0.4641\n",
            "Epoch 216/300\n",
            "88/88 [==============================] - 1s 6ms/step - loss: 0.5533 - accuracy: 0.7401 - f1_m: 0.5719 - precision_m: 0.7658 - recall_m: 0.4605 - val_loss: 0.5676 - val_accuracy: 0.7393 - val_f1_m: 0.5656 - val_precision_m: 0.7411 - val_recall_m: 0.4631\n",
            "Epoch 217/300\n",
            "88/88 [==============================] - 1s 6ms/step - loss: 0.5532 - accuracy: 0.7408 - f1_m: 0.5731 - precision_m: 0.7663 - recall_m: 0.4613 - val_loss: 0.5677 - val_accuracy: 0.7393 - val_f1_m: 0.5656 - val_precision_m: 0.7411 - val_recall_m: 0.4631\n",
            "Epoch 218/300\n",
            "88/88 [==============================] - 1s 6ms/step - loss: 0.5532 - accuracy: 0.7400 - f1_m: 0.5734 - precision_m: 0.7649 - recall_m: 0.4636 - val_loss: 0.5674 - val_accuracy: 0.7386 - val_f1_m: 0.5620 - val_precision_m: 0.7418 - val_recall_m: 0.4581\n",
            "Epoch 219/300\n",
            "88/88 [==============================] - 0s 6ms/step - loss: 0.5531 - accuracy: 0.7403 - f1_m: 0.5713 - precision_m: 0.7639 - recall_m: 0.4601 - val_loss: 0.5675 - val_accuracy: 0.7389 - val_f1_m: 0.5635 - val_precision_m: 0.7416 - val_recall_m: 0.4601\n",
            "Epoch 220/300\n",
            "88/88 [==============================] - 1s 6ms/step - loss: 0.5530 - accuracy: 0.7401 - f1_m: 0.5735 - precision_m: 0.7657 - recall_m: 0.4621 - val_loss: 0.5679 - val_accuracy: 0.7411 - val_f1_m: 0.5709 - val_precision_m: 0.7393 - val_recall_m: 0.4705\n",
            "Epoch 221/300\n",
            "88/88 [==============================] - 1s 6ms/step - loss: 0.5530 - accuracy: 0.7398 - f1_m: 0.5708 - precision_m: 0.7613 - recall_m: 0.4611 - val_loss: 0.5676 - val_accuracy: 0.7396 - val_f1_m: 0.5663 - val_precision_m: 0.7413 - val_recall_m: 0.4639\n",
            "Epoch 222/300\n",
            "88/88 [==============================] - 1s 6ms/step - loss: 0.5529 - accuracy: 0.7404 - f1_m: 0.5729 - precision_m: 0.7655 - recall_m: 0.4624 - val_loss: 0.5675 - val_accuracy: 0.7396 - val_f1_m: 0.5668 - val_precision_m: 0.7404 - val_recall_m: 0.4646\n",
            "Epoch 223/300\n",
            "88/88 [==============================] - 0s 5ms/step - loss: 0.5528 - accuracy: 0.7407 - f1_m: 0.5740 - precision_m: 0.7653 - recall_m: 0.4632 - val_loss: 0.5673 - val_accuracy: 0.7375 - val_f1_m: 0.5585 - val_precision_m: 0.7413 - val_recall_m: 0.4539\n",
            "Epoch 224/300\n",
            "88/88 [==============================] - 0s 5ms/step - loss: 0.5528 - accuracy: 0.7396 - f1_m: 0.5707 - precision_m: 0.7646 - recall_m: 0.4586 - val_loss: 0.5677 - val_accuracy: 0.7404 - val_f1_m: 0.5699 - val_precision_m: 0.7379 - val_recall_m: 0.4695\n",
            "Epoch 225/300\n",
            "88/88 [==============================] - 1s 6ms/step - loss: 0.5527 - accuracy: 0.7400 - f1_m: 0.5728 - precision_m: 0.7660 - recall_m: 0.4616 - val_loss: 0.5682 - val_accuracy: 0.7411 - val_f1_m: 0.5737 - val_precision_m: 0.7372 - val_recall_m: 0.4745\n",
            "Epoch 226/300\n",
            "88/88 [==============================] - 1s 6ms/step - loss: 0.5527 - accuracy: 0.7408 - f1_m: 0.5764 - precision_m: 0.7662 - recall_m: 0.4660 - val_loss: 0.5674 - val_accuracy: 0.7396 - val_f1_m: 0.5661 - val_precision_m: 0.7411 - val_recall_m: 0.4634\n",
            "Epoch 227/300\n",
            "88/88 [==============================] - 1s 6ms/step - loss: 0.5527 - accuracy: 0.7393 - f1_m: 0.5727 - precision_m: 0.7611 - recall_m: 0.4632 - val_loss: 0.5674 - val_accuracy: 0.7396 - val_f1_m: 0.5672 - val_precision_m: 0.7386 - val_recall_m: 0.4652\n",
            "Epoch 228/300\n",
            "88/88 [==============================] - 0s 6ms/step - loss: 0.5526 - accuracy: 0.7399 - f1_m: 0.5722 - precision_m: 0.7616 - recall_m: 0.4615 - val_loss: 0.5673 - val_accuracy: 0.7393 - val_f1_m: 0.5651 - val_precision_m: 0.7403 - val_recall_m: 0.4624\n",
            "Epoch 229/300\n",
            "88/88 [==============================] - 0s 6ms/step - loss: 0.5525 - accuracy: 0.7401 - f1_m: 0.5721 - precision_m: 0.7652 - recall_m: 0.4615 - val_loss: 0.5676 - val_accuracy: 0.7404 - val_f1_m: 0.5699 - val_precision_m: 0.7380 - val_recall_m: 0.4696\n",
            "Epoch 230/300\n",
            "88/88 [==============================] - 1s 6ms/step - loss: 0.5525 - accuracy: 0.7394 - f1_m: 0.5723 - precision_m: 0.7624 - recall_m: 0.4617 - val_loss: 0.5676 - val_accuracy: 0.7404 - val_f1_m: 0.5699 - val_precision_m: 0.7380 - val_recall_m: 0.4696\n",
            "Epoch 231/300\n",
            "88/88 [==============================] - 1s 6ms/step - loss: 0.5524 - accuracy: 0.7407 - f1_m: 0.5761 - precision_m: 0.7684 - recall_m: 0.4655 - val_loss: 0.5674 - val_accuracy: 0.7400 - val_f1_m: 0.5686 - val_precision_m: 0.7383 - val_recall_m: 0.4674\n",
            "Epoch 232/300\n",
            "88/88 [==============================] - 0s 5ms/step - loss: 0.5523 - accuracy: 0.7396 - f1_m: 0.5746 - precision_m: 0.7642 - recall_m: 0.4645 - val_loss: 0.5672 - val_accuracy: 0.7396 - val_f1_m: 0.5652 - val_precision_m: 0.7408 - val_recall_m: 0.4624\n",
            "Epoch 233/300\n",
            "88/88 [==============================] - 0s 6ms/step - loss: 0.5523 - accuracy: 0.7400 - f1_m: 0.5731 - precision_m: 0.7647 - recall_m: 0.4631 - val_loss: 0.5676 - val_accuracy: 0.7411 - val_f1_m: 0.5719 - val_precision_m: 0.7392 - val_recall_m: 0.4714\n",
            "Epoch 234/300\n",
            "88/88 [==============================] - 0s 5ms/step - loss: 0.5523 - accuracy: 0.7399 - f1_m: 0.5738 - precision_m: 0.7617 - recall_m: 0.4645 - val_loss: 0.5673 - val_accuracy: 0.7400 - val_f1_m: 0.5681 - val_precision_m: 0.7393 - val_recall_m: 0.4664\n",
            "Epoch 235/300\n",
            "88/88 [==============================] - 1s 6ms/step - loss: 0.5522 - accuracy: 0.7407 - f1_m: 0.5745 - precision_m: 0.7652 - recall_m: 0.4653 - val_loss: 0.5672 - val_accuracy: 0.7393 - val_f1_m: 0.5656 - val_precision_m: 0.7395 - val_recall_m: 0.4633\n",
            "Epoch 236/300\n",
            "88/88 [==============================] - 1s 6ms/step - loss: 0.5521 - accuracy: 0.7395 - f1_m: 0.5732 - precision_m: 0.7634 - recall_m: 0.4629 - val_loss: 0.5675 - val_accuracy: 0.7411 - val_f1_m: 0.5719 - val_precision_m: 0.7392 - val_recall_m: 0.4714\n",
            "Epoch 237/300\n",
            "88/88 [==============================] - 0s 5ms/step - loss: 0.5520 - accuracy: 0.7407 - f1_m: 0.5765 - precision_m: 0.7622 - recall_m: 0.4672 - val_loss: 0.5670 - val_accuracy: 0.7389 - val_f1_m: 0.5622 - val_precision_m: 0.7416 - val_recall_m: 0.4583\n",
            "Epoch 238/300\n",
            "88/88 [==============================] - 1s 6ms/step - loss: 0.5520 - accuracy: 0.7401 - f1_m: 0.5753 - precision_m: 0.7639 - recall_m: 0.4655 - val_loss: 0.5670 - val_accuracy: 0.7389 - val_f1_m: 0.5625 - val_precision_m: 0.7406 - val_recall_m: 0.4591\n",
            "Epoch 239/300\n",
            "88/88 [==============================] - 1s 6ms/step - loss: 0.5520 - accuracy: 0.7408 - f1_m: 0.5739 - precision_m: 0.7657 - recall_m: 0.4634 - val_loss: 0.5675 - val_accuracy: 0.7414 - val_f1_m: 0.5741 - val_precision_m: 0.7390 - val_recall_m: 0.4746\n",
            "Epoch 240/300\n",
            "88/88 [==============================] - 1s 6ms/step - loss: 0.5519 - accuracy: 0.7407 - f1_m: 0.5758 - precision_m: 0.7635 - recall_m: 0.4671 - val_loss: 0.5671 - val_accuracy: 0.7407 - val_f1_m: 0.5693 - val_precision_m: 0.7416 - val_recall_m: 0.4674\n",
            "Epoch 241/300\n",
            "88/88 [==============================] - 1s 6ms/step - loss: 0.5519 - accuracy: 0.7404 - f1_m: 0.5741 - precision_m: 0.7642 - recall_m: 0.4645 - val_loss: 0.5671 - val_accuracy: 0.7400 - val_f1_m: 0.5679 - val_precision_m: 0.7389 - val_recall_m: 0.4665\n",
            "Epoch 242/300\n",
            "88/88 [==============================] - 0s 5ms/step - loss: 0.5518 - accuracy: 0.7407 - f1_m: 0.5794 - precision_m: 0.7622 - recall_m: 0.4724 - val_loss: 0.5669 - val_accuracy: 0.7382 - val_f1_m: 0.5606 - val_precision_m: 0.7410 - val_recall_m: 0.4564\n",
            "Epoch 243/300\n",
            "88/88 [==============================] - 1s 6ms/step - loss: 0.5517 - accuracy: 0.7412 - f1_m: 0.5763 - precision_m: 0.7684 - recall_m: 0.4651 - val_loss: 0.5677 - val_accuracy: 0.7411 - val_f1_m: 0.5744 - val_precision_m: 0.7370 - val_recall_m: 0.4756\n",
            "Epoch 244/300\n",
            "88/88 [==============================] - 1s 6ms/step - loss: 0.5517 - accuracy: 0.7416 - f1_m: 0.5787 - precision_m: 0.7634 - recall_m: 0.4694 - val_loss: 0.5674 - val_accuracy: 0.7429 - val_f1_m: 0.5762 - val_precision_m: 0.7429 - val_recall_m: 0.4756\n",
            "Epoch 245/300\n",
            "88/88 [==============================] - 1s 6ms/step - loss: 0.5516 - accuracy: 0.7404 - f1_m: 0.5763 - precision_m: 0.7625 - recall_m: 0.4678 - val_loss: 0.5672 - val_accuracy: 0.7414 - val_f1_m: 0.5722 - val_precision_m: 0.7409 - val_recall_m: 0.4714\n",
            "Epoch 246/300\n",
            "88/88 [==============================] - 0s 6ms/step - loss: 0.5515 - accuracy: 0.7402 - f1_m: 0.5745 - precision_m: 0.7623 - recall_m: 0.4655 - val_loss: 0.5674 - val_accuracy: 0.7411 - val_f1_m: 0.5744 - val_precision_m: 0.7370 - val_recall_m: 0.4756\n",
            "Epoch 247/300\n",
            "88/88 [==============================] - 1s 6ms/step - loss: 0.5515 - accuracy: 0.7416 - f1_m: 0.5792 - precision_m: 0.7649 - recall_m: 0.4707 - val_loss: 0.5669 - val_accuracy: 0.7393 - val_f1_m: 0.5646 - val_precision_m: 0.7401 - val_recall_m: 0.4621\n",
            "Epoch 248/300\n",
            "88/88 [==============================] - 0s 6ms/step - loss: 0.5515 - accuracy: 0.7408 - f1_m: 0.5753 - precision_m: 0.7623 - recall_m: 0.4657 - val_loss: 0.5674 - val_accuracy: 0.7407 - val_f1_m: 0.5741 - val_precision_m: 0.7359 - val_recall_m: 0.4756\n",
            "Epoch 249/300\n",
            "88/88 [==============================] - 0s 6ms/step - loss: 0.5514 - accuracy: 0.7410 - f1_m: 0.5779 - precision_m: 0.7585 - recall_m: 0.4701 - val_loss: 0.5669 - val_accuracy: 0.7411 - val_f1_m: 0.5700 - val_precision_m: 0.7411 - val_recall_m: 0.4683\n",
            "Epoch 250/300\n",
            "88/88 [==============================] - 1s 6ms/step - loss: 0.5514 - accuracy: 0.7410 - f1_m: 0.5775 - precision_m: 0.7616 - recall_m: 0.4696 - val_loss: 0.5669 - val_accuracy: 0.7407 - val_f1_m: 0.5690 - val_precision_m: 0.7404 - val_recall_m: 0.4673\n",
            "Epoch 251/300\n",
            "88/88 [==============================] - 1s 6ms/step - loss: 0.5513 - accuracy: 0.7411 - f1_m: 0.5771 - precision_m: 0.7645 - recall_m: 0.4682 - val_loss: 0.5668 - val_accuracy: 0.7404 - val_f1_m: 0.5672 - val_precision_m: 0.7413 - val_recall_m: 0.4653\n",
            "Epoch 252/300\n",
            "88/88 [==============================] - 1s 6ms/step - loss: 0.5512 - accuracy: 0.7402 - f1_m: 0.5743 - precision_m: 0.7602 - recall_m: 0.4655 - val_loss: 0.5671 - val_accuracy: 0.7411 - val_f1_m: 0.5734 - val_precision_m: 0.7387 - val_recall_m: 0.4735\n",
            "Epoch 253/300\n",
            "88/88 [==============================] - 1s 6ms/step - loss: 0.5512 - accuracy: 0.7413 - f1_m: 0.5788 - precision_m: 0.7633 - recall_m: 0.4692 - val_loss: 0.5670 - val_accuracy: 0.7414 - val_f1_m: 0.5722 - val_precision_m: 0.7409 - val_recall_m: 0.4714\n",
            "Epoch 254/300\n",
            "88/88 [==============================] - 0s 5ms/step - loss: 0.5512 - accuracy: 0.7413 - f1_m: 0.5768 - precision_m: 0.7646 - recall_m: 0.4690 - val_loss: 0.5672 - val_accuracy: 0.7407 - val_f1_m: 0.5748 - val_precision_m: 0.7356 - val_recall_m: 0.4764\n",
            "Epoch 255/300\n",
            "88/88 [==============================] - 1s 6ms/step - loss: 0.5511 - accuracy: 0.7406 - f1_m: 0.5766 - precision_m: 0.7604 - recall_m: 0.4684 - val_loss: 0.5674 - val_accuracy: 0.7396 - val_f1_m: 0.5749 - val_precision_m: 0.7313 - val_recall_m: 0.4784\n",
            "Epoch 256/300\n",
            "88/88 [==============================] - 1s 6ms/step - loss: 0.5511 - accuracy: 0.7415 - f1_m: 0.5794 - precision_m: 0.7586 - recall_m: 0.4730 - val_loss: 0.5667 - val_accuracy: 0.7400 - val_f1_m: 0.5674 - val_precision_m: 0.7388 - val_recall_m: 0.4663\n",
            "Epoch 257/300\n",
            "88/88 [==============================] - 0s 6ms/step - loss: 0.5510 - accuracy: 0.7416 - f1_m: 0.5787 - precision_m: 0.7655 - recall_m: 0.4700 - val_loss: 0.5670 - val_accuracy: 0.7411 - val_f1_m: 0.5746 - val_precision_m: 0.7373 - val_recall_m: 0.4754\n",
            "Epoch 258/300\n",
            "88/88 [==============================] - 0s 5ms/step - loss: 0.5511 - accuracy: 0.7417 - f1_m: 0.5785 - precision_m: 0.7598 - recall_m: 0.4724 - val_loss: 0.5669 - val_accuracy: 0.7411 - val_f1_m: 0.5734 - val_precision_m: 0.7384 - val_recall_m: 0.4735\n",
            "Epoch 259/300\n",
            "88/88 [==============================] - 1s 6ms/step - loss: 0.5510 - accuracy: 0.7413 - f1_m: 0.5770 - precision_m: 0.7624 - recall_m: 0.4688 - val_loss: 0.5670 - val_accuracy: 0.7407 - val_f1_m: 0.5743 - val_precision_m: 0.7366 - val_recall_m: 0.4754\n",
            "Epoch 260/300\n",
            "88/88 [==============================] - 1s 6ms/step - loss: 0.5509 - accuracy: 0.7416 - f1_m: 0.5799 - precision_m: 0.7646 - recall_m: 0.4709 - val_loss: 0.5673 - val_accuracy: 0.7400 - val_f1_m: 0.5760 - val_precision_m: 0.7307 - val_recall_m: 0.4804\n",
            "Epoch 261/300\n",
            "88/88 [==============================] - 1s 6ms/step - loss: 0.5509 - accuracy: 0.7410 - f1_m: 0.5792 - precision_m: 0.7621 - recall_m: 0.4706 - val_loss: 0.5673 - val_accuracy: 0.7400 - val_f1_m: 0.5760 - val_precision_m: 0.7307 - val_recall_m: 0.4804\n",
            "Epoch 262/300\n",
            "88/88 [==============================] - 1s 6ms/step - loss: 0.5509 - accuracy: 0.7414 - f1_m: 0.5795 - precision_m: 0.7597 - recall_m: 0.4730 - val_loss: 0.5667 - val_accuracy: 0.7404 - val_f1_m: 0.5706 - val_precision_m: 0.7377 - val_recall_m: 0.4703\n",
            "Epoch 263/300\n",
            "88/88 [==============================] - 1s 6ms/step - loss: 0.5508 - accuracy: 0.7414 - f1_m: 0.5797 - precision_m: 0.7623 - recall_m: 0.4721 - val_loss: 0.5668 - val_accuracy: 0.7414 - val_f1_m: 0.5749 - val_precision_m: 0.7381 - val_recall_m: 0.4754\n",
            "Epoch 264/300\n",
            "88/88 [==============================] - 0s 5ms/step - loss: 0.5507 - accuracy: 0.7415 - f1_m: 0.5769 - precision_m: 0.7567 - recall_m: 0.4697 - val_loss: 0.5668 - val_accuracy: 0.7407 - val_f1_m: 0.5748 - val_precision_m: 0.7356 - val_recall_m: 0.4763\n",
            "Epoch 265/300\n",
            "88/88 [==============================] - 1s 6ms/step - loss: 0.5506 - accuracy: 0.7421 - f1_m: 0.5824 - precision_m: 0.7665 - recall_m: 0.4736 - val_loss: 0.5671 - val_accuracy: 0.7393 - val_f1_m: 0.5749 - val_precision_m: 0.7291 - val_recall_m: 0.4792\n",
            "Epoch 266/300\n",
            "88/88 [==============================] - 1s 6ms/step - loss: 0.5506 - accuracy: 0.7413 - f1_m: 0.5803 - precision_m: 0.7585 - recall_m: 0.4742 - val_loss: 0.5667 - val_accuracy: 0.7414 - val_f1_m: 0.5743 - val_precision_m: 0.7387 - val_recall_m: 0.4742\n",
            "Epoch 267/300\n",
            "88/88 [==============================] - 1s 6ms/step - loss: 0.5506 - accuracy: 0.7419 - f1_m: 0.5798 - precision_m: 0.7597 - recall_m: 0.4730 - val_loss: 0.5664 - val_accuracy: 0.7396 - val_f1_m: 0.5656 - val_precision_m: 0.7400 - val_recall_m: 0.4633\n",
            "Epoch 268/300\n",
            "88/88 [==============================] - 1s 6ms/step - loss: 0.5505 - accuracy: 0.7421 - f1_m: 0.5803 - precision_m: 0.7620 - recall_m: 0.4724 - val_loss: 0.5665 - val_accuracy: 0.7386 - val_f1_m: 0.5655 - val_precision_m: 0.7346 - val_recall_m: 0.4653\n",
            "Epoch 269/300\n",
            "88/88 [==============================] - 1s 6ms/step - loss: 0.5504 - accuracy: 0.7417 - f1_m: 0.5805 - precision_m: 0.7649 - recall_m: 0.4718 - val_loss: 0.5666 - val_accuracy: 0.7400 - val_f1_m: 0.5729 - val_precision_m: 0.7345 - val_recall_m: 0.4741\n",
            "Epoch 270/300\n",
            "88/88 [==============================] - 1s 6ms/step - loss: 0.5504 - accuracy: 0.7404 - f1_m: 0.5794 - precision_m: 0.7560 - recall_m: 0.4738 - val_loss: 0.5664 - val_accuracy: 0.7400 - val_f1_m: 0.5659 - val_precision_m: 0.7414 - val_recall_m: 0.4633\n",
            "Epoch 271/300\n",
            "88/88 [==============================] - 1s 6ms/step - loss: 0.5505 - accuracy: 0.7418 - f1_m: 0.5766 - precision_m: 0.7583 - recall_m: 0.4707 - val_loss: 0.5664 - val_accuracy: 0.7400 - val_f1_m: 0.5654 - val_precision_m: 0.7431 - val_recall_m: 0.4622\n",
            "Epoch 272/300\n",
            "88/88 [==============================] - 1s 6ms/step - loss: 0.5504 - accuracy: 0.7414 - f1_m: 0.5783 - precision_m: 0.7613 - recall_m: 0.4708 - val_loss: 0.5666 - val_accuracy: 0.7400 - val_f1_m: 0.5736 - val_precision_m: 0.7344 - val_recall_m: 0.4751\n",
            "Epoch 273/300\n",
            "88/88 [==============================] - 0s 6ms/step - loss: 0.5504 - accuracy: 0.7420 - f1_m: 0.5819 - precision_m: 0.7631 - recall_m: 0.4752 - val_loss: 0.5668 - val_accuracy: 0.7393 - val_f1_m: 0.5750 - val_precision_m: 0.7294 - val_recall_m: 0.4792\n",
            "Epoch 274/300\n",
            "88/88 [==============================] - 1s 6ms/step - loss: 0.5502 - accuracy: 0.7422 - f1_m: 0.5807 - precision_m: 0.7607 - recall_m: 0.4729 - val_loss: 0.5666 - val_accuracy: 0.7393 - val_f1_m: 0.5744 - val_precision_m: 0.7302 - val_recall_m: 0.4780\n",
            "Epoch 275/300\n",
            "88/88 [==============================] - 1s 6ms/step - loss: 0.5502 - accuracy: 0.7417 - f1_m: 0.5819 - precision_m: 0.7584 - recall_m: 0.4763 - val_loss: 0.5664 - val_accuracy: 0.7389 - val_f1_m: 0.5693 - val_precision_m: 0.7335 - val_recall_m: 0.4700\n",
            "Epoch 276/300\n",
            "88/88 [==============================] - 1s 6ms/step - loss: 0.5502 - accuracy: 0.7419 - f1_m: 0.5823 - precision_m: 0.7626 - recall_m: 0.4750 - val_loss: 0.5665 - val_accuracy: 0.7400 - val_f1_m: 0.5746 - val_precision_m: 0.7334 - val_recall_m: 0.4769\n",
            "Epoch 277/300\n",
            "88/88 [==============================] - 1s 6ms/step - loss: 0.5501 - accuracy: 0.7421 - f1_m: 0.5806 - precision_m: 0.7614 - recall_m: 0.4740 - val_loss: 0.5665 - val_accuracy: 0.7396 - val_f1_m: 0.5748 - val_precision_m: 0.7315 - val_recall_m: 0.4780\n",
            "Epoch 278/300\n",
            "88/88 [==============================] - 1s 6ms/step - loss: 0.5501 - accuracy: 0.7413 - f1_m: 0.5802 - precision_m: 0.7569 - recall_m: 0.4736 - val_loss: 0.5664 - val_accuracy: 0.7386 - val_f1_m: 0.5703 - val_precision_m: 0.7314 - val_recall_m: 0.4720\n",
            "Epoch 279/300\n",
            "88/88 [==============================] - 1s 6ms/step - loss: 0.5501 - accuracy: 0.7410 - f1_m: 0.5798 - precision_m: 0.7596 - recall_m: 0.4732 - val_loss: 0.5662 - val_accuracy: 0.7386 - val_f1_m: 0.5672 - val_precision_m: 0.7346 - val_recall_m: 0.4670\n",
            "Epoch 280/300\n",
            "88/88 [==============================] - 1s 6ms/step - loss: 0.5500 - accuracy: 0.7421 - f1_m: 0.5806 - precision_m: 0.7616 - recall_m: 0.4735 - val_loss: 0.5666 - val_accuracy: 0.7389 - val_f1_m: 0.5755 - val_precision_m: 0.7267 - val_recall_m: 0.4812\n",
            "Epoch 281/300\n",
            "88/88 [==============================] - 1s 6ms/step - loss: 0.5499 - accuracy: 0.7422 - f1_m: 0.5831 - precision_m: 0.7631 - recall_m: 0.4762 - val_loss: 0.5667 - val_accuracy: 0.7382 - val_f1_m: 0.5754 - val_precision_m: 0.7241 - val_recall_m: 0.4822\n",
            "Epoch 282/300\n",
            "88/88 [==============================] - 1s 6ms/step - loss: 0.5499 - accuracy: 0.7421 - f1_m: 0.5829 - precision_m: 0.7598 - recall_m: 0.4765 - val_loss: 0.5663 - val_accuracy: 0.7386 - val_f1_m: 0.5707 - val_precision_m: 0.7306 - val_recall_m: 0.4729\n",
            "Epoch 283/300\n",
            "88/88 [==============================] - 1s 6ms/step - loss: 0.5498 - accuracy: 0.7422 - f1_m: 0.5813 - precision_m: 0.7644 - recall_m: 0.4738 - val_loss: 0.5670 - val_accuracy: 0.7364 - val_f1_m: 0.5752 - val_precision_m: 0.7171 - val_recall_m: 0.4850\n",
            "Epoch 284/300\n",
            "88/88 [==============================] - 1s 7ms/step - loss: 0.5498 - accuracy: 0.7416 - f1_m: 0.5822 - precision_m: 0.7580 - recall_m: 0.4766 - val_loss: 0.5664 - val_accuracy: 0.7386 - val_f1_m: 0.5737 - val_precision_m: 0.7277 - val_recall_m: 0.4780\n",
            "Epoch 285/300\n",
            "88/88 [==============================] - 1s 7ms/step - loss: 0.5498 - accuracy: 0.7421 - f1_m: 0.5830 - precision_m: 0.7612 - recall_m: 0.4765 - val_loss: 0.5668 - val_accuracy: 0.7368 - val_f1_m: 0.5745 - val_precision_m: 0.7193 - val_recall_m: 0.4831\n",
            "Epoch 286/300\n",
            "88/88 [==============================] - 1s 6ms/step - loss: 0.5497 - accuracy: 0.7413 - f1_m: 0.5811 - precision_m: 0.7549 - recall_m: 0.4773 - val_loss: 0.5661 - val_accuracy: 0.7389 - val_f1_m: 0.5675 - val_precision_m: 0.7358 - val_recall_m: 0.4670\n",
            "Epoch 287/300\n",
            "88/88 [==============================] - 1s 7ms/step - loss: 0.5497 - accuracy: 0.7420 - f1_m: 0.5807 - precision_m: 0.7608 - recall_m: 0.4730 - val_loss: 0.5664 - val_accuracy: 0.7379 - val_f1_m: 0.5734 - val_precision_m: 0.7249 - val_recall_m: 0.4789\n",
            "Epoch 288/300\n",
            "88/88 [==============================] - 1s 7ms/step - loss: 0.5496 - accuracy: 0.7421 - f1_m: 0.5834 - precision_m: 0.7610 - recall_m: 0.4772 - val_loss: 0.5663 - val_accuracy: 0.7382 - val_f1_m: 0.5729 - val_precision_m: 0.7272 - val_recall_m: 0.4769\n",
            "Epoch 289/300\n",
            "88/88 [==============================] - 1s 7ms/step - loss: 0.5496 - accuracy: 0.7423 - f1_m: 0.5818 - precision_m: 0.7625 - recall_m: 0.4754 - val_loss: 0.5666 - val_accuracy: 0.7375 - val_f1_m: 0.5752 - val_precision_m: 0.7215 - val_recall_m: 0.4831\n",
            "Epoch 290/300\n",
            "88/88 [==============================] - 1s 7ms/step - loss: 0.5495 - accuracy: 0.7415 - f1_m: 0.5814 - precision_m: 0.7536 - recall_m: 0.4762 - val_loss: 0.5660 - val_accuracy: 0.7393 - val_f1_m: 0.5692 - val_precision_m: 0.7358 - val_recall_m: 0.4690\n",
            "Epoch 291/300\n",
            "88/88 [==============================] - 1s 6ms/step - loss: 0.5495 - accuracy: 0.7429 - f1_m: 0.5827 - precision_m: 0.7654 - recall_m: 0.4754 - val_loss: 0.5667 - val_accuracy: 0.7364 - val_f1_m: 0.5746 - val_precision_m: 0.7173 - val_recall_m: 0.4841\n",
            "Epoch 292/300\n",
            "88/88 [==============================] - 1s 7ms/step - loss: 0.5495 - accuracy: 0.7421 - f1_m: 0.5835 - precision_m: 0.7570 - recall_m: 0.4786 - val_loss: 0.5661 - val_accuracy: 0.7375 - val_f1_m: 0.5707 - val_precision_m: 0.7266 - val_recall_m: 0.4740\n",
            "Epoch 293/300\n",
            "88/88 [==============================] - 1s 7ms/step - loss: 0.5494 - accuracy: 0.7421 - f1_m: 0.5816 - precision_m: 0.7581 - recall_m: 0.4753 - val_loss: 0.5662 - val_accuracy: 0.7375 - val_f1_m: 0.5731 - val_precision_m: 0.7236 - val_recall_m: 0.4789\n",
            "Epoch 294/300\n",
            "88/88 [==============================] - 1s 7ms/step - loss: 0.5494 - accuracy: 0.7425 - f1_m: 0.5834 - precision_m: 0.7610 - recall_m: 0.4769 - val_loss: 0.5661 - val_accuracy: 0.7371 - val_f1_m: 0.5715 - val_precision_m: 0.7247 - val_recall_m: 0.4759\n",
            "Epoch 295/300\n",
            "88/88 [==============================] - 1s 7ms/step - loss: 0.5493 - accuracy: 0.7426 - f1_m: 0.5820 - precision_m: 0.7593 - recall_m: 0.4759 - val_loss: 0.5660 - val_accuracy: 0.7379 - val_f1_m: 0.5715 - val_precision_m: 0.7272 - val_recall_m: 0.4749\n",
            "Epoch 296/300\n",
            "88/88 [==============================] - 1s 7ms/step - loss: 0.5492 - accuracy: 0.7422 - f1_m: 0.5820 - precision_m: 0.7560 - recall_m: 0.4778 - val_loss: 0.5658 - val_accuracy: 0.7393 - val_f1_m: 0.5643 - val_precision_m: 0.7400 - val_recall_m: 0.4606\n",
            "Epoch 297/300\n",
            "88/88 [==============================] - 1s 6ms/step - loss: 0.5492 - accuracy: 0.7421 - f1_m: 0.5816 - precision_m: 0.7599 - recall_m: 0.4753 - val_loss: 0.5659 - val_accuracy: 0.7386 - val_f1_m: 0.5698 - val_precision_m: 0.7324 - val_recall_m: 0.4711\n",
            "Epoch 298/300\n",
            "88/88 [==============================] - 1s 6ms/step - loss: 0.5492 - accuracy: 0.7419 - f1_m: 0.5822 - precision_m: 0.7608 - recall_m: 0.4749 - val_loss: 0.5669 - val_accuracy: 0.7364 - val_f1_m: 0.5787 - val_precision_m: 0.7128 - val_recall_m: 0.4920\n",
            "Epoch 299/300\n",
            "88/88 [==============================] - 1s 6ms/step - loss: 0.5492 - accuracy: 0.7418 - f1_m: 0.5842 - precision_m: 0.7600 - recall_m: 0.4789 - val_loss: 0.5658 - val_accuracy: 0.7386 - val_f1_m: 0.5702 - val_precision_m: 0.7317 - val_recall_m: 0.4720\n",
            "Epoch 300/300\n",
            "88/88 [==============================] - 1s 6ms/step - loss: 0.5490 - accuracy: 0.7418 - f1_m: 0.5816 - precision_m: 0.7585 - recall_m: 0.4750 - val_loss: 0.5659 - val_accuracy: 0.7371 - val_f1_m: 0.5700 - val_precision_m: 0.7255 - val_recall_m: 0.4739\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8w7UIdtIWuCD"
      },
      "source": [
        "## Analyze and comment the training results\n",
        "\n",
        "here goes any comment/visualization of the training history and any initial consideration on the training results  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UeWBr2syQpqM"
      },
      "source": [
        "Visualizzo ora il training history e il grafico che riassume la loss e l'accuracy del training e della validazione."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "smZC9EwAJRGP",
        "outputId": "8dcd2cfb-14bc-48c4-f7f1-377b09b17dcf"
      },
      "source": [
        "print('history dict:', history.history)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "history dict: {'loss': [0.6928974986076355, 0.679283857345581, 0.671806275844574, 0.6669813394546509, 0.6635037064552307, 0.6607262492179871, 0.6584093570709229, 0.6564255356788635, 0.6546501517295837, 0.6529910564422607, 0.651432991027832, 0.6498768925666809, 0.6483328342437744, 0.6467543244361877, 0.6450706124305725, 0.6432973146438599, 0.6415086388587952, 0.6397655010223389, 0.6380029916763306, 0.6362161040306091, 0.6344086527824402, 0.6325723528862, 0.6307286620140076, 0.6288773417472839, 0.6270195841789246, 0.6251474022865295, 0.623275101184845, 0.6213984489440918, 0.6195276379585266, 0.617659330368042, 0.6157807111740112, 0.6139562129974365, 0.612152636051178, 0.6103790998458862, 0.6086375117301941, 0.6069695949554443, 0.6053430438041687, 0.6037756204605103, 0.6022614240646362, 0.6008132696151733, 0.5994254946708679, 0.5980519652366638, 0.5967886447906494, 0.5955371260643005, 0.5943451523780823, 0.5931766033172607, 0.592066764831543, 0.590995192527771, 0.5899688005447388, 0.5889878869056702, 0.5880330801010132, 0.5871268510818481, 0.5862449407577515, 0.5854213833808899, 0.5846031904220581, 0.5838339924812317, 0.5830969214439392, 0.5823720097541809, 0.5816940069198608, 0.581028163433075, 0.5803514719009399, 0.579807460308075, 0.5792131423950195, 0.5786213278770447, 0.5781622529029846, 0.5776290893554688, 0.5771277546882629, 0.5766858458518982, 0.5761526823043823, 0.5758147835731506, 0.5753673911094666, 0.5750008821487427, 0.5745896100997925, 0.5742167234420776, 0.5738402605056763, 0.5735096335411072, 0.5731629133224487, 0.5728455185890198, 0.57246994972229, 0.5722115635871887, 0.5719022154808044, 0.5716284513473511, 0.5713201761245728, 0.5710533261299133, 0.5707530975341797, 0.5704934597015381, 0.5702505707740784, 0.5699796676635742, 0.569797933101654, 0.569502055644989, 0.5692040920257568, 0.5690329670906067, 0.5687291622161865, 0.5685131549835205, 0.5683177709579468, 0.5681407451629639, 0.5679391026496887, 0.5676314234733582, 0.5675184726715088, 0.567291796207428, 0.5670934915542603, 0.5669159293174744, 0.5667080879211426, 0.566505491733551, 0.5662928819656372, 0.5661235451698303, 0.5659245848655701, 0.5657477974891663, 0.5655546188354492, 0.5654106736183167, 0.565214991569519, 0.5650764107704163, 0.5647805333137512, 0.5648139119148254, 0.5645756125450134, 0.5643458366394043, 0.5642024278640747, 0.5640417337417603, 0.5638514757156372, 0.5637367367744446, 0.5635585784912109, 0.5633691549301147, 0.5632292628288269, 0.5630552172660828, 0.5628439784049988, 0.5627919435501099, 0.5626066327095032, 0.5624218583106995, 0.5623021721839905, 0.5621433854103088, 0.561897873878479, 0.5617558360099792, 0.561728298664093, 0.5615156292915344, 0.5613572597503662, 0.5612719655036926, 0.5610783100128174, 0.5609684586524963, 0.5607203245162964, 0.5606234073638916, 0.5605167150497437, 0.560382068157196, 0.5601701140403748, 0.5599947571754456, 0.5598579049110413, 0.5597913265228271, 0.5595903396606445, 0.5595386624336243, 0.559415876865387, 0.55923992395401, 0.5591387152671814, 0.5590061545372009, 0.5588991045951843, 0.558755099773407, 0.5586183071136475, 0.5584467053413391, 0.5583988428115845, 0.5582717657089233, 0.5581690073013306, 0.5580281019210815, 0.5577978491783142, 0.5577694177627563, 0.5576711893081665, 0.557604193687439, 0.5574922561645508, 0.5573000907897949, 0.5571471452713013, 0.5571673512458801, 0.5569822788238525, 0.5569303035736084, 0.5567918419837952, 0.5566246509552002, 0.5566040873527527, 0.5565123558044434, 0.5564286708831787, 0.5562561750411987, 0.5561999678611755, 0.5561094880104065, 0.5560572743415833, 0.5559365153312683, 0.5557774305343628, 0.5557920932769775, 0.5556748509407043, 0.5555776357650757, 0.5553885102272034, 0.5554437637329102, 0.5552904605865479, 0.555306613445282, 0.5552130341529846, 0.55512535572052, 0.5549958348274231, 0.5549631714820862, 0.5548726916313171, 0.554787278175354, 0.5547577142715454, 0.5546458959579468, 0.554523766040802, 0.5544639229774475, 0.5543606281280518, 0.5543322563171387, 0.5543245077133179, 0.5541976094245911, 0.554061233997345, 0.5540890693664551, 0.5540450811386108, 0.5539087057113647, 0.5539311170578003, 0.5537754893302917, 0.5537259578704834, 0.5536400675773621, 0.5535258054733276, 0.5535758137702942, 0.5534699559211731, 0.5534461140632629, 0.55330890417099, 0.5532504320144653, 0.5532019138336182, 0.5531588196754456, 0.553141176700592, 0.5529953241348267, 0.5529816746711731, 0.552939772605896, 0.5528066158294678, 0.5527758598327637, 0.5527117252349854, 0.5526759028434753, 0.5526697635650635, 0.5525705218315125, 0.552490234375, 0.5524503588676453, 0.5523782968521118, 0.5522589087486267, 0.552270233631134, 0.552269697189331, 0.552161693572998, 0.552086591720581, 0.552030622959137, 0.5519959926605225, 0.5519552230834961, 0.5519315004348755, 0.5518681406974792, 0.5517576932907104, 0.551688015460968, 0.5517391562461853, 0.5516420006752014, 0.5515224933624268, 0.5515374541282654, 0.5515275597572327, 0.5513944029808044, 0.5513909459114075, 0.551347553730011, 0.5512484908103943, 0.5512382984161377, 0.551202118396759, 0.5510591268539429, 0.5510997772216797, 0.5510441660881042, 0.5510633587837219, 0.5509554147720337, 0.5508715510368347, 0.5508506298065186, 0.5508500337600708, 0.5507764220237732, 0.5506873726844788, 0.5506165623664856, 0.5506197810173035, 0.5505545735359192, 0.5505447387695312, 0.550432026386261, 0.5504045486450195, 0.5504870414733887, 0.5503910183906555, 0.5503711104393005, 0.5502254366874695, 0.5501991510391235, 0.5502057075500488, 0.5500909686088562, 0.5500519871711731, 0.5500672459602356, 0.5499794483184814, 0.5499182939529419, 0.5499173998832703, 0.5498248934745789, 0.549821674823761, 0.5497593283653259, 0.5496522784233093, 0.5497024655342102, 0.5496174693107605, 0.5495774149894714, 0.5494731664657593, 0.5494869351387024, 0.54951012134552, 0.5494464635848999, 0.5493655204772949, 0.5493462085723877, 0.5492367744445801, 0.5492067337036133, 0.5491546392440796, 0.549193263053894, 0.5490430593490601], 'accuracy': [0.5148214101791382, 0.634196400642395, 0.6186606884002686, 0.617946445941925, 0.6178571581840515, 0.6178571581840515, 0.617767870426178, 0.617767870426178, 0.6180357336997986, 0.6183928847312927, 0.6184821724891663, 0.6194642782211304, 0.6200000047683716, 0.6208928823471069, 0.6219642758369446, 0.6229464411735535, 0.6236607432365417, 0.6270535588264465, 0.6291964054107666, 0.6347321271896362, 0.6411607265472412, 0.6497321724891663, 0.6596428751945496, 0.6657142639160156, 0.6702678799629211, 0.6748214364051819, 0.6775000095367432, 0.6825892925262451, 0.6859821677207947, 0.6878571510314941, 0.6913393139839172, 0.6951785683631897, 0.6963393092155457, 0.7013393044471741, 0.7013393044471741, 0.705089271068573, 0.7072321176528931, 0.7081249952316284, 0.7093750238418579, 0.7113392949104309, 0.7140178680419922, 0.7146428823471069, 0.7149999737739563, 0.7163392901420593, 0.7163392901420593, 0.7174107432365417, 0.7191964387893677, 0.7197321653366089, 0.7202678322792053, 0.7214285731315613, 0.7216071486473083, 0.7222321629524231, 0.7225000262260437, 0.7224107384681702, 0.722678542137146, 0.7227678298950195, 0.7242857217788696, 0.7244642972946167, 0.7241071462631226, 0.7254464030265808, 0.725178599357605, 0.7268750071525574, 0.7272321581840515, 0.7275000214576721, 0.7267857193946838, 0.7276785969734192, 0.7275000214576721, 0.7284821271896362, 0.7297321557998657, 0.7283035516738892, 0.7294642925262451, 0.7297321557998657, 0.7303571701049805, 0.7313392758369446, 0.7315178513526917, 0.7319642901420593, 0.7316964268684387, 0.7326785922050476, 0.7325000166893005, 0.7328571677207947, 0.7331249713897705, 0.7326785922050476, 0.7331249713897705, 0.7330357432365417, 0.7331249713897705, 0.734375, 0.7330357432365417, 0.7341964244842529, 0.7346428632736206, 0.7345535755157471, 0.7348214387893677, 0.7353571653366089, 0.7353571653366089, 0.7350893020629883, 0.7352678775787354, 0.7351785898208618, 0.7351785898208618, 0.7358035445213318, 0.7358928322792053, 0.7358928322792053, 0.7361606955528259, 0.7354464530944824, 0.7361606955528259, 0.7358928322792053, 0.7358035445213318, 0.7366964221000671, 0.7366071343421936, 0.736339271068573, 0.7364285588264465, 0.7364285588264465, 0.7365178465843201, 0.7361606955528259, 0.7364285588264465, 0.7361606955528259, 0.7366964221000671, 0.7370535731315613, 0.7367857098579407, 0.7372321486473083, 0.7370535731315613, 0.7368749976158142, 0.7371428608894348, 0.7369642853736877, 0.7373214364051819, 0.7365178465843201, 0.7378571629524231, 0.7366071343421936, 0.7369642853736877, 0.7372321486473083, 0.7368749976158142, 0.7378571629524231, 0.7378571629524231, 0.7373214364051819, 0.7371428608894348, 0.7375892996788025, 0.737500011920929, 0.737678587436676, 0.7383928298950195, 0.7385714054107666, 0.7382143139839172, 0.7374107241630554, 0.7391071319580078, 0.7389285564422607, 0.7382143139839172, 0.7389285564422607, 0.7381250262260437, 0.7387499809265137, 0.739464282989502, 0.7385714054107666, 0.7386606931686401, 0.739464282989502, 0.7400892972946167, 0.7398214340209961, 0.739464282989502, 0.7392857074737549, 0.7393749952316284, 0.7390178442001343, 0.7397321462631226, 0.7387499809265137, 0.739464282989502, 0.7393749952316284, 0.739642858505249, 0.739464282989502, 0.7400892972946167, 0.7398214340209961, 0.7400892972946167, 0.739464282989502, 0.7398214340209961, 0.739464282989502, 0.739642858505249, 0.7398214340209961, 0.7399107217788696, 0.739464282989502, 0.7399107217788696, 0.7400892972946167, 0.7395535707473755, 0.7391071319580078, 0.739464282989502, 0.739642858505249, 0.7391071319580078, 0.7391964197158813, 0.7399107217788696, 0.7398214340209961, 0.7397321462631226, 0.7393749952316284, 0.7399107217788696, 0.7400000095367432, 0.7398214340209961, 0.7391071319580078, 0.7397321462631226, 0.7395535707473755, 0.7401785850524902, 0.7399107217788696, 0.7400000095367432, 0.7400000095367432, 0.7402678728103638, 0.7395535707473755, 0.7397321462631226, 0.7393749952316284, 0.7395535707473755, 0.7399107217788696, 0.7397321462631226, 0.7395535707473755, 0.7395535707473755, 0.7397321462631226, 0.7395535707473755, 0.739464282989502, 0.7398214340209961, 0.7397321462631226, 0.7400892972946167, 0.7395535707473755, 0.7406250238418579, 0.7397321462631226, 0.7402678728103638, 0.7390178442001343, 0.7400000095367432, 0.7400892972946167, 0.740803599357605, 0.7400000095367432, 0.7402678728103638, 0.7400892972946167, 0.7398214340209961, 0.7403571605682373, 0.7407143115997314, 0.7395535707473755, 0.7400000095367432, 0.740803599357605, 0.7392857074737549, 0.7399107217788696, 0.7400892972946167, 0.7393749952316284, 0.7407143115997314, 0.739642858505249, 0.7400000095367432, 0.7399107217788696, 0.7407143115997314, 0.739464282989502, 0.7407143115997314, 0.7400892972946167, 0.740803599357605, 0.7407143115997314, 0.7404464483261108, 0.7407143115997314, 0.7411606907844543, 0.741607129573822, 0.7404464483261108, 0.7401785850524902, 0.741607129573822, 0.740803599357605, 0.7409821152687073, 0.7409821152687073, 0.7410714030265808, 0.7401785850524902, 0.7413392663002014, 0.7413392663002014, 0.7406250238418579, 0.7415178418159485, 0.741607129573822, 0.7416964173316956, 0.7413392663002014, 0.741607129573822, 0.7409821152687073, 0.741428554058075, 0.741428554058075, 0.7415178418159485, 0.7420535683631897, 0.7413392663002014, 0.7418749928474426, 0.7420535683631897, 0.7416964173316956, 0.7403571605682373, 0.7417857050895691, 0.741428554058075, 0.7419642806053162, 0.7422321438789368, 0.7416964173316956, 0.7418749928474426, 0.7421428561210632, 0.7413392663002014, 0.7409821152687073, 0.7421428561210632, 0.7422321438789368, 0.7421428561210632, 0.7422321438789368, 0.741607129573822, 0.7420535683631897, 0.7413392663002014, 0.7419642806053162, 0.7421428561210632, 0.7423214316368103, 0.7415178418159485, 0.7428571581840515, 0.7421428561210632, 0.7420535683631897, 0.7425000071525574, 0.7425892949104309, 0.7422321438789368, 0.7421428561210632, 0.7418749928474426, 0.7417857050895691, 0.7417857050895691], 'f1_m': [0.5238495469093323, 0.11969130486249924, 0.006462055258452892, 0.0020060890819877386, 0.00042881647823378444, 0.00041322308243252337, 0.00044563275878317654, 0.0003551135887391865, 0.001832342124544084, 0.0038802577182650566, 0.004050149116665125, 0.00901192519813776, 0.012347965501248837, 0.017323441803455353, 0.024834472686052322, 0.032539550215005875, 0.036705322563648224, 0.057294655591249466, 0.06954096257686615, 0.10097251087427139, 0.13411399722099304, 0.17445318400859833, 0.2231660932302475, 0.2542276382446289, 0.27863332629203796, 0.3001827299594879, 0.3153705894947052, 0.3387095630168915, 0.354336142539978, 0.3618455231189728, 0.3818897008895874, 0.39706656336784363, 0.40228697657585144, 0.42406436800956726, 0.42500221729278564, 0.4397350549697876, 0.4508649706840515, 0.4569650888442993, 0.4609163999557495, 0.4675147235393524, 0.47740668058395386, 0.4820334315299988, 0.4825531840324402, 0.487362265586853, 0.4869985282421112, 0.4928290545940399, 0.49952298402786255, 0.5006285309791565, 0.5021425485610962, 0.5030038952827454, 0.5051436424255371, 0.5096842050552368, 0.5098814368247986, 0.5119007229804993, 0.5133159160614014, 0.5152587294578552, 0.5208883285522461, 0.520065188407898, 0.5171908736228943, 0.5228283405303955, 0.5220071077346802, 0.5264933109283447, 0.5314532518386841, 0.5348023176193237, 0.5264102220535278, 0.5303220152854919, 0.5344411730766296, 0.532710611820221, 0.5408620238304138, 0.5337964296340942, 0.5387670397758484, 0.5349531173706055, 0.5420233011245728, 0.5434548854827881, 0.5435262322425842, 0.5448790788650513, 0.5438117384910583, 0.5468489527702332, 0.5469704866409302, 0.5481795072555542, 0.5470883846282959, 0.5458794236183167, 0.5503218173980713, 0.5491642355918884, 0.5489751100540161, 0.5542975068092346, 0.5498403310775757, 0.5530600547790527, 0.5536177158355713, 0.5511747002601624, 0.5548022389411926, 0.5563264489173889, 0.5566033720970154, 0.5566985011100769, 0.5550545454025269, 0.5551131367683411, 0.5569044947624207, 0.5573077201843262, 0.5568525195121765, 0.5585245490074158, 0.5600298643112183, 0.5572299361228943, 0.5579980611801147, 0.5592870116233826, 0.5585601925849915, 0.5598962306976318, 0.5602688193321228, 0.5608289837837219, 0.560348391532898, 0.5611200332641602, 0.5616316795349121, 0.5600647330284119, 0.5606455206871033, 0.5607540011405945, 0.5615631341934204, 0.5609518885612488, 0.5619444251060486, 0.5622732043266296, 0.5621743202209473, 0.5624473094940186, 0.562265932559967, 0.5622780919075012, 0.5649282336235046, 0.5626726746559143, 0.5671015381813049, 0.5641955733299255, 0.5611250400543213, 0.5618168711662292, 0.5612528920173645, 0.5652464032173157, 0.5634758472442627, 0.5652729272842407, 0.562592625617981, 0.563517153263092, 0.5644077658653259, 0.5634123682975769, 0.5666382908821106, 0.5667761564254761, 0.5656639933586121, 0.5636206269264221, 0.567298948764801, 0.5678907036781311, 0.5641489624977112, 0.5678331255912781, 0.5643417835235596, 0.5665430426597595, 0.5688666701316833, 0.566555917263031, 0.5677232146263123, 0.566929280757904, 0.5702237486839294, 0.5692660808563232, 0.5666152834892273, 0.5701513886451721, 0.5702487230300903, 0.567057192325592, 0.5699141025543213, 0.5646116137504578, 0.5694072842597961, 0.5671053528785706, 0.5702587962150574, 0.5679231286048889, 0.5692047476768494, 0.5691990256309509, 0.5714030861854553, 0.5704159140586853, 0.5705298781394958, 0.567654013633728, 0.5700569152832031, 0.569440484046936, 0.5684435367584229, 0.5672001242637634, 0.5721262693405151, 0.5703935027122498, 0.5719470381736755, 0.5680894255638123, 0.5692827701568604, 0.5710360407829285, 0.5691502094268799, 0.5680453777313232, 0.5706114768981934, 0.5717712044715881, 0.5701656937599182, 0.5702874660491943, 0.5705682039260864, 0.57157301902771, 0.5703842639923096, 0.5696716904640198, 0.5707114934921265, 0.5710193514823914, 0.5701941251754761, 0.5730788707733154, 0.5721632242202759, 0.5709596276283264, 0.5731789469718933, 0.5714768767356873, 0.5724084377288818, 0.568796694278717, 0.5689069628715515, 0.5726955533027649, 0.5697984099388123, 0.5703740119934082, 0.5694165229797363, 0.5705309510231018, 0.5701452493667603, 0.5711254477500916, 0.5727393627166748, 0.5708733201026917, 0.5699173212051392, 0.5712617635726929, 0.5725017189979553, 0.5714895129203796, 0.5719142556190491, 0.56972736120224, 0.5730631947517395, 0.571874737739563, 0.5731456875801086, 0.573363184928894, 0.5713016390800476, 0.573517382144928, 0.5708169341087341, 0.5729340314865112, 0.573993444442749, 0.5706618428230286, 0.572766900062561, 0.5763911604881287, 0.5727468729019165, 0.5721574425697327, 0.5721384286880493, 0.5722659230232239, 0.5760600566864014, 0.5745567679405212, 0.5730652213096619, 0.5738115906715393, 0.5745239853858948, 0.5732139945030212, 0.5765275955200195, 0.5752628445625305, 0.5738778710365295, 0.5757941603660583, 0.5740787386894226, 0.5793985724449158, 0.5762871503829956, 0.5786585211753845, 0.5762901902198792, 0.5745411515235901, 0.5791683793067932, 0.5752511024475098, 0.5778617858886719, 0.5774819850921631, 0.5771411061286926, 0.5742934942245483, 0.5787702202796936, 0.5767660737037659, 0.5766294002532959, 0.5794230103492737, 0.5787230134010315, 0.5785008072853088, 0.5769995450973511, 0.5799314379692078, 0.5792052745819092, 0.5794910788536072, 0.5796982645988464, 0.5768865346908569, 0.5823872089385986, 0.5803380012512207, 0.5797844529151917, 0.5802810192108154, 0.5805140733718872, 0.5794023871421814, 0.5765823721885681, 0.5782549977302551, 0.5818754434585571, 0.5807483196258545, 0.581916093826294, 0.5823269486427307, 0.5806495547294617, 0.5802242755889893, 0.5797634720802307, 0.5806243419647217, 0.5830603241920471, 0.5828608870506287, 0.5812966227531433, 0.5822083950042725, 0.5829975605010986, 0.5811373591423035, 0.5806825160980225, 0.5834090709686279, 0.5817546248435974, 0.5813732743263245, 0.5827159881591797, 0.5834895372390747, 0.5815696120262146, 0.5834486484527588, 0.5819710493087769, 0.5819968581199646, 0.5816311836242676, 0.5821600556373596, 0.5841806530952454, 0.5816445350646973], 'precision_m': [0.47247838973999023, 0.5428112745285034, 0.14772726595401764, 0.045454539358615875, 0.011363634839653969, 0.011363634839653969, 0.011363634839653969, 0.011363634839653969, 0.045454539358615875, 0.07954544574022293, 0.10227271914482117, 0.22727270424365997, 0.2613636255264282, 0.3011363744735718, 0.4791666269302368, 0.49810609221458435, 0.5450757741928101, 0.6702651977539062, 0.7550865411758423, 0.7928390502929688, 0.8505590558052063, 0.8644567131996155, 0.8627027869224548, 0.8539016842842102, 0.8410797119140625, 0.8423660397529602, 0.8278900384902954, 0.8297094702720642, 0.8205125331878662, 0.813983142375946, 0.8092935085296631, 0.804373025894165, 0.8053664565086365, 0.8005509972572327, 0.7971507906913757, 0.7951288223266602, 0.7913312315940857, 0.7869356274604797, 0.7865228056907654, 0.7856664061546326, 0.7847810387611389, 0.7798353433609009, 0.7821062207221985, 0.7820939421653748, 0.776241660118103, 0.779696524143219, 0.7823578119277954, 0.7781215906143188, 0.7797654867172241, 0.778455376625061, 0.7802526354789734, 0.7756660580635071, 0.77994704246521, 0.7752750515937805, 0.7712242603302002, 0.7729247808456421, 0.7741254568099976, 0.7719629406929016, 0.7707134485244751, 0.7724984884262085, 0.7756679654121399, 0.7683969736099243, 0.7699189782142639, 0.767905056476593, 0.7724207043647766, 0.7663642764091492, 0.7713919281959534, 0.7695392370223999, 0.7690200209617615, 0.7683019638061523, 0.7702531218528748, 0.7695768475532532, 0.7709704041481018, 0.7701370716094971, 0.7698479294776917, 0.7711077332496643, 0.7672443389892578, 0.7718827724456787, 0.7665770053863525, 0.7720972299575806, 0.7715545892715454, 0.7649391293525696, 0.7708162069320679, 0.7706167101860046, 0.7689173221588135, 0.7709025740623474, 0.7666730880737305, 0.7707186341285706, 0.7652377486228943, 0.7676210403442383, 0.7679979205131531, 0.7727880477905273, 0.7689598202705383, 0.767844557762146, 0.7739347815513611, 0.7662050127983093, 0.7695003151893616, 0.7656329870223999, 0.7698925137519836, 0.770267903804779, 0.7706567645072937, 0.7644442915916443, 0.7687673568725586, 0.7708415389060974, 0.7658994793891907, 0.7672060132026672, 0.7702804207801819, 0.7684321999549866, 0.7661457061767578, 0.7663658261299133, 0.7681705951690674, 0.7660858035087585, 0.7655101418495178, 0.7670074105262756, 0.7706827521324158, 0.76558917760849, 0.7685248851776123, 0.7671403884887695, 0.7674549221992493, 0.768119752407074, 0.7649911642074585, 0.7662095427513123, 0.7668257355690002, 0.7675163149833679, 0.7676432132720947, 0.7699916362762451, 0.7644005417823792, 0.7645776271820068, 0.7655025124549866, 0.7658680081367493, 0.7665658593177795, 0.7651344537734985, 0.7663185000419617, 0.7644092440605164, 0.7658928036689758, 0.7668306231498718, 0.7678379416465759, 0.7678781747817993, 0.7666788697242737, 0.7657472491264343, 0.7673010230064392, 0.7661823034286499, 0.7662883996963501, 0.7670305371284485, 0.7672126889228821, 0.7691265344619751, 0.7645586133003235, 0.766696572303772, 0.7680988311767578, 0.7701877355575562, 0.7687746286392212, 0.7680664658546448, 0.7651355862617493, 0.770551860332489, 0.7668426632881165, 0.7691128253936768, 0.7676445245742798, 0.7658928036689758, 0.7672557234764099, 0.7661585211753845, 0.7714437246322632, 0.7686994075775146, 0.7664670348167419, 0.7671886682510376, 0.7712158560752869, 0.7688542008399963, 0.7688669562339783, 0.7683348059654236, 0.7655382752418518, 0.7692741751670837, 0.767338216304779, 0.7655777335166931, 0.7700645923614502, 0.7657686471939087, 0.7707658410072327, 0.7664499282836914, 0.7644827365875244, 0.7696596384048462, 0.7678167819976807, 0.7630544304847717, 0.7687118053436279, 0.7672199606895447, 0.7666067481040955, 0.7686685919761658, 0.7702090740203857, 0.7685129046440125, 0.7662014365196228, 0.7690349221229553, 0.7680137157440186, 0.7647992968559265, 0.7655582427978516, 0.7684249877929688, 0.7695667743682861, 0.7669160962104797, 0.767791211605072, 0.7673898935317993, 0.7659620046615601, 0.7642061710357666, 0.7696447968482971, 0.7658409476280212, 0.7678627371788025, 0.7683485150337219, 0.7659740447998047, 0.7674093246459961, 0.7610704898834229, 0.7665010690689087, 0.7657347321510315, 0.7630760669708252, 0.7663993239402771, 0.7657791376113892, 0.7653293609619141, 0.7648847103118896, 0.7652202844619751, 0.7621557116508484, 0.7655572295188904, 0.7658029198646545, 0.7663066387176514, 0.7648969292640686, 0.7638921737670898, 0.7656984925270081, 0.761345624923706, 0.765537679195404, 0.7653124332427979, 0.7646051645278931, 0.7659875750541687, 0.7661872506141663, 0.7610710263252258, 0.7616167068481445, 0.7652150988578796, 0.7623640298843384, 0.7683675289154053, 0.764168381690979, 0.7646596431732178, 0.761735737323761, 0.7651805877685547, 0.763388991355896, 0.7622232437133789, 0.763934850692749, 0.7657102346420288, 0.7634508013725281, 0.764230489730835, 0.7622038125991821, 0.7684028744697571, 0.7634291052818298, 0.7625095844268799, 0.7622890472412109, 0.764880359172821, 0.7622811794281006, 0.7585086822509766, 0.7616186738014221, 0.764523446559906, 0.7601951956748962, 0.763310432434082, 0.7646059393882751, 0.7603908181190491, 0.7585647106170654, 0.7654731869697571, 0.7598448395729065, 0.7623856067657471, 0.764644205570221, 0.7621427178382874, 0.7596673965454102, 0.7623437643051147, 0.7567015290260315, 0.7664865851402283, 0.7584642767906189, 0.7597187161445618, 0.7620127201080322, 0.764880359172821, 0.7560455799102783, 0.7583299875259399, 0.7612999081611633, 0.7631040811538696, 0.7607248425483704, 0.7584080100059509, 0.7626174688339233, 0.7613687515258789, 0.7569403052330017, 0.7596185803413391, 0.7615699172019958, 0.7631255984306335, 0.7598224878311157, 0.7644475698471069, 0.7580497860908508, 0.7611860632896423, 0.7549327611923218, 0.7608293890953064, 0.761012613773346, 0.7625057101249695, 0.7536153197288513, 0.765392541885376, 0.7570245265960693, 0.758139967918396, 0.7610039114952087, 0.7593116164207458, 0.7560443878173828, 0.7599019408226013, 0.7608395218849182, 0.760013997554779, 0.7584772706031799], 'recall_m': [0.7338182330131531, 0.07322577387094498, 0.0033068701159209013, 0.0010260104900225997, 0.00021853148064110428, 0.00021043770539108664, 0.00022727272880729288, 0.00018037519475910813, 0.0009350718464702368, 0.001996660605072975, 0.002066458575427532, 0.004598197061568499, 0.006345785688608885, 0.008972925134003162, 0.01281186006963253, 0.016925839707255363, 0.019268250092864037, 0.03037342242896557, 0.03710980340838432, 0.054734230041503906, 0.07407491654157639, 0.09896454960107803, 0.1301422268152237, 0.15157592296600342, 0.17013676464557648, 0.18463391065597534, 0.19806309044361115, 0.2157760113477707, 0.2288760095834732, 0.23556651175022125, 0.25322386622428894, 0.26744669675827026, 0.270919531583786, 0.292174369096756, 0.29315927624702454, 0.3074100911617279, 0.3190512955188751, 0.32519182562828064, 0.329237699508667, 0.33614709973335266, 0.346784383058548, 0.3530091345310211, 0.3528331518173218, 0.357700377702713, 0.358697772026062, 0.3639325797557831, 0.3711560368537903, 0.3730114996433258, 0.3733285665512085, 0.3755020797252655, 0.3779304325580597, 0.3833925127983093, 0.38266944885253906, 0.38573405146598816, 0.3886733055114746, 0.3896087408065796, 0.397396445274353, 0.396813303232193, 0.3921680152416229, 0.40006396174430847, 0.3977515399456024, 0.40418165922164917, 0.40932419896125793, 0.4138537347316742, 0.40399980545043945, 0.40824586153030396, 0.41246941685676575, 0.41230151057243347, 0.42216217517852783, 0.4126071035861969, 0.41921040415763855, 0.41408631205558777, 0.4209116995334625, 0.4239670932292938, 0.42460963129997253, 0.42427217960357666, 0.4248400926589966, 0.4276159405708313, 0.4290766716003418, 0.4290211498737335, 0.42826393246650696, 0.4280654191970825, 0.4326336681842804, 0.430566668510437, 0.43004101514816284, 0.43615972995758057, 0.4323711693286896, 0.435101717710495, 0.4374401867389679, 0.4339132010936737, 0.4380139410495758, 0.43966925144195557, 0.4398994743824005, 0.44135627150535583, 0.436665415763855, 0.43917515873908997, 0.4398921728134155, 0.4413122236728668, 0.4401841163635254, 0.44275081157684326, 0.4431442320346832, 0.44301870465278625, 0.4413171708583832, 0.443454772233963, 0.4425676465034485, 0.44426238536834717, 0.4444074332714081, 0.44599124789237976, 0.44491299986839294, 0.4465756416320801, 0.44601714611053467, 0.44621407985687256, 0.4464069604873657, 0.4460938274860382, 0.44529053568840027, 0.44744136929512024, 0.4471074044704437, 0.4467143714427948, 0.44710931181907654, 0.44847574830055237, 0.4481574296951294, 0.44795048236846924, 0.45066919922828674, 0.44848302006721497, 0.4536811113357544, 0.45040562748908997, 0.44695940613746643, 0.4474495053291321, 0.44661423563957214, 0.45250204205513, 0.44947949051856995, 0.4525889456272125, 0.44783708453178406, 0.4498627781867981, 0.4507364332675934, 0.44877147674560547, 0.4527888298034668, 0.45327654480934143, 0.4520297348499298, 0.44930756092071533, 0.45479917526245117, 0.45462092757225037, 0.4507479667663574, 0.4540243446826935, 0.4509005844593048, 0.4536924362182617, 0.4567581117153168, 0.4535442590713501, 0.45478394627571106, 0.4538017213344574, 0.4568479657173157, 0.4563107490539551, 0.45300158858299255, 0.4568130671977997, 0.4571859538555145, 0.45381832122802734, 0.4570575952529907, 0.45178377628326416, 0.45709219574928284, 0.4547717869281769, 0.45756757259368896, 0.4543212652206421, 0.455775648355484, 0.45644551515579224, 0.4585639238357544, 0.457109659910202, 0.4575055241584778, 0.4545336067676544, 0.45726630091667175, 0.4562559425830841, 0.4550655484199524, 0.4544675052165985, 0.4586944580078125, 0.45824986696243286, 0.45866087079048157, 0.4546632468700409, 0.45778799057006836, 0.457112580537796, 0.4565240740776062, 0.45551061630249023, 0.4576997458934784, 0.45927906036376953, 0.4577677845954895, 0.4573378264904022, 0.4571020007133484, 0.4601389765739441, 0.4577462077140808, 0.45737001299858093, 0.45813292264938354, 0.45947572588920593, 0.458140105009079, 0.4611251652240753, 0.4586469829082489, 0.4583166241645813, 0.4615395665168762, 0.4587557017803192, 0.4613383412361145, 0.45615431666374207, 0.4562261402606964, 0.4614437222480774, 0.45782145857810974, 0.45760834217071533, 0.4568627178668976, 0.45741596817970276, 0.4596126675605774, 0.4580965042114258, 0.4618041217327118, 0.46043235063552856, 0.457947701215744, 0.4597637355327606, 0.46195363998413086, 0.4601958990097046, 0.4594075381755829, 0.45940181612968445, 0.4618932902812958, 0.460450679063797, 0.46133261919021606, 0.4635840356349945, 0.4601169526576996, 0.4621283710002899, 0.4610726237297058, 0.46235987544059753, 0.46315401792526245, 0.45856237411499023, 0.4616203308105469, 0.465999573469162, 0.46320870518684387, 0.4614729881286621, 0.4615223705768585, 0.4616645574569702, 0.465478777885437, 0.46446493268013, 0.46312615275382996, 0.4644618034362793, 0.46526411175727844, 0.4629133939743042, 0.4671581983566284, 0.46549463272094727, 0.46340763568878174, 0.46714428067207336, 0.4645068347454071, 0.4723561108112335, 0.46507778763771057, 0.46943381428718567, 0.4677737057209015, 0.4654965400695801, 0.4707235097885132, 0.46568772196769714, 0.47012367844581604, 0.4696122109889984, 0.46819019317626953, 0.46548375487327576, 0.4692492187023163, 0.4690419137477875, 0.46838560700416565, 0.4729733169078827, 0.46996966004371643, 0.47239863872528076, 0.4687509536743164, 0.47086307406425476, 0.4705832600593567, 0.4730299115180969, 0.47208166122436523, 0.4696917235851288, 0.47361433506011963, 0.47416257858276367, 0.4729815721511841, 0.4724075198173523, 0.4717966914176941, 0.4737997353076935, 0.47074854373931885, 0.4707779586315155, 0.4752448797225952, 0.47291862964630127, 0.47627732157707214, 0.47497043013572693, 0.4740193486213684, 0.4736489951610565, 0.47318536043167114, 0.4735410511493683, 0.476163774728775, 0.47650405764579773, 0.47378185391426086, 0.4766046404838562, 0.4764995276927948, 0.477273553609848, 0.4729578197002411, 0.47722873091697693, 0.4753991365432739, 0.47619086503982544, 0.47543105483055115, 0.47855550050735474, 0.4753235876560211, 0.4769146144390106, 0.47593453526496887, 0.4777923822402954, 0.4753066897392273, 0.47494184970855713, 0.47890713810920715, 0.4750122129917145], 'val_loss': [0.6841932535171509, 0.673596203327179, 0.6671768426895142, 0.6627394556999207, 0.6593335866928101, 0.6565493941307068, 0.6541948318481445, 0.6522098779678345, 0.6504040360450745, 0.6487693786621094, 0.6471572518348694, 0.6455445885658264, 0.6439950466156006, 0.6423861384391785, 0.6407086849212646, 0.6389222145080566, 0.6372516751289368, 0.6355034708976746, 0.6338266134262085, 0.6321013569831848, 0.6303651928901672, 0.6286351680755615, 0.6268488168716431, 0.6250787973403931, 0.6233143210411072, 0.6215644478797913, 0.6198144555091858, 0.6180794835090637, 0.6162840127944946, 0.6146018505096436, 0.612819492816925, 0.6110725998878479, 0.6095098257064819, 0.6078900098800659, 0.6064063310623169, 0.6048755049705505, 0.6033520102500916, 0.6019344329833984, 0.6007137894630432, 0.5994818210601807, 0.5981764793395996, 0.5968651175498962, 0.595785915851593, 0.59475177526474, 0.5938296318054199, 0.5929109454154968, 0.5919198989868164, 0.5910218954086304, 0.5902172923088074, 0.5894920825958252, 0.5888177752494812, 0.5880747437477112, 0.5875605940818787, 0.5869430899620056, 0.5862664580345154, 0.5857638716697693, 0.585118293762207, 0.5845907330513, 0.5842435956001282, 0.5837352871894836, 0.5836538076400757, 0.5831220149993896, 0.582707941532135, 0.5821941494941711, 0.5819509625434875, 0.5816289782524109, 0.5813796520233154, 0.5811858177185059, 0.5807969570159912, 0.5805317759513855, 0.5802791714668274, 0.5801218152046204, 0.5799087285995483, 0.579649806022644, 0.5795075297355652, 0.5793554782867432, 0.5791147947311401, 0.5789356231689453, 0.5786293148994446, 0.5785117745399475, 0.5784618854522705, 0.5782333612442017, 0.5779536366462708, 0.5778403282165527, 0.5778552293777466, 0.577483057975769, 0.5774235129356384, 0.5774824023246765, 0.5769699811935425, 0.5769651532173157, 0.5766475796699524, 0.5765800476074219, 0.5766748189926147, 0.5761604309082031, 0.5762455463409424, 0.5759189128875732, 0.5758602023124695, 0.5755875110626221, 0.5754668712615967, 0.5753784775733948, 0.5753509998321533, 0.5751093029975891, 0.5750462412834167, 0.57488614320755, 0.5747196078300476, 0.5745735168457031, 0.5747079849243164, 0.5743659138679504, 0.5741545557975769, 0.5740231871604919, 0.5739071369171143, 0.5737761855125427, 0.5737087726593018, 0.5735477805137634, 0.573635995388031, 0.5733295679092407, 0.5732390880584717, 0.573170006275177, 0.5730761885643005, 0.5729509592056274, 0.5727460980415344, 0.5728288292884827, 0.5726523399353027, 0.5728777050971985, 0.572292149066925, 0.5722860097885132, 0.5721744298934937, 0.5719686150550842, 0.5721616744995117, 0.5717918276786804, 0.5716536641120911, 0.5715839862823486, 0.5716373324394226, 0.5716173648834229, 0.5713058710098267, 0.5714663863182068, 0.5711677074432373, 0.5711475014686584, 0.5708459615707397, 0.5713013410568237, 0.5710325241088867, 0.5706930756568909, 0.5705568194389343, 0.570410966873169, 0.5707221627235413, 0.5708727836608887, 0.5701613426208496, 0.5701837539672852, 0.5701199769973755, 0.570310652256012, 0.5699365735054016, 0.5698496699333191, 0.5697927474975586, 0.5700641870498657, 0.5697104930877686, 0.5700754523277283, 0.5696446895599365, 0.5697590708732605, 0.5693978071212769, 0.5694018006324768, 0.569401741027832, 0.5695181488990784, 0.5692862272262573, 0.5690668821334839, 0.5690708756446838, 0.5690697431564331, 0.5688790678977966, 0.5691850185394287, 0.5688266158103943, 0.5689841508865356, 0.5688628554344177, 0.5695565938949585, 0.5691832900047302, 0.5687483549118042, 0.5687130093574524, 0.5688761472702026, 0.5684775114059448, 0.5684863924980164, 0.5686599016189575, 0.5684415102005005, 0.5686671733856201, 0.568311333656311, 0.5682730078697205, 0.568415641784668, 0.5688096284866333, 0.5683055520057678, 0.5681198835372925, 0.5682239532470703, 0.5684294104576111, 0.5681003928184509, 0.5681890249252319, 0.5681790113449097, 0.5683793425559998, 0.5681237578392029, 0.567965567111969, 0.5680520534515381, 0.5678145885467529, 0.5678028464317322, 0.568300187587738, 0.5677530169487, 0.5679252743721008, 0.5678645968437195, 0.5677578449249268, 0.5682865381240845, 0.5677763819694519, 0.5677672624588013, 0.56777423620224, 0.5676023960113525, 0.567714512348175, 0.5680050849914551, 0.5675212144851685, 0.567585825920105, 0.5674931406974792, 0.5675183534622192, 0.5677503347396851, 0.5676164031028748, 0.5676678419113159, 0.5674252510070801, 0.5674976706504822, 0.5678606033325195, 0.5675674676895142, 0.5675349831581116, 0.5672863721847534, 0.5677265524864197, 0.5682153701782227, 0.567381739616394, 0.5674129128456116, 0.5672832727432251, 0.5676326155662537, 0.567579448223114, 0.5674051642417908, 0.567194402217865, 0.5676316022872925, 0.5672521591186523, 0.5671733021736145, 0.5675287842750549, 0.5670300126075745, 0.5670256018638611, 0.5675287246704102, 0.5670856237411499, 0.5670626163482666, 0.5669264197349548, 0.567723274230957, 0.5673552751541138, 0.5671882033348083, 0.5674360990524292, 0.5668997168540955, 0.5674100518226624, 0.56691575050354, 0.5668821334838867, 0.5668006539344788, 0.5671308040618896, 0.5669525861740112, 0.5672121047973633, 0.5674187541007996, 0.5667408108711243, 0.5670304298400879, 0.5669440627098083, 0.5670287609100342, 0.5673491358757019, 0.5672717690467834, 0.5667150020599365, 0.5667895674705505, 0.5667896270751953, 0.5671144723892212, 0.5666919946670532, 0.5664405822753906, 0.5664815306663513, 0.5666146278381348, 0.5663862228393555, 0.56635582447052, 0.5665767192840576, 0.5667550563812256, 0.5666483044624329, 0.5663626790046692, 0.5665265321731567, 0.566526472568512, 0.5663686394691467, 0.5662379860877991, 0.5666331052780151, 0.5667023062705994, 0.5662921667098999, 0.5670287609100342, 0.5663963556289673, 0.5667975544929504, 0.5660845041275024, 0.5664159059524536, 0.5662837624549866, 0.566555917263031, 0.5659831762313843, 0.5666676163673401, 0.5661073923110962, 0.5662407875061035, 0.566093385219574, 0.5660298466682434, 0.5658233165740967, 0.5658627152442932, 0.56691575050354, 0.565831184387207, 0.565909206867218], 'val_accuracy': [0.6721428632736206, 0.6303571462631226, 0.6282142996788025, 0.6274999976158142, 0.6274999976158142, 0.6274999976158142, 0.6274999976158142, 0.6274999976158142, 0.6271428465843201, 0.6271428465843201, 0.6274999976158142, 0.6282142996788025, 0.628928542137146, 0.628928542137146, 0.6303571462631226, 0.6310714483261108, 0.6310714483261108, 0.6339285969734192, 0.6392857432365417, 0.6449999809265137, 0.6485714316368103, 0.6557142734527588, 0.6610714197158813, 0.6642857193946838, 0.6717857122421265, 0.6746428608894348, 0.681071400642395, 0.6839285492897034, 0.6875, 0.6907142996788025, 0.6928571462631226, 0.6949999928474426, 0.6985714435577393, 0.7010714411735535, 0.7024999856948853, 0.7035714387893677, 0.704285740852356, 0.704285740852356, 0.7060714364051819, 0.7089285850524902, 0.7092857360839844, 0.7092857360839844, 0.7099999785423279, 0.7117857336997986, 0.7124999761581421, 0.7149999737739563, 0.7153571248054504, 0.7153571248054504, 0.7149999737739563, 0.716785728931427, 0.7171428799629211, 0.7171428799629211, 0.7174999713897705, 0.7182142734527588, 0.7182142734527588, 0.7189285755157471, 0.7185714244842529, 0.7185714244842529, 0.7217857241630554, 0.7210714221000671, 0.7250000238418579, 0.7246428728103638, 0.7250000238418579, 0.7228571176528931, 0.7250000238418579, 0.7250000238418579, 0.7260714173316956, 0.727142870426178, 0.7250000238418579, 0.7267857193946838, 0.7267857193946838, 0.7278571724891663, 0.729285717010498, 0.7282142639160156, 0.7303571701049805, 0.7300000190734863, 0.7303571701049805, 0.7303571701049805, 0.7307142615318298, 0.7307142615318298, 0.7307142615318298, 0.7307142615318298, 0.7317857146263123, 0.7307142615318298, 0.7321428656578064, 0.7314285635948181, 0.7321428656578064, 0.7335714101791382, 0.7328571677207947, 0.7335714101791382, 0.7339285612106323, 0.7350000143051147, 0.7350000143051147, 0.7339285612106323, 0.7360714077949524, 0.7367857098579407, 0.7364285588264465, 0.7335714101791382, 0.7367857098579407, 0.737500011920929, 0.737500011920929, 0.7367857098579407, 0.7371428608894348, 0.737500011920929, 0.7367857098579407, 0.7364285588264465, 0.7367857098579407, 0.7360714077949524, 0.7360714077949524, 0.7353571653366089, 0.7350000143051147, 0.7342857122421265, 0.733214259147644, 0.7353571653366089, 0.7389285564422607, 0.7367857098579407, 0.7385714054107666, 0.7389285564422607, 0.7378571629524231, 0.7385714054107666, 0.7371428608894348, 0.7382143139839172, 0.7382143139839172, 0.7382143139839172, 0.7371428608894348, 0.7378571629524231, 0.7378571629524231, 0.7382143139839172, 0.7371428608894348, 0.737500011920929, 0.7378571629524231, 0.7367857098579407, 0.7367857098579407, 0.7371428608894348, 0.7371428608894348, 0.7371428608894348, 0.7367857098579407, 0.7364285588264465, 0.7364285588264465, 0.7382143139839172, 0.7371428608894348, 0.7371428608894348, 0.7378571629524231, 0.7360714077949524, 0.7360714077949524, 0.7382143139839172, 0.7367857098579407, 0.7371428608894348, 0.7367857098579407, 0.7360714077949524, 0.7371428608894348, 0.7364285588264465, 0.7364285588264465, 0.7364285588264465, 0.7367857098579407, 0.737500011920929, 0.7364285588264465, 0.7367857098579407, 0.7364285588264465, 0.7371428608894348, 0.737500011920929, 0.7371428608894348, 0.7378571629524231, 0.7364285588264465, 0.7378571629524231, 0.7378571629524231, 0.7350000143051147, 0.7378571629524231, 0.7378571629524231, 0.7378571629524231, 0.7378571629524231, 0.7382143139839172, 0.7378571629524231, 0.737500011920929, 0.7385714054107666, 0.7385714054107666, 0.7371428608894348, 0.737500011920929, 0.7389285564422607, 0.7378571629524231, 0.7392857074737549, 0.737500011920929, 0.7378571629524231, 0.739642858505249, 0.7385714054107666, 0.7392857074737549, 0.7382143139839172, 0.7392857074737549, 0.7389285564422607, 0.7382143139839172, 0.7392857074737549, 0.739642858505249, 0.7385714054107666, 0.7400000095367432, 0.7382143139839172, 0.7392857074737549, 0.737500011920929, 0.7378571629524231, 0.7392857074737549, 0.737500011920929, 0.739642858505249, 0.739642858505249, 0.7378571629524231, 0.739642858505249, 0.7400000095367432, 0.7407143115997314, 0.7407143115997314, 0.737500011920929, 0.7407143115997314, 0.739642858505249, 0.7371428608894348, 0.7385714054107666, 0.7385714054107666, 0.7382143139839172, 0.7392857074737549, 0.7392857074737549, 0.7392857074737549, 0.7385714054107666, 0.7389285564422607, 0.7410714030265808, 0.739642858505249, 0.739642858505249, 0.737500011920929, 0.7403571605682373, 0.7410714030265808, 0.739642858505249, 0.739642858505249, 0.7392857074737549, 0.7403571605682373, 0.7403571605682373, 0.7400000095367432, 0.739642858505249, 0.7410714030265808, 0.7400000095367432, 0.7392857074737549, 0.7410714030265808, 0.7389285564422607, 0.7389285564422607, 0.741428554058075, 0.7407143115997314, 0.7400000095367432, 0.7382143139839172, 0.7410714030265808, 0.7428571581840515, 0.741428554058075, 0.7410714030265808, 0.7392857074737549, 0.7407143115997314, 0.7410714030265808, 0.7407143115997314, 0.7403571605682373, 0.7410714030265808, 0.741428554058075, 0.7407143115997314, 0.739642858505249, 0.7400000095367432, 0.7410714030265808, 0.7410714030265808, 0.7407143115997314, 0.7400000095367432, 0.7400000095367432, 0.7403571605682373, 0.741428554058075, 0.7407143115997314, 0.7392857074737549, 0.741428554058075, 0.739642858505249, 0.7385714054107666, 0.7400000095367432, 0.7400000095367432, 0.7400000095367432, 0.7400000095367432, 0.7392857074737549, 0.7392857074737549, 0.7389285564422607, 0.7400000095367432, 0.739642858505249, 0.7385714054107666, 0.7385714054107666, 0.7389285564422607, 0.7382143139839172, 0.7385714054107666, 0.7364285588264465, 0.7385714054107666, 0.7367857098579407, 0.7389285564422607, 0.7378571629524231, 0.7382143139839172, 0.737500011920929, 0.7392857074737549, 0.7364285588264465, 0.737500011920929, 0.737500011920929, 0.7371428608894348, 0.7378571629524231, 0.7392857074737549, 0.7385714054107666, 0.7364285588264465, 0.7385714054107666, 0.7371428608894348], 'val_f1_m': [0.36977264285087585, 0.019463850185275078, 0.00422494113445282, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.002331001916900277, 0.00422494113445282, 0.010078934021294117, 0.013715296983718872, 0.01564953289926052, 0.023448752239346504, 0.028905827552080154, 0.034996144473552704, 0.050421908497810364, 0.08098158240318298, 0.11356232315301895, 0.14529988169670105, 0.18347272276878357, 0.21462726593017578, 0.23900727927684784, 0.27311572432518005, 0.29268696904182434, 0.3211838901042938, 0.33923813700675964, 0.35477960109710693, 0.36968281865119934, 0.3776620626449585, 0.3879319131374359, 0.40518656373023987, 0.42006149888038635, 0.42788952589035034, 0.4338134825229645, 0.4390049874782562, 0.4419008493423462, 0.45039549469947815, 0.4601512849330902, 0.4632253348827362, 0.4615573585033417, 0.46663153171539307, 0.4725053608417511, 0.47707557678222656, 0.48715344071388245, 0.4866710603237152, 0.4866710603237152, 0.48711395263671875, 0.4923563003540039, 0.49547243118286133, 0.4947296977043152, 0.5005139112472534, 0.5029568076133728, 0.5022718906402588, 0.5053300261497498, 0.5025405287742615, 0.5011131763458252, 0.5132627487182617, 0.5112662315368652, 0.5245253443717957, 0.5221735239028931, 0.5232110619544983, 0.5140250325202942, 0.5224668979644775, 0.5224668979644775, 0.5260959267616272, 0.5293777585029602, 0.5189338326454163, 0.5273504853248596, 0.526732325553894, 0.5313082933425903, 0.5347886681556702, 0.5323158502578735, 0.5378018021583557, 0.5379090905189514, 0.5381951928138733, 0.5391840934753418, 0.5376666784286499, 0.5394701361656189, 0.5420662760734558, 0.5420662760734558, 0.5426503419876099, 0.5420662760734558, 0.5454319715499878, 0.5426916480064392, 0.5454319715499878, 0.5495132803916931, 0.5437958836555481, 0.548973023891449, 0.5458529591560364, 0.5512306094169617, 0.5537065863609314, 0.5453692674636841, 0.555252730846405, 0.5528604388237, 0.5548163652420044, 0.5454508066177368, 0.5536903142929077, 0.5552992224693298, 0.5564603209495544, 0.5531299114227295, 0.5546026825904846, 0.5548943281173706, 0.5531463027000427, 0.5524358749389648, 0.5579620003700256, 0.5528703331947327, 0.5505074858665466, 0.5485037565231323, 0.5494040250778198, 0.5464552640914917, 0.5423811674118042, 0.5500709414482117, 0.5611717700958252, 0.5533109307289124, 0.5581132769584656, 0.5592304468154907, 0.5574934482574463, 0.5592647194862366, 0.5535739660263062, 0.5604099631309509, 0.5601974129676819, 0.5637340545654297, 0.5541609525680542, 0.5586439371109009, 0.5581766366958618, 0.5562446117401123, 0.5603986382484436, 0.5565120577812195, 0.5559608936309814, 0.5529184341430664, 0.5572647452354431, 0.5589308738708496, 0.5570603609085083, 0.5589282512664795, 0.5569867491722107, 0.5567100048065186, 0.55372154712677, 0.5626137256622314, 0.5589840412139893, 0.5575079321861267, 0.5581877827644348, 0.5527586340904236, 0.5567799210548401, 0.56329345703125, 0.5550002455711365, 0.5575079321861267, 0.557114839553833, 0.5570951104164124, 0.5574406385421753, 0.5557301044464111, 0.5557301044464111, 0.5579655766487122, 0.5565654635429382, 0.5612472891807556, 0.5561926364898682, 0.5589394569396973, 0.5557301044464111, 0.5577179193496704, 0.5589444637298584, 0.560308039188385, 0.5593172907829285, 0.5546867251396179, 0.558818519115448, 0.5592034459114075, 0.551011323928833, 0.5609791278839111, 0.558018684387207, 0.5603320002555847, 0.5591210126876831, 0.5636438727378845, 0.5628145337104797, 0.5592100620269775, 0.5617846250534058, 0.562972366809845, 0.5563476085662842, 0.5576276183128357, 0.5633792281150818, 0.5587157607078552, 0.5644035935401917, 0.557990312576294, 0.5590753555297852, 0.5639267563819885, 0.5646448731422424, 0.5625673532485962, 0.5599525570869446, 0.5625672936439514, 0.5648572444915771, 0.5599525570869446, 0.5631774663925171, 0.5642017722129822, 0.5645905137062073, 0.56499844789505, 0.5605626702308655, 0.5640063285827637, 0.556907057762146, 0.5583165884017944, 0.5664462447166443, 0.5574582815170288, 0.5653638243675232, 0.564852774143219, 0.560294508934021, 0.5672938823699951, 0.565531849861145, 0.5673747658729553, 0.5673747658729553, 0.559406578540802, 0.5673747658729553, 0.5677264332771301, 0.5570824146270752, 0.5625095963478088, 0.5614222288131714, 0.561674177646637, 0.566138744354248, 0.5656322836875916, 0.5656322836875916, 0.5620001554489136, 0.5635461807250977, 0.5709389448165894, 0.566338062286377, 0.5667502284049988, 0.5584807395935059, 0.5698901414871216, 0.5736961960792542, 0.5661271214485168, 0.5671572685241699, 0.5651134848594666, 0.5699419379234314, 0.5699419379234314, 0.5685859322547913, 0.5652214288711548, 0.5718528628349304, 0.5680739879608154, 0.5656311511993408, 0.5718528628349304, 0.5621680021286011, 0.562527596950531, 0.5740641355514526, 0.569271981716156, 0.5679354667663574, 0.5606384873390198, 0.5744257569313049, 0.5761733055114746, 0.5722193717956543, 0.5744257569313049, 0.5646295547485352, 0.5740588903427124, 0.5699945688247681, 0.5689809322357178, 0.5672450661659241, 0.5734389424324036, 0.5722193717956543, 0.5748202204704285, 0.5748753547668457, 0.5674378871917725, 0.5746432542800903, 0.5733738541603088, 0.5743482112884521, 0.5760340094566345, 0.5760340094566345, 0.5706122517585754, 0.5749450325965881, 0.5748453736305237, 0.5748952031135559, 0.5742878317832947, 0.5656046271324158, 0.5655418038368225, 0.5729295015335083, 0.565892219543457, 0.5653582215309143, 0.5735731720924377, 0.5750371813774109, 0.5744165778160095, 0.569303572177887, 0.5745904445648193, 0.5747833847999573, 0.5703131556510925, 0.5671985745429993, 0.5754673480987549, 0.5754010677337646, 0.5706708431243896, 0.5752066969871521, 0.5737061500549316, 0.574529230594635, 0.567493200302124, 0.5734436511993408, 0.5729259848594666, 0.5751999020576477, 0.5691826343536377, 0.574647843837738, 0.5706965923309326, 0.5730863213539124, 0.5715193152427673, 0.5714631676673889, 0.5643448829650879, 0.5698170065879822, 0.5786641836166382, 0.5702470541000366, 0.5700176358222961], 'val_precision_m': [0.6382402181625366, 0.3636363446712494, 0.09090907871723175, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.045454539358615875, 0.09090907871723175, 0.22727270424365997, 0.3181817829608917, 0.3636363446712494, 0.4772726893424988, 0.5303030014038086, 0.5545454621315002, 0.6833333373069763, 0.7553030252456665, 0.8185425400733948, 0.7790042757987976, 0.7862012982368469, 0.7704479694366455, 0.757297158241272, 0.7712690830230713, 0.7635570764541626, 0.7625896334648132, 0.7563998699188232, 0.7626991271972656, 0.759762167930603, 0.7620913982391357, 0.7622363567352295, 0.7616752982139587, 0.7566877603530884, 0.7542142271995544, 0.752297043800354, 0.7495982646942139, 0.7448969483375549, 0.7419734597206116, 0.7413769364356995, 0.7384802103042603, 0.7438205480575562, 0.7378151416778564, 0.7398281097412109, 0.7377275824546814, 0.7373082637786865, 0.7395450472831726, 0.7395450472831726, 0.7364917993545532, 0.7384964227676392, 0.7367336750030518, 0.7373047471046448, 0.7316569685935974, 0.7320442795753479, 0.7324796319007874, 0.7320968508720398, 0.733368456363678, 0.7343136072158813, 0.7357181310653687, 0.7345060110092163, 0.7358176708221436, 0.7362096309661865, 0.736799955368042, 0.7385919690132141, 0.7369515895843506, 0.7369515895843506, 0.7366737127304077, 0.7369430661201477, 0.7426378726959229, 0.7383927702903748, 0.7389803528785706, 0.7383001446723938, 0.7398771047592163, 0.7388391494750977, 0.7401559948921204, 0.7386440634727478, 0.7394737601280212, 0.7380422353744507, 0.7419165968894958, 0.7388719916343689, 0.7344989776611328, 0.7344989776611328, 0.7386232018470764, 0.7344989776611328, 0.736175537109375, 0.7362072467803955, 0.736175537109375, 0.7366205453872681, 0.7404676675796509, 0.7371076345443726, 0.7426280379295349, 0.7395815253257751, 0.7360867857933044, 0.7434945702552795, 0.7384487390518188, 0.7443036437034607, 0.7395830154418945, 0.7413617968559265, 0.7435205578804016, 0.7442010641098022, 0.7427528500556946, 0.7445002198219299, 0.7444027066230774, 0.745221734046936, 0.7444952130317688, 0.7442997097969055, 0.7387481927871704, 0.7426368594169617, 0.7454524636268616, 0.7442895770072937, 0.7418623566627502, 0.7410942316055298, 0.7423282861709595, 0.7423738837242126, 0.7449081540107727, 0.7440629601478577, 0.7465938925743103, 0.7471920251846313, 0.7435194253921509, 0.7442349791526794, 0.7451446056365967, 0.7418470978736877, 0.7415573596954346, 0.7374563217163086, 0.7440423965454102, 0.7416166663169861, 0.7424876689910889, 0.7454279065132141, 0.7358962893486023, 0.7424655556678772, 0.7441217303276062, 0.7442957162857056, 0.7387759685516357, 0.737494707107544, 0.7400917410850525, 0.7373486161231995, 0.7385066747665405, 0.7371892333030701, 0.7408048510551453, 0.7383623123168945, 0.7371333837509155, 0.7390143871307373, 0.7416057586669922, 0.7404507994651794, 0.7347185611724854, 0.7378048300743103, 0.740004301071167, 0.7390143871307373, 0.7375713586807251, 0.7353822588920593, 0.7392460107803345, 0.7386136651039124, 0.7386136651039124, 0.7358632683753967, 0.738854169845581, 0.7372567653656006, 0.7376003265380859, 0.7366304397583008, 0.7386136651039124, 0.7400516271591187, 0.7393314838409424, 0.7366997003555298, 0.7405853867530823, 0.7411203980445862, 0.7422056198120117, 0.7405893206596375, 0.7393548488616943, 0.7392401099205017, 0.7425816059112549, 0.7394153475761414, 0.7414236664772034, 0.7361730933189392, 0.7358932495117188, 0.7392347455024719, 0.7408979535102844, 0.7392829656600952, 0.741908848285675, 0.7417621612548828, 0.7402452230453491, 0.7411771416664124, 0.740963876247406, 0.7409746050834656, 0.7418403625488281, 0.743147075176239, 0.7374300360679626, 0.7434914112091064, 0.7422612905502319, 0.7434914112091064, 0.7383337616920471, 0.7422612905502319, 0.7429611086845398, 0.7436797618865967, 0.7370986938476562, 0.7440617084503174, 0.7417309284210205, 0.7426239252090454, 0.7432260513305664, 0.7425997257232666, 0.7383205890655518, 0.7423408031463623, 0.7425790429115295, 0.7435210943222046, 0.7408029437065125, 0.7388378381729126, 0.7436360716819763, 0.7446569800376892, 0.7446569800376892, 0.7406306862831116, 0.7446569800376892, 0.7379245758056641, 0.7415241599082947, 0.7407713532447815, 0.7428082823753357, 0.7405309081077576, 0.7394514679908752, 0.7411026358604431, 0.7411026358604431, 0.7418370842933655, 0.7416192889213562, 0.73928302526474, 0.7413318157196045, 0.7403581738471985, 0.7412691116333008, 0.7378891110420227, 0.7371584177017212, 0.7410867810249329, 0.7385732531547546, 0.7403457164764404, 0.7379674911499023, 0.7379674911499023, 0.7383459210395813, 0.7407585382461548, 0.7392321228981018, 0.7392967343330383, 0.7395227551460266, 0.7392321228981018, 0.741607129573822, 0.7405517101287842, 0.7390028238296509, 0.7416196465492249, 0.7389475107192993, 0.7409707903862, 0.73700350522995, 0.742877185344696, 0.7408568859100342, 0.73700350522995, 0.7400538921356201, 0.7358793616294861, 0.741098940372467, 0.7403578162193298, 0.7412547469139099, 0.7386851906776428, 0.7408568859100342, 0.7356391549110413, 0.7313360571861267, 0.7387619614601135, 0.7373184561729431, 0.7383584976196289, 0.7365608811378479, 0.7307077646255493, 0.7307077646255493, 0.7376986145973206, 0.7381159067153931, 0.7356424927711487, 0.729056179523468, 0.738746702671051, 0.7400409579277039, 0.7345579266548157, 0.7344869375228882, 0.7413696050643921, 0.7431151866912842, 0.7343605160713196, 0.729434609413147, 0.7302242517471313, 0.7334509491920471, 0.7334163784980774, 0.7315117120742798, 0.7314016222953796, 0.7346275448799133, 0.726692259311676, 0.7241154909133911, 0.7305786609649658, 0.7170917391777039, 0.7276500463485718, 0.719318151473999, 0.7358030676841736, 0.7248511910438538, 0.7272297739982605, 0.7215496897697449, 0.7358257174491882, 0.7173324823379517, 0.7266374230384827, 0.7236495018005371, 0.7247495651245117, 0.7272242903709412, 0.7399802803993225, 0.7323809266090393, 0.7128166556358337, 0.7316903471946716, 0.7255101203918457], 'val_recall_m': [0.2626662254333496, 0.010046184062957764, 0.0021632902789860964, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0011961722047999501, 0.0021632902789860964, 0.005154608283191919, 0.007009895518422127, 0.007998038083314896, 0.012054353021085262, 0.014945792965590954, 0.01827777549624443, 0.026508718729019165, 0.04367607459425926, 0.06230587139725685, 0.08186954259872437, 0.10524248331785202, 0.12636657059192657, 0.1441645622253418, 0.16843567788600922, 0.18447770178318024, 0.20670227706432343, 0.22149966657161713, 0.23425112664699554, 0.24758754670619965, 0.25428274273872375, 0.2635246813297272, 0.2800487279891968, 0.29509463906288147, 0.30297937989234924, 0.30857160687446594, 0.31451892852783203, 0.3183382749557495, 0.3274975121021271, 0.3377535343170166, 0.3415440320968628, 0.33913299441337585, 0.34530213475227356, 0.35088226199150085, 0.3566162884235382, 0.3677270710468292, 0.36679938435554504, 0.36679938435554504, 0.3677665591239929, 0.37333372235298157, 0.377242773771286, 0.3763151168823242, 0.3851819932460785, 0.3882426917552948, 0.3871062994003296, 0.3912737965583801, 0.3873150646686554, 0.384950190782547, 0.40022432804107666, 0.39832955598831177, 0.41423654556274414, 0.41105279326438904, 0.41204091906547546, 0.40030673146247864, 0.41111329197883606, 0.41111329197883606, 0.4157331585884094, 0.4196261465549469, 0.4051588177680969, 0.4167002737522125, 0.4157331585884094, 0.42161083221435547, 0.4251653850078583, 0.4225989580154419, 0.42884886264801025, 0.42960646748542786, 0.42960646748542786, 0.43154069781303406, 0.4280071258544922, 0.43154069781303406, 0.43628767132759094, 0.43628767132759094, 0.43547600507736206, 0.43628767132759094, 0.44039222598075867, 0.43628767132759094, 0.44039222598075867, 0.44505149126052856, 0.4362187087535858, 0.44417741894721985, 0.43804866075515747, 0.4459984600543976, 0.4505498707294464, 0.4371955692768097, 0.4515169858932495, 0.4466080367565155, 0.45045986771583557, 0.43803760409355164, 0.44767168164253235, 0.4495099186897278, 0.451351135969162, 0.4466317296028137, 0.44865596294403076, 0.44865596294403076, 0.4465887248516083, 0.44583117961883545, 0.45434990525245667, 0.446888267993927, 0.44296833872795105, 0.4408782124519348, 0.44296833872795105, 0.43982112407684326, 0.4335201382637024, 0.4438052177429199, 0.45687076449394226, 0.4476093351840973, 0.45265132188796997, 0.45370838046073914, 0.45265132188796997, 0.45469656586647034, 0.4473107159137726, 0.45681074261665344, 0.4567417800426483, 0.4626389443874359, 0.4484282433986664, 0.45456168055534363, 0.4534253180027008, 0.4503834843635559, 0.4587189853191376, 0.4514405429363251, 0.4503834843635559, 0.4462737739086151, 0.4534253180027008, 0.4564064145088196, 0.452497661113739, 0.4562802016735077, 0.4533093273639679, 0.4533093273639679, 0.4484492540359497, 0.4603876769542694, 0.45621421933174133, 0.4533093273639679, 0.4533093273639679, 0.4470854699611664, 0.454276442527771, 0.46133461594581604, 0.4502280354499817, 0.4533093273639679, 0.4533093273639679, 0.4542562663555145, 0.4533093273639679, 0.45133209228515625, 0.45133209228515625, 0.45514756441116333, 0.45229920744895935, 0.4589163064956665, 0.45229920744895935, 0.45592615008354187, 0.45133209228515625, 0.4531703293323517, 0.45497921109199524, 0.4576307237148285, 0.45497921109199524, 0.4489907920360565, 0.45401209592819214, 0.45476964116096497, 0.44500675797462463, 0.4576307237148285, 0.4527655839920044, 0.4566837251186371, 0.45476964116096497, 0.46239611506462097, 0.46142899990081787, 0.45573675632476807, 0.4585019052028656, 0.4603959023952484, 0.4505901336669922, 0.45272770524024963, 0.4603959023952484, 0.4545837640762329, 0.46134284138679504, 0.4536368250846863, 0.4545837640762329, 0.4602857530117035, 0.4633430540561676, 0.45839184522628784, 0.4555307626724243, 0.45839184522628784, 0.46321362257003784, 0.4555307626724243, 0.45931950211524963, 0.46026647090911865, 0.46321362257003784, 0.46129950881004333, 0.4564584493637085, 0.4603525400161743, 0.4513837695121765, 0.453297883272171, 0.46501538157463074, 0.4523307681083679, 0.46226662397384644, 0.46129950881004333, 0.4562689960002899, 0.46615174412727356, 0.462154358625412, 0.4640684127807617, 0.4640684127807617, 0.4552624523639679, 0.4640684127807617, 0.4669093191623688, 0.45202988386154175, 0.45910385251045227, 0.45716962218284607, 0.45813673734664917, 0.4640684127807617, 0.4631012976169586, 0.4631012976169586, 0.45813673734664917, 0.4600508511066437, 0.4704569876194, 0.46385887265205383, 0.46461641788482666, 0.45385101437568665, 0.4695100486278534, 0.4745088815689087, 0.4633547067642212, 0.4652130901813507, 0.46240776777267456, 0.46955302357673645, 0.46955302357673645, 0.4674065411090851, 0.4623520076274872, 0.4714276194572449, 0.466439425945282, 0.4632990062236786, 0.4714276194572449, 0.4583417773246765, 0.45909932255744934, 0.4745519459247589, 0.4674495458602905, 0.4665025770664215, 0.4563617408275604, 0.4756090044975281, 0.4756090044975281, 0.47139132022857666, 0.4756090044975281, 0.46214982867240906, 0.4756090044975281, 0.46828824281692505, 0.4673413038253784, 0.4652741253376007, 0.47350549697875977, 0.47139132022857666, 0.4764038026332855, 0.4784143269062042, 0.46628424525260925, 0.4754366874694824, 0.47350549697875977, 0.4754366874694824, 0.48038050532341003, 0.48038050532341003, 0.4702729880809784, 0.4754366874694824, 0.4763108193874359, 0.4792441129684448, 0.47420817613601685, 0.4632711410522461, 0.4652511775493622, 0.4741353690624237, 0.4632711410522461, 0.4622381031513214, 0.47508230805397034, 0.4792046546936035, 0.47797614336013794, 0.4700411856174469, 0.4769190549850464, 0.47797614336013794, 0.47202590107917786, 0.46700403094291687, 0.4811893403530121, 0.4821774661540985, 0.4729349911212921, 0.48504599928855896, 0.47797614336013794, 0.4830687642097473, 0.46700403094291687, 0.47890377044677734, 0.47694310545921326, 0.4830687642097473, 0.4689888060092926, 0.48407888412475586, 0.47401610016822815, 0.47890377044677734, 0.4758906662464142, 0.47494372725486755, 0.4605879783630371, 0.47110292315483093, 0.4919895827770233, 0.4720120429992676, 0.4738866090774536]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eJeaFTAzKWr-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 546
        },
        "outputId": "db38b7f2-df3c-4db7-b876-d908948e2011"
      },
      "source": [
        "#plot training history\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "x_plot = list(range(1,n_epochs+1))\n",
        "\n",
        "def plot_history(network_history):\n",
        "    plt.figure()\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    #plt.ylim(0.6,0.8)#just for better viz\n",
        "    plt.plot(x_plot, network_history.history['loss'])\n",
        "    plt.plot(x_plot, network_history.history['val_loss'])\n",
        "    plt.legend(['Training', 'Validation'])\n",
        "\n",
        "    plt.figure()\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.plot(x_plot, network_history.history['accuracy'])\n",
        "    plt.plot(x_plot, network_history.history['val_accuracy'])\n",
        "    plt.legend(['Training', 'Validation'], loc='lower right')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "plot_history(history)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEKCAYAAAAIO8L1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3dd3xc1Zn/8c+jmVHvxU2SbRnLNjbucqcYWMCU4EBoDklwSGgJYcMGElI2sCTZ325CNlk2JLuUQEIAQ0IwJoHQCQQbbLmBJSMXuUkuKlbv5fn9ca5tWci2bGs8Ks/79ZoXM7fMPFdj9NU5595zRVUxxhhjOgsLdQHGGGN6JwsIY4wxXbKAMMYY0yULCGOMMV2ygDDGGNMlCwhjjDFdCmpAiMgCESkQkS0ick8X638hIuu8xyYRqeyw7gYR2ew9bghmncYYYz5NgnUdhIj4gE3ABUARsApYpKr5R9j+G8BUVb1RRJKBXCAHUGA1MF1VK4JSrDHGmE8JZgtiJrBFVQtVtRlYAiw8yvaLgGe85xcBr6vqfi8UXgcWBLFWY4wxnfiD+N7pwK4Or4uAWV1tKCIjgCzgraPsm97FfjcDNwPExMRMHzdu3MlXbYwxA8jq1avLVDWtq3XBDIjjcR3wJ1VtO56dVPVh4GGAnJwczc3NDUZtxhjTb4nIjiOtC2YXUzGQ2eF1hresK9dxqHvpePc1xhgTBMEMiFVAtohkiUg4LgSWdd5IRMYBScCKDotfBS4UkSQRSQIu9JYZY4w5RYLWxaSqrSJyO+4Xuw/4rarmicj9QK6qHgiL64Al2uF0KlXdLyI/woUMwP2quj9YtRpjjPm0oJ3meqrZGIQx/UtLSwtFRUU0NjaGupR+ITIykoyMDAKBwGHLRWS1quZ0tU9vGaQ2xpjDFBUVERcXx8iRIxGRUJfTp6kq5eXlFBUVkZWV1e39bKoNY0yv1NjYSEpKioVDDxARUlJSjrs1ZgFhjOm1LBx6zon8LAd8QNQ0tvCL1zexblflsTc2xpgBZMAHRFu78t9vbmbtTpvmyRhzSHl5OVOmTGHKlCkMGTKE9PT0g6+bm5uPum9ubi533HHHMT9j7ty5PVVuUAz4QeqYCPcjqG1sDXElxpjeJCUlhXXr1gFw3333ERsby1133XVwfWtrK35/179Cc3JyyMnp8sSgwyxfvrxnig2SAd+CCPjCiAyEUdtkAWGMObrFixdz6623MmvWLL797W+zcuVK5syZw9SpU5k7dy4FBQUAvPPOO1x22WWAC5cbb7yR+fPnM2rUKB588MGD7xcbG3tw+/nz53PVVVcxbtw4rr/+eg5cgvDyyy8zbtw4pk+fzh133HHwfU+FAd+CAIiNCFBjAWFMr/VvL+WRv7u6R99z/LB47v3MhOPer6ioiOXLl+Pz+aiurua9997D7/fzxhtv8L3vfY/nn3/+U/t88sknvP3229TU1DB27Fhuu+22T12PsHbtWvLy8hg2bBjz5s3j/fffJycnh1tuuYV3332XrKwsFi1adMLHeyIsIIC4SL91MRljuuXqq6/G5/MBUFVVxQ033MDmzZsREVpaWrrc59JLLyUiIoKIiAgGDRrEvn37yMjIOGybmTNnHlw2ZcoUtm/fTmxsLKNGjTp47cKiRYt4+OGHg3h0h7OAAGIifNbFZEwvdiJ/6QdLTEzMwef/+q//yrnnnssLL7zA9u3bmT9/fpf7REREHHzu8/lobf3075vubHOqDfgxCIDYCGtBGGOOX1VVFenp7lY1TzzxRI+//9ixYyksLGT79u0APPvssz3+GUdjAYGNQRhjTsy3v/1tvvvd7zJ16tSg/MUfFRXFr3/9axYsWMD06dOJi4sjISGhxz/nSGyyPuDOZ9eRu2M/7337vB6uyhhzojZu3Mjpp58e6jJCrra2ltjYWFSVr3/962RnZ3PnnXee0Ht19TM92mR91oLAupiMMb3XI488wpQpU5gwYQJVVVXccsstp+yzbZAaiI302yC1MaZXuvPOO0+4xXCyrAXRWMUFRQ8xsb2AptbjuiW2Mcb0axYQ2s60ot8zJWyrdTMZY0wHFhARCShCgtRaN5MxxnRgAREWRkt4AonUUmMtCGOMOSioASEiC0SkQES2iMg9R9jmGhHJF5E8EXm6w/Kfess2isiDEsQ7h7RFJJAoddaCMMYcdO655/Lqq68etuyXv/wlt912W5fbz58/nwOn2l9yySVUVn76HjP33XcfDzzwwFE/d+nSpeTn5x98/cMf/pA33njjeMvvEUELCBHxAQ8BFwPjgUUiMr7TNtnAd4F5qjoB+Ka3fC4wD5gEnAHMAM4JVq0amUQitTYGYYw5aNGiRSxZsuSwZUuWLOnWhHkvv/wyiYmJJ/S5nQPi/vvv55/+6Z9O6L1OVjBbEDOBLapaqKrNwBJgYadtbgIeUtUKAFUt8ZYrEAmEAxFAANgXtEqjkmwMwhhzmKuuuoq//vWvB28OtH37dnbv3s0zzzxDTk4OEyZM4N577+1y35EjR1JWVgbAT37yE8aMGcOZZ555cDpwcNc3zJgxg8mTJ/O5z32O+vp6li9fzrJly7j77ruZMmUKW7duZfHixfzpT38C4M0332Tq1KlMnDiRG2+8kaampoOfd++99zJt2jQmTpzIJ5980iM/g2BeB5EO7OrwugiY1WmbMQAi8j7gA+5T1b+p6goReRvYAwjwK1Xd2PkDRORm4GaA4cOHn3ChYdGJJJJPTWPXMzEaY0LslXtg78c9+55DJsLF/3HE1cnJycycOZNXXnmFhQsXsmTJEq655hq+973vkZycTFtbG+effz4fffQRkyZN6vI9Vq9ezZIlS1i3bh2tra1MmzaN6dOnA3DllVdy0003AfCDH/yAxx57jG984xtcfvnlXHbZZVx11VWHvVdjYyOLFy/mzTffZMyYMXzpS1/iN7/5Dd/85jcBSE1NZc2aNfz617/mgQce4NFHHz3pH1GoB6n9QDYwH1gEPCIiiSIyGjgdyMAFzXkiclbnnVX1YVXNUdWctLS0Ey4iEJtCotSyv84CwhhzSMdupgPdS8899xzTpk1j6tSp5OXlHdYd1Nl7773HFVdcQXR0NPHx8Vx++eUH123YsIGzzjqLiRMn8tRTT5GXl3fUWgoKCsjKymLMmDEA3HDDDbz77rsH11955ZUATJ8+/eDkficrmC2IYiCzw+sMb1lHRcCHqtoCbBORTRwKjA9UtRZARF4B5gDvBaNQX3Qy8VJPSXV9MN7eGHOyjvKXfjAtXLiQO++8kzVr1lBfX09ycjIPPPAAq1atIikpicWLF9PY2HhC77148WKWLl3K5MmTeeKJJ3jnnXdOqtYD04X35FThwWxBrAKyRSRLRMKB64BlnbZZigsDRCQV1+VUCOwEzhERv4gEcAPUn+pi6jFRSYSh1FTuD9pHGGP6ntjYWM4991xuvPFGFi1aRHV1NTExMSQkJLBv3z5eeeWVo+5/9tlns3TpUhoaGqipqeGll146uK6mpoahQ4fS0tLCU089dXB5XFwcNTU1n3qvsWPHsn37drZs2QLAk08+yTnnBO3cHSCIAaGqrcDtwKu4X+7PqWqeiNwvIgfaWa8C5SKSD7wN3K2q5cCfgK3Ax8B6YL2qvvSpD+kpUUkANNWUBe0jjDF906JFi1i/fj2LFi1i8uTJTJ06lXHjxvH5z3+eefPmHXXfadOmce211zJ58mQuvvhiZsyYcXDdj370I2bNmsW8efMYN27cweXXXXcdP/vZz5g6dSpbt249uDwyMpLHH3+cq6++mokTJxIWFsatt97a8wfcgU33DVDwN3jmWr7s/08e/0Fwf+DGmO6x6b57nk33fSK8FoQ2VNDe3j8C0xhjTpYFBBwMiHitpbyuOcTFGGNM72ABAQcDIlFqKKk5sTMSjDE9r790gfcGJ/KztIAAiE5Gxc9gqaCkpinU1RhjcIOy5eXlFhI9QFUpLy8nMjLyuPazO8oBhPloixvKsIpySqqtBWFMb5CRkUFRURGlpaWhLqVfiIyMJCMj47j2sYDwhCVkMKyynPcrGkJdijEGCAQCZGVlhbqMAc26mDxhiZkM9+1na1ldqEsxxphewQLigIR00nQ/20uqQ12JMcb0ChYQByRk4KeVmvJiuxbCGGOwgDgk3g3eJLeWsbvKxiGMMcYC4oAEFxDDpIzCUhuHMMYYC4gDvIDIkFK2ltaGuBhjjAk9C4gDohLRmEFMCN/HhmIbqDbGGAuIDiRtLGeE72bdropQl2KMMSFnAdHRoNPJbN3F1tJaqu3+1MaYAc4CoqO0cYS31TGMcj4uqgp1NcYYE1IWEB0NcjfSyA4rZs0O62YyxgxsFhAdpbnb/p2dUML7W+32o8aYgS2oASEiC0SkQES2iMg9R9jmGhHJF5E8EXm6w/LhIvKaiGz01o8MZq0ARCdD4nDmRu1kzY5K6ptbg/6RxhjTWwUtIETEBzwEXAyMBxaJyPhO22QD3wXmqeoE4JsdVv8e+Jmqng7MBEqCVethMmaQ1ZhPc1s7q7ZbN5MxZuAKZgtiJrBFVQtVtRlYAizstM1NwEOqWgGgqiUAXpD4VfV1b3mtqtYHsdZDMmYQWb+HDH8l7xScmkwyxpjeKJgBkQ7s6vC6yFvW0RhgjIi8LyIfiMiCDssrReTPIrJWRH7mtUgOIyI3i0iuiOT22E1F0nMAWDSshNfy9tndrIwxA1aoB6n9QDYwH1gEPCIiid7ys4C7gBnAKGBx551V9WFVzVHVnLS0tJ6paOgk8EVwQUwhxZUNdlW1MWbACmZAFAOZHV5neMs6KgKWqWqLqm4DNuECowhY53VPtQJLgWlBrPUQfwQMn82o2tX4woS/fLT7lHysMcb0NsEMiFVAtohkiUg4cB2wrNM2S3GtB0QkFde1VOjtmygiB5oF5wH5Qaz1cKPm4y/NY+FoP8+vKaalrf2UfbQxxvQWQQsI7y//24FXgY3Ac6qaJyL3i8jl3mavAuUikg+8DdytquWq2obrXnpTRD4GBHgkWLV+yqj5ANw4bCdltU28udEGq40xA4/0l0HYnJwczc3N7Zk3a2+DB7JpH3UeczctYtzQOJ748syeeW9jjOlFRGS1quZ0tS7Ug9S9U5gPxlxM2ObXuHbaYP6+qZTiSrvLnDFmYLGAOJJxl0JTFdcPcWfqPrtyZ4gLMsaYU8sC4khOOxcCMQza8RfOHzeIJz/YQUNzW6irMsaYU8YC4kgCUXDGFbDhBW6bO5iK+hb+uHrXsfczxph+wgLiaKbdAC11TKt+m2nDE3nkvUJa7ZRXY8wAYQFxNBkzIG0csub33HLOaeza38DLG/aGuipjjDklLCCORsS1IopzuSC5jFFpMfzf37fa/EzGmAHBAuJYJl0LvnDC1jzBLWePIm93Nf/YYjcTMsb0fxYQxxKTAmdcBeue4rPjYhgcH8H//n1rqKsyxpigs4Dojjlfh5Z6Itb9jq+cmcX7W8r5qKgy1FUZY0xQWUB0x5Az3PxMKx9m0fQhxEX6rRVhjOn3LCC6a843oGYPcVte4ouzR/DKhr1sK6sLdVXGGBM0FhDdNfp8SBsHK37Fl+eOJOAL4+F3C0NdlTHGBI0FRHeJuLGIvR+TVvYhV0/P4PnVRZRUN4a6MmOMCQoLiOMx8RqISYMVD3Hz2aNobW/n8eXbQ12VMcYEhQXE8QhEwoybYPOrjGgv4qIJQ1iycieNLTaJnzGm/7GAOF4zvgL+SFjxEF+cPYKK+hZe/nhPqKsyxpgeZwFxvGJSYfJ1sH4Jc4Yoo9JiePKDHaGuyhhjepwFxImY/XVoa0JyH+MLs0awdmclG4qrQl2VMcb0qKAGhIgsEJECEdkiIvccYZtrRCRfRPJE5OlO6+JFpEhEfhXMOo9b2hjIvghWPsLnJqcSGQjjD9aKMMb0M0ELCBHxAQ8BFwPjgUUiMr7TNtnAd4F5qjoB+Gant/kR8G6wajwpc74O9WUkbPozn52SzovrdlPV0BLqqowxpscEswUxE9iiqoWq2gwsARZ22uYm4CFVrQBQ1ZIDK0RkOjAYeC2INZ64rLNhyERY8RBfmDWchpY2/rymKNRVGWNMjwlmQKQDHe/RWeQt62gMMEZE3heRD0RkAYCIhAE/B+462geIyM0ikisiuaWlpT1YejeIwJzboayAMxpWMSUzkSc/2GH3ijDG9BuhHqT2A9nAfGAR8IiIJAJfA15W1aP+Sa6qD6tqjqrmpKWlBb3YT5lwJcQNhRW/4ouzR1BYWseKreWnvg5jjAmCYAZEMZDZ4XWGt6yjImCZqrao6jZgEy4w5gC3i8h24AHgSyLyH0Gs9cT4w2HmzVD4DpcNLicxOmCnvBpj+o1gBsQqIFtEskQkHLgOWNZpm6W41gMikorrcipU1etVdbiqjsR1M/1eVbs8Cyrkcr4MgWgiVv0v1+Zk8lr+PvbZ/EzGmH4gaAGhqq3A7cCrwEbgOVXNE5H7ReRyb7NXgXIRyQfeBu5W1b7VRxOVBFO/AB//kS9OCKetXXlm5c5QV2WMMSdN+sugak5Ojubm5obmw/cXwoPT4Mw7uWHnxXyyt5p/fOc8Ar5QD/EYY8zRichqVc3pap39BusJyaNg3KWw+nFuyBnMvuom3sjfF+qqjDHmpFhA9JTZt0FDBfNb3iE9McoGq40xfZ4FRE8ZMQ8Gn0HYh//H52dmsnxrOVtKakNdlTHGnDALiJ4iArNuhZI8rh+yi4BPeOpDa0UYY/ouC4ieNPEqiEom8aPHuPiMofxpdRH1za2hrsoYY06IBURPCkS56yIKXuYrZ4RR09jKsnW7Q12VMcacEAuInpbzFUCYtPs5xg2Js/mZjDF9lgVET0tIh/ELkTVP8qXpqeTtruajIruZkDGm77GACIZZt0JTFVf4/0FUwMfTH9qV1caYvscCIhgyZ8KwqUStfpTPTBrCsvW7qW60mwkZY/oWC4hgEHGzvJYVcPPwYhpa2nhxbeeJbI0xpnezgAiWCVdCVDKnbX+G8UPjeerDnTZYbYzpU7oVECIS493lDREZIyKXi0gguKX1cYFImPYl5JO/8tVJ4Xyyt4a1uypDXZUxxnRbd1sQ7wKRIpKOu0f0F4EnglVUv5FzI6hyWcvfiAm3wWpjTN/S3YAQVa0HrgR+rapXAxOCV1Y/kTQCxl5M+Lrfc8WkNP7y0W6qGmyw2hjTN3Q7IERkDnA98FdvmS84JfUzM74K9WXckvYxjS3tvLDmqLfZNsaYXqO7AfFN4LvAC95d4Ubh7gBnjmXUuZAymszNf2BSRgJPr7TBamNM39CtgFDVv6vq5ar6n95gdZmq3hHk2vqHsDDXiihaxdfH1rBpXy2rd1SEuipjjDmm7p7F9LSIxItIDLAByBeRu7ux3wIRKRCRLSJyzxG2uUZE8kUkT0Se9pZNEZEV3rKPROTa4zmoXmfyIgjEcF7NMmIj/DZYbYzpE7rbxTReVauBzwKvAFm4M5mOSER8wEPAxcB4YJGIjO+0TTau62qeqk7AdWUB1ANf8pYtAH4pIondrLX3iUqESdcQyP8ziybG8JeP91BZ3xzqqowx5qi6GxAB77qHzwLLVLUFOFZH+kxgi6oWqmozsARY2Gmbm4CHVLUCQFVLvP9uUtXN3vPdQAmQ1s1ae6eZN0FrI1+NWU5zazvPr7Erq40xvVt3A+L/gO1ADPCuiIwAqo+xTzqwq8PrIm9ZR2OAMSLyvoh8ICILOr+JiMwEwoGtXay7WURyRSS3tLS0m4cSIoMnwIh5DC74A9My4nj6Q5sG3BjTu3V3kPpBVU1X1UvU2QGc2wOf7weygfnAIuCRjl1JIjIUeBL4sqq2d1HXw6qao6o5aWl9oIEx46tQuYM7s3awtbSOldv2h7oiY4w5ou4OUieIyH8d+GtdRH6Oa00cTTGQ2eF1hresoyK8LitV3QZswgUGIhKPu+bi+6r6QXfq7PVO/wzEDmFu+QvERfp5eqUNVhtjeq/udjH9FqgBrvEe1cDjx9hnFZAtIlkiEg5cByzrtM1SXOsBEUnFdTkVetu/APxeVf/UzRp7P18Acr6Mr/BNvnp6O698vJf9dTZYbYzpnbobEKep6r3egHOhqv4bMOpoO6hqK3A78CqwEXjOu8jufhG53NvsVaBcRPJxF97drarluBA6G1gsIuu8x5QTOL7eZ/piCPPzxcCbNLe18/xqu7LaGNM7SXcGSkVkBe6X9z+81/OAB1R1TpDr67acnBzNzc0NdRnd88cvw5Y3uT7+cfY0+HjzW+cgIqGuyhgzAInIalXN6Wpdd1sQtwIPich2EdkO/Aq4pYfqG3hm3wZNVdw9OJfCsjpWFJaHuiJjjPmU7p7FtF5VJwOTgEmqOhU4L6iV9WeZMyE9h0nFS0iMtGnAjTG903HdUU5Vq70rqgH+JQj1DBxzvkZYRSHfOW07r+btpay2KdQVGWPMYU7mlqPWaX4yTl8I8RksbFhKS5vyJxusNsb0MicTEHYZ8Mnw+WHWzUTvXsHV6RU8s3In7e32IzXG9B5HDQgRqRGR6i4eNcCwU1Rj/zXtBgjE8I3o19hRXs/yrTZYbYzpPY4aEKoap6rxXTziVNV/qorst6ISYer1ZBa/THZULU+v3BHqiowx5qCT6WIyPWHWrUh7Kz8cspzX8vZRUtMY6oqMMQawgAi9lNNg7CXMrViGr72JP+baYLUxpnewgOgN5nwNX+N+vjV4Lc+s3EmbDVYbY3oBC4jeYMQ8GDKJRe1/pbiijtfz94a6ImOMsYDoFURg7h3E1Wzh8/Eb+L93C+1mQsaYkLOA6C3OuBJSRvOtiBdYt3M/uTsqQl2RMWaAs4DoLcJ8cPbdJNcU8Nmo9fzf3wtDXZExZoCzgOhNzrgKkkfx3egXeWPjXraU1Ia6ImPMAGYB0Zv4/HD2txlUt4lLAmt59D1rRRhjQscCoreZeDUkj+IHscv485oi9lXbhXPGmNCwgOhtfH446y6GNWxiPrn85p2toa7IGDNAWUD0RpOuhaSR/DDuJZ5eucNaEcaYkAhqQIjIAhEpEJEtInLPEba5RkTyRSRPRJ7usPwGEdnsPW4IZp29js8P53yHjMZNXMZ71oowxoRE0AJCRHzAQ8DFwHhgkYiM77RNNvBdYJ6qTgC+6S1PBu4FZgEzgXtFJClYtfZKk66DYdO4L/JZXlxZwJ6qhlBXZIwZYILZgpgJbFHVQlVtBpYACzttcxPwkKpWAKhqibf8IuB1Vd3vrXsdWBDEWnufsDC45AHiW8v5mvyZn7+2KdQVGWMGmGAGRDqwq8PrIm9ZR2OAMSLyvoh8ICILjmNfRORmEckVkdzS0tIeLL2XyJgOU7/Ajf6XWbv2Q/J3Vx97H2OM6SGhHqT2A9nAfGAR8IiIJHZ3Z1V9WFVzVDUnLS0tSCWG2Pn3ERYew4/C/8C//zXf5mgyxpwywQyIYiCzw+sMb1lHRcAyVW1R1W3AJlxgdGffgSE2DTn3+8xlPZHbXuPvm/phS8kY0ysFMyBWAdkikiUi4cB1wLJO2yzFtR4QkVRcl1Mh8CpwoYgkeYPTF3rLBqYZX0FTxvDDiGf42V8/tvtFGGNOiaAFhKq2ArfjfrFvBJ5T1TwRuV9ELvc2exUoF5F84G3gblUtV9X9wI9wIbMKuN9bNjD5AsiCf2e47ubS/U/whw/s3tXGmOCT/tKnnZOTo7m5uaEuI6j0xW+ga5/kRr2Xn37rNgbFR4a6JGNMHyciq1U1p6t1oR6kNsdBFvw/2hJG8GN+xQPLVoa6HGNMP2cB0ZdExBK46lGGhlVyacH3ea/Abk1qjAkeC4i+JnMG7Rf/jHN8H7H7j9+msaUt1BUZY/opC4g+KDDzRnaP+SLXtr7Ia0//ItTlGGP6KQuIPmrYtb9ga+w0Lir8Dz5e+VaoyzHG9EMWEH2VL8DQry5hf1gSQ1++kZrSnaGuyBjTz1hA9GHRiYOpWPh7orSe/Y9dAy0246sxpudYQPRx46fM4Y1xPyKz4RNKH/4sNNeFuiRjTD9hAdEPXHLNTfwq4Vskl3xIw28XQqPN+mqMOXkWEP1AwBfGdV+9m+/7/4XA3jW0/e5yqLVJ/YwxJ8cCop8YFB/JlV+4ndta7qR97wb0N3Og8J1Ql2WM6cMsIPqRmVnJzL74C1za+GPK2+PgySth5SPQT+bbMsacWhYQ/cyN80YyZfoc5ld8j+K0s+Dlu+Avd0Jrc6hLM8b0MRYQ/YyI8JMrJjJtzAjmF93EjtNvgdWPw5NX2LiEMea4WED0QwFfGL++fhpjhiRwcd557Djnl1C0Cn4zF9Y8CW0toS7RGNMHWED0U7ERfh5fPIPU2Agu+/swCi5fBvHDYNnt8Oj58OHDUL0n1GUaY3oxC4h+bFB8JM/cPJvE6ABXvVDFR5cshat/54LhlbvhN3Ng/RJobw91qcaYXsgCop9LT4zimZtmkxAV4AuPrWRd/Hy4axPctgKSR8ELt8DvLoPda+1sJ2PMYYIaECKyQEQKRGSLiNzTxfrFIlIqIuu8x1c7rPupiOSJyEYReVBEJJi19mcZSdEsuXk2CdEBPv/IB7y7uQwGj4evvAGX/w/s+Qgeng//Mx3e+jHsyw91ycaYXiBoASEiPuAh4GJgPLBIRMZ3semzqjrFezzq7TsXmAdMAs4AZgDnBKvWgSAjKZrnb53LiJQYbnxiFS+uK4awMJj2JfjmR/CZ/4aEDHjv567r6bGL4JOX7fRYYwYwfxDfeyawRVULAURkCbAQ6M6fpwpEAuGAAAFgX5DqHDAGxUfy7C2zuel3ufzzknXsKK/n6+eOxhedDNMXu0dtCWx4Ht5/EJYsgshEyL4Qss6C086HhPRQH4Yx5hQJZkCkA7s6vC4CZnWx3edE5GxgE3Cnqu5S1RUi8jawBxcQv1LVjZ13FJGbgZsBhg8f3tP190vxkQF+d+NM7nn+I/7r9U28v6WMX143haEJUW6D2EEw+zbI+QoUvu3CYutb8PFzbv2QiTB0CoyYB2MXQFRS6A7GGBNUokEamBSRq4AFqvpV7/UXgVmqenuHbVKAWlVtEpFbgGtV9TwRGQ38N3Ctt+nrwLdV9b0jfV5OTo7m5uYG5Vj6I1Xl+X7YR5kAABjESURBVDXF/PDFDYT7w/jPz03ioglDjrQxlBbAxpdgx/uwZx00VECYH0bNh9EXwOAJ7hGdfCoPwxhzkkRktarmdLUumC2IYiCzw+sMb9lBqlre4eWjwE+951cAH6hqLYCIvALMAY4YEOb4iAhXTc9g2vBE7liyllueXM0XZ4/g+5eeTmTA13ljGDTOPbjbBUbxGshfChuXwZY3Dm2bkg2nXwbx6RA3FMZeDGGd3s8Y0ycEswXhx3UbnY8LhlXA51U1r8M2Q1V1j/f8CuA7qjpbRK4FbgIW4LqY/gb8UlVfOtLnWQvixDW1tvHAqwU88t42xg6O47+uncyEYQnd21kVavfBvg2wLw82vQY7V4C2ufXRKRAR57qi5n8P0sZA0sigHYsx5vgcrQURtIDwPvgS4JeAD/itqv5ERO4HclV1mYj8P+ByoBXYD9ymqp94Z0D9GjgbN2D9N1X9l6N9lgXEyXunoIS7/rie/XXNXDtjOHddOIaU2Ijjf6PWZmishB3LYcvr0NoEO1ZAdZFbnz7dBYaEwZl3QswgSB3dswdjjOmWkAXEqWQB0TOq6lv45ZubeHLFDqICPu44P5sb5o4k3H+SZ0Q3VsOuD2Hvx65LqrkOqoqgvsytH3wGpI1zXVKDz4CkEeCPdN1bxpigsYAwx21LSS0//ms+7xSUkpUawz0Xj+PC8YPp0esV68pdWNSXQ8HLULbJdVcd4I+CrLMh5TR39lR6DqSMhpY6121ljDlpFhDmhL1dUMKP/5LP1tI6JmcmcveFY5k3OqVng+KA9jZ3hlR5IVTthKpi2P6ea2m01LttwgLQ3uJOsx13KWTOhkGnQ3h0z9djzABgAWFOSmtbO8+vKeK/39jM7qpGckYk8ZUzs7hg/GD8vlMwnVd7G5RthuLVUJLvup7y/gz7C70NBBIzIXUspGZDayNkzISRZ7oZbO0sKmOOyALC9Iim1jae+XAnj/5jG0UVDaQnRrF47kiumZFJQlTg1BdUvQeKVkLJJ1BWAKWbYP9Wd31GU7XbJszvphBJHAHp0yA8xg2K15W4wfLTzjv1dRvTi1hAmB7V1q68nr+P376/jZXb9hMd7uOq6Rl8ac5IRg+KDXV57tTbXR9CyUao3AGVO2H/Ntiz/tDptwekT3eBkT7NtVRi02DyIhckxgwAFhAmaDYUV/H4+9t5af1umtvamZyRwJXTMvjM5GEkx4SHurzDNde5U2urit2YxQe/hr0boGYvlH7ibeT9/xA3zJ2K29bszqgadDr4wmH0P7nWSHOdG1QfdY4NmJs+zQLCBF1pTRNL1xbz57XFbNxTjT9MmD82jSumZnD+6YM+fXV2b1O/341V7N3grt/YX+i6qXwB2LXKdUm1t3ktEHFBo21uPCT7Ahh7KbS3urOxELjsFxA32L13e5uNg5heywLCnFIb91Tzwtpilq4tpqSmibhIP5dNGsoVUzPIGZFEWFgfu7ahvd2FQX25u2J810poaXDjFwWvQP6LULvXbZs00s2IGxaA6CQXDnWlMOlaSBvrQmRfHgyZ5K4JmXAFRMRC5iwXRsdD1a4TMSfNAsKERFu7snxrGS+sKeaVDXtpaGkjIymKz0wexlmjU5k9KqXvhUVX2ttcF1VLoxvLKF4Nqx5zp+O2t7lf/PkvurOrAMLjoLkGfBHQ1uSWxQ5x4x9tLa7LK306jLkIopJdCCRlufdNnw4+vwurF25z14dc8G9uosTWJtcNZqFhjoMFhAm5uqZWXs3bywtri3l/Sxnt6m6HesH4wZw3bhCzRiUT4e/H3TCqbgbc1kaIToXyza61se1d1xrJf9GNd0iYG/vY8X6H03iPID4dmmvdVepxQ6Fmj7sKPessNz4SlQxNNe5U3zELoLoYVj7sxkwu/Imb2r1oFfgjXAumuRYqdrigGTUfApGHPquh0u0/eMKhZS0NrsaOy3pCbQnEpFnQnSIWEKZXqWtq5a1PSg6GRVNrO9HhPs7KTuWcMYMYPyyeCcPiCZyKayx6K1X3C7ml0bUySja6gfJdKyEy3s13Nfp8dxrvykfc2Voxqe4ugNW73cB6bYk7G6tmjwsfcLPt1pe5sDoaX7gLMl/Ahce2v7ur3LMvhOZ6iBsCez9yQTTuMhdC/kh3n/M9690j58sQO9i9T0MFFOe61wAZOa7rLe8FGDYNUse4q+Tf+7kLsSmfh4v+HcJj3f1IWupdayl5FBTlQkyKe16zzx1jRKxrefnCD005v+0919I6/TOg7SA+Fzrtba4V1ll7u/tv2MD6d2cBYXqthuY2VhSW8ebGEt76pIQ9Va4bJjbCz5zTUjgrO5V5o1MZlRoTnKu3B4K6MveXflSy+6XaWOl+6TbXuV+6daXuNODIBO8vd6DwHTcVSmOlG7iPH+YuQix8xz0/8Ms462w35Xt7qwuOtiYXFPHDjt0CgsO72QAQGDHXtaDABWB766HVUcnQsN/9sh80HkryXM3jLoP1z7ixn9Mvc8ey8hHXzReT5gI1PMaFaF0pnH0XJAx3F16Wb3bHUvA3t/0533HHF4hyp0tv/wcknwYX/diFXeknruU17QZ3b/fO2tu9wE5zwdXV+qYqiIiH3N+6CzoDUa4V6I/49LZHCqyyLS7wUk479s/5KCwgTJ+gqmwvryd/dzX/2FLGe5tLKapoAGBIfCRzR6dw5mgXGIPjI4/xbuaUa2txrZ64Ya6rrDjX/SKvL3e/ANOnuZZEW4v7BRsRB6PO9a5V2eUuehyzwI2zbHje/ZKu2QMZM7yWwyrY+QEMnexaQXs/dhM87lzhLpbMOgsQN11LbQkMmwKzv+bm+wrzwc4PXbjEpHU4rRkXJgdaZCX5hwdb3DA3geTm1930LwdImGuVDJnkjRvtdnOFBaJcjfVlrpbhsw8dc8pod8vej593ARGf7n5e/kjX9Zg5y53IoO3ueIZOckGekAlnXAmBaBdUqaNdy/I3c1wdX/vQBVEg6oS+NgsI0yepKjvK63l/axnLt5SzfGsZFfUtAJyWFsOsUSnMykpmxshkhiZEWgvDHNLVgH1bi3v4I6Biu/vFHRHvWkYHup3q97uus+FzXcvlwBxfDZUuJHwBN3aTOgZWP+5aF4Fod9+T4jXeNC85LhiqimDza+4XvD/C3ZWxbLM7LXroFFj3FJzxOdd9GJMC65e4sPBHuK64knzXYmptPDQXGbhQCER7p123u1ZP5iy48W8n9KOygDD9Qnu7kr+nmuVby1i+tZzc7RXUNrnuh9TYCC4YP5ihCZFMTE9g7uiU/j3obfqmo52avH+bC5rIePd65weulRGIdicQtNS7WQGKV3tjK5e7ENv6lhufmvqFEyrJAsL0S23tysY91eRu38/qnZW8lreXplY30Bgb4Wf6iCQmZyYyJTOByRmJJ3bzI2P6OQsIMyC0trXT2q6sKCzn9fx9rNlRwaZ9NbR7/8Qzk6OYPjyJmVkpzMxK4rS0WOuWMgPe0QKii3O9jOmb/L4w/D44d+wgzh07CHCn1G4ormLdrkrW7arkH1vKWbpuNwApMeGclhbLyNRopg5PYurwRLIHxeHrDxfvGdMDrAVhBpQDZ0qt3FbOym0VFFXUs2lfzcHB75hwHzOzkpk/dhDDk6MZlhhF9qDY/nHFtzFdCFkLQkQWAP8N+IBHVfU/Oq1fDPwMKPYW/UpVH/XWDQceBTJxU2xeoqrbg1mv6f9EhKzUGLJSY7h2xnDg0NlSa3ZWsHZnJW99UsLbBaUH90mKDjAxI5HsQbHkjEjijPQEMpPtDnam/wtaC0JEfMAm4AKgCFgFLFLV/A7bLAZyVPX2LvZ/B/iJqr4uIrFAu6rWd97uAGtBmJ6iquypamRfdSNbS+tYsbWcgn3VbN5Xe3AQfFRaDKPTYhmVFsu4IXHMHpXCkAS7NsP0PaFqQcwEtqhqoVfEEmAhkH/Uvdy24wG/qr4OoKq1QazTmMOICMMSoxiWGMXU4UlcNT0DgMaWNjbvq+XDbeWs3LafwrI63i4ooaXN/ZE1MiWa8cPiGTcknjGD48hIiiIzOTo0d9szpgcEMyDSgV0dXhcBs7rY7nMicjautXGnqu4CxgCVIvJnIAt4A7hH9fDbgYnIzcDNAMOHD+/5IzCmg8iAj4kZCUzMSOCrZ40C3JlTn+ytYfnWMlbvqCBvdzUvf7z3sP2GJURy+tB4sgfHMWZwLGMGxzF6UGzvv0eGGfBCfRbTS8AzqtokIrcAvwPOw9V1FjAV2Ak8CywGHuu4s6o+DDwMrovp1JVtjOP3hXFGegJnpCccXFbb1EphaS27KxvYXl7Pxj3VbNxTzbubSw+2NkRgeHI02YNcaGR7wXH6kHgbEDe9RjADohg3wHxABocGowFQ1fIOLx8Ffuo9LwLWdeieWgrMplNAGNMbxUb4mZSRyKSMxMOWt7S1s6O8jk37atm0r4bN3n/fKSih1btYIzkmnMykKFJiIxiWGElGUjQT0xOYkplITESo/54zA00w/8WtArJFJAsXDNcBn++4gYgMVdU93svLgY0d9k0UkTRVLcW1KmwE2vRpAV8YowfFMXpQHJdMHHpw+YHg+Li4iuVbytlX08SeqkbW7Kyg0jv9VgTCRMgeFEtmcjSjB8UyOSOB9MRokmICpMVF2NQipscFLSBUtVVEbgdexZ3m+ltVzROR+4FcVV0G3CEilwOtwH5cNxKq2iYidwFvirvUdTXwSLBqNSaUOgbHFVMzDltXVd/C2l0VrN9VRUNLG/l7qtlRXsfbnxxqdRxwWloMWamxpMWFkxobwciUGMYPiycpOpzE6ICNeZjjZhfKGdMHNba0UbC3hn3VjVTUN7OnqpENxVUUVTRQVttEeV0zHf/X9oUJo1JjGDc0nrhIP3ERfgbHRxId7jt4xtawxEiiw60ba6CxqTaM6WciAz4mZyYecX1rWzvby+vI211NTWMrJdWN5O+pYe3OChpb2qhuaKW5rf1T+yVGBxiWEEVqXASJUQGGJ0czPCWa4cnRpCdGkRQTTky4z+awGiAsIIzph/wduq260tzaTl1TK/UtbeyubGB3ZQPF3n/3VDZSVttEYWktf/14D22durICPiEtNoKRqTH4fWFEB3xkJkcR7g9j7JB4hsRHMigugqGJkbS3Q1S4dW31VRYQxgxA4f4wwv3hJAHpiUe+E1lLWzt7KhvZsb+OPZWNVDY0U1Hfwp7KBnZVNFDX3EZRRT1vFZTQ1q6fChOA+Eg/cZEBhiVGkpkUTXJMOAlRARKjAyREh5MYFSApOpyRqdHERdpFhb2JBYQx5ogCvjDXxZRy7Lmnmlrb2FZWR2lNE3urGtlb1UhYmFBS3UhNYytFlQ18uG0/FfXN1De3dfke4f4wosN9xIT7iQ73kRIbzqA41yJJjYsgOTqc5JhwIgJhJEWHk5EURUJUwLq8gsQCwhjTIyL8PsYNiWfckGNv29TaRlVDC1X1LVQ2tFBe20RhWR3VDa3UN7dS19RGXVMr5XVNrC+qZF91I40tnx4zAQj3hZEU41ohSV6AHHgtIoT7hJGpMaR6N4yKi/STmRxNpN9HwCcWLkdhAWGMOeUi/D4GxfkYFNe9CQ5VlYaWNsprm9lf10xTazv765q8s7aaqahrZn99M5X1zXyyt5qK+hYq65tR4GgnakYGwhgcH0mk30dCdICMxCjaVYmPCpDgPeKjAsRHBoiN8BMf5WdIfCQpsRED4r4hFhDGmF5PRIgO9xOd7O/2VOvt7YriWivby+qprG8GgfLaZvZWNdLU2kZFfQulNU00t7ZTUtPIh9v24wsTqhtbqG5ooYshFa8eiAr4iAr4iPCHkZ4URYTfR1VDC6mx4QxLjGJoQiRxkQGiwn1EH3z4SY0NJy0ukgh/GBH+sF7dgrGAMMb0SwfmtIoO9zN+WPxx79/ertQ2t1JV30JVQwt1Ta1UN7ayt7qR0upG6pvbaGhxj+KKBmqbWkmOCae0ton1RVXsr2s+5mfERvhJiAoQHe47OF18rHeNyqD4CGLC/YSFCf4wIS7ST1psBGlxESTHhBMXGQh6K8YCwhhjuhAWJsRHuu6lzGNv/imNLW4c5UCQ1De3Ud/USmltE6U1TTS1tlNa00RNYyt1Ta3sqWoAYE9VI//YUkZNY+tR31/kUMBMG57Eg4umnkCVR2cBYYwxQRAZ8BEZ8JFygvvXN7fS0NxGm7rTh6sbWimtaaKkppEKr1VT7T2GJgbnZlUWEMYY0wtFh/sPm/pkaAKMHdL1hY/BEnZKP80YY0yfYQFhjDGmSxYQxhhjumQBYYwxpksWEMYYY7pkAWGMMaZLFhDGGGO6ZAFhjDGmS/3mntQiUgrsOMHdU4GyHiwnlPrLsfSX4wA7lt7KjsUZoappXa3oNwFxMkQk90g37e5r+sux9JfjADuW3sqO5disi8kYY0yXLCCMMcZ0yQLCeTjUBfSg/nIs/eU4wI6lt7JjOQYbgzDGGNMla0EYY4zpkgWEMcaYLg3ogBCRBSJSICJbROSeUNdzvERku4h8LCLrRCTXW5YsIq+LyGbvv0mhrrMrIvJbESkRkQ0dlnVZuzgPet/TRyIyLXSVf9oRjuU+ESn2vpt1InJJh3Xf9Y6lQEQuCk3VXRORTBF5W0TyRSRPRP7ZW96nvpujHEef+15EJFJEVorIeu9Y/s1bniUiH3o1Pysi4d7yCO/1Fm/9yBP+cFUdkA/AB2wFRgHhwHpgfKjrOs5j2A6kdlr2U+Ae7/k9wH+Gus4j1H42MA3YcKzagUuAVwABZgMfhrr+bhzLfcBdXWw73vu3FgFkef8GfaE+hg71DQWmec/jgE1ezX3quznKcfS578X72cZ6zwPAh97P+jngOm/5/wK3ec+/Bvyv9/w64NkT/eyB3IKYCWxR1UJVbQaWAAtDXFNPWAj8znv+O+CzIazliFT1XWB/p8VHqn0h8Ht1PgASRWToqan02I5wLEeyEFiiqk2qug3Ygvu32Cuo6h5VXeM9rwE2Aun0se/mKMdxJL32e/F+trXey4D3UOA84E/e8s7fyYHv6k/A+SIiJ/LZAzkg0oFdHV4XcfR/QL2RAq+JyGoRudlbNlhV93jP9wKDQ1PaCTlS7X31u7rd63b5bYeuvj5zLF7XxFTcX6x99rvpdBzQB78XEfGJyDqgBHgd18KpVNVWb5OO9R48Fm99FZByIp87kAOiPzhTVacBFwNfF5GzO65U18bsk+cx9+XaPb8BTgOmAHuAn4e2nOMjIrHA88A3VbW647q+9N10cRx98ntR1TZVnQJk4Fo2407F5w7kgCgGMju8zvCW9RmqWuz9twR4AfcPZ9+BJr7335LQVXjcjlR7n/uuVHWf9z91O/AIh7orev2xiEgA90v1KVX9s7e4z303XR1HX/5eAFS1EngbmIPrzvN7qzrWe/BYvPUJQPmJfN5ADohVQLZ3JkA4bjBnWYhr6jYRiRGRuAPPgQuBDbhjuMHb7AbgxdBUeEKOVPsy4EveGTOzgaoO3R29Uqd++Ctw3w24Y7nOO9MkC8gGVp7q+o7E66t+DNioqv/VYVWf+m6OdBx98XsRkTQRSfSeRwEX4MZU3gau8jbr/J0c+K6uAt7yWn3HL9Qj9KF84M7A2ITrz/t+qOs5ztpH4c66WA/kHagf19f4JrAZeANIDnWtR6j/GVwTvwXXf/qVI9WOO4vjIe97+hjICXX93TiWJ71aP/L+hx3aYfvve8dSAFwc6vo7HcuZuO6jj4B13uOSvvbdHOU4+tz3AkwC1no1bwB+6C0fhQuxLcAfgQhveaT3eou3ftSJfrZNtWGMMaZLA7mLyRhjzFFYQBhjjOmSBYQxxpguWUAYY4zpkgWEMcaYLllAGHMMItLWYfbPddKDM/+KyMiOs8Aa05v4j72JMQNeg7ppDowZUKwFYcwJEnc/jp+KuyfHShEZ7S0fKSJveRPCvSkiw73lg0XkBW9e//UiMtd7K5+IPOLN9f+ad7UsInKHdz+Dj0RkSYgO0wxgFhDGHFtUpy6mazusq1LVicCvgF96y/4H+J2qTgKeAh70lj8I/F1VJ+PuH5HnLc8GHlLVCUAl8Dlv+T3AVO99bg3WwRlzJHYltTHHICK1qhrbxfLtwHmqWuhNDLdXVVNEpAw3hUOLt3yPqqaKSCmQoapNHd5jJPC6qmZ7r78DBFT1xyLyN6AWWAos1UP3BDDmlLAWhDEnR4/w/Hg0dXjexqGxwUtx8xxNA1Z1mLnTmFPCAsKYk3Nth/+u8J4vx80ODHA98J73/E3gNjh4A5iEI72piIQBmar6NvAd3JTNn2rFGBNM9heJMccW5d3N64C/qeqBU12TROQjXCtgkbfsG8DjInI3UAp82Vv+z8DDIvIVXEvhNtwssF3xAX/wQkSAB9XdC8CYU8bGIIw5Qd4YRI6qloW6FmOCwbqYjDHGdMlaEMYYY7pkLQhjjDFdsoAwxhjTJQsIY4wxXbKAMMYY0yULCGOMMV36/35XLAA5KU1bAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJfaluKCSxn6"
      },
      "source": [
        "Dal grafico possiamo vedere come la trainingloss tenda a diminuire e questo è una cosa positiva perchè l'obbiettivo del problema di ottimizzazione è minimizzarla.\n",
        "La validation loss vediamo che tende a rimanere discostata e iniziare ad alzarsi, segno che con l'aumentare delle epoche avremo un overfitt ovvero che avremmo memorizzato alla perfezione il trainig set e il modello fallirà sul validation data. Questo andamento è segno che non bisogna aumentare le epoche.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XLTSr8Wba49M"
      },
      "source": [
        "Per l'accuracy vediamo che si attesta sul 75% e possiamo osservare come la training e la validation accuracy siano simili che ci da un indicazione sul fatto che non stiamo andando in completo overfitting, ma possiamo già intuire che se proseguiamo con le epoche il training tenderà ad avvicinarsi ad uno e la validation a crollare come già sembra fare intorno alle 260 epoche trovandoci quindi in un caso di overfitting.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2jiOZzvyJbsN"
      },
      "source": [
        "## Validate the model and comment the results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Zmx9AiJdv7U"
      },
      "source": [
        "La valutazione del modello si basa su loss,accuracy, f1, precision e recall per avere un idea il più completa possibile del modello."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sgGlAIaEJbsO",
        "outputId": "15b00a12-97b3-4bb4-e06f-5f6d06ccc336"
      },
      "source": [
        "loss, accuracy, f1_score, precision, recall = model.evaluate(XX_val, YY_val, batch_size=batch_size)\n"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "22/22 [==============================] - 0s 2ms/step - loss: 0.5659 - accuracy: 0.7371 - f1_m: 0.5700 - precision_m: 0.7255 - recall_m: 0.4739\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I8lz8GyZTDj5",
        "outputId": "359b577e-b380-4d61-e5fb-958071fb1d87"
      },
      "source": [
        "print(\"\\n%s: %.2f\" % (\"loss\", loss))\n",
        "print(\"\\n%s: %.2f\" % (\"accuracy\", accuracy))\n",
        "print(\"\\n%s: %.2f\" % (\"f1_score\", f1_score))\n",
        "print(\"\\n%s: %.2f\" % (\"precision\", precision))\n",
        "print(\"\\n%s: %.2f\" % (\"recall\", recall))"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "loss: 0.57\n",
            "\n",
            "accuracy: 0.74\n",
            "\n",
            "f1_score: 0.57\n",
            "\n",
            "precision: 0.73\n",
            "\n",
            "recall: 0.47\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N3DZymDr3maP"
      },
      "source": [
        "Il modello in particolare presenta un buon livello di f1_score ma non altissimo da assicurare una buona capacità del modello, si può anche notare come l'accuracy e la precision non siano molto alta se pure buone."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5MhCwXroWmf9"
      },
      "source": [
        "## Make predictions (on the provided test set)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X8bL1jl85AVU"
      },
      "source": [
        "Date le misure di performance precedentemnte illustrate penso che il modello potrebbe non performare al meglio su un dei dai che non sono mai stati visti."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t3uy1YsATyH7"
      },
      "source": [
        "# preprocessing per fare le predizioni\n",
        "Test.drop(['ID','SEX'], axis=1, inplace=True)\n",
        "test = Test.astype('float32')\n",
        "test = scaler.transform(test.values)  \n"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fbtA2vJRWpMY",
        "outputId": "de20208c-f48d-4c5a-9426-4bb540a20bdc"
      },
      "source": [
        "predictions = model.predict(test)\n",
        "print('predictions shape:', predictions.shape)\n",
        "predictions[:10]"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "predictions shape: (6000, 1)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.2647308 ],\n",
              "       [0.22536147],\n",
              "       [0.2619921 ],\n",
              "       [0.2816325 ],\n",
              "       [0.31144792],\n",
              "       [0.22998843],\n",
              "       [0.44146666],\n",
              "       [0.52903944],\n",
              "       [0.20752895],\n",
              "       [0.33353367]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9w-sa4AlaBJg"
      },
      "source": [
        "# OPTIONAL -- Export the predictions in the format indicated in the assignment release page."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tTPSYsbVaAQ_",
        "outputId": "790c4d60-b23c-46af-e1b6-f9e6a5a2f5df"
      },
      "source": [
        "y_classes = (predictions > 0.5).astype(np.uint8)\n",
        "unique, counts = np.unique(y_classes, return_counts=True)\n",
        "dict(zip(unique, counts))"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 4919, 1: 1081}"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lItlVIzSWX52"
      },
      "source": [
        "np.savetxt(\"Artemisia_Sarteschi_829677_score2.txt\",y_classes, fmt='%-d')"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ubxNAt97Z-Jv"
      },
      "source": [
        "# OPTIONAL -- Implement some regularization methods of your choice and make a comparison between (training/validation) performances of regularized models (also compare with the case of no regularization)\n",
        "\n",
        "Attempts in this section will be taken into account, if well-enough done, to (at least partially) compensate for potential incorrectessness in the mandatory sections. \n",
        "On the other hand, any incorrectessness in _this_ section won't be taken into account in the final score."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "itpvnhdnZ2Yt"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}